  public TokenStream tokenStream(String fieldName, Reader reader) {
    if (!removeStopFilter) {
      return queryAnalyzer.tokenStream(fieldName, reader);
    }
    
    Analyzer a = map.get(fieldName);
    if (a != null) {
      return a.tokenStream(fieldName, reader);
    }

    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);
    Analyzer qa = ft.getQueryAnalyzer();
    if (!(qa instanceof TokenizerChain)) {
      map.put(fieldName, qa);
      return qa.tokenStream(fieldName, reader);
    }
    TokenizerChain tcq = (TokenizerChain)qa;
    Analyzer ia = ft.getAnalyzer();
    if (ia == qa || !(ia instanceof TokenizerChain)) {
      map.put(fieldName, qa);
      return qa.tokenStream(fieldName, reader);
    }
    TokenizerChain tci = (TokenizerChain)ia;

    // make sure that there isn't a stop filter in the indexer
    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {
      if (tf instanceof StopFilterFactory) {
        map.put(fieldName, qa);
        return qa.tokenStream(fieldName, reader);
      }
    }

    // now if there is a stop filter in the query analyzer, remove it
    int stopIdx = -1;
    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();

    for (int i=0; i<facs.length; i++) {
      TokenFilterFactory tf = facs[i];
      if (tf instanceof StopFilterFactory) {
        stopIdx = i;
        break;
      }
    }

    if (stopIdx == -1) {
      // no stop filter exists
      map.put(fieldName, qa);
      return qa.tokenStream(fieldName, reader);
    }

    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];
    for (int i=0,j=0; i<facs.length; i++) {
      if (i==stopIdx) continue;
      newtf[j++] = facs[i];
    }

    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);
    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));

    map.put(fieldName, newa);
    return newa.tokenStream(fieldName, reader);        
  }

