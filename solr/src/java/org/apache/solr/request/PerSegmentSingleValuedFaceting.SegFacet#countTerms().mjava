    void countTerms() throws IOException {
      FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(reader, fieldName);
      final String[] terms = this.terms = si.lookup;
      final int[] termNum = this.ords = si.order;
      // SolrCore.log.info("reader= " + reader + "  FC=" + System.identityHashCode(si));

      if (prefix!=null) {
        startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);
        if (startTermIndex<0) startTermIndex=-startTermIndex-1;
        // find the end term.  \uffff isn't a legal unicode char, but only compareTo
        // is used, so it should be fine, and is guaranteed to be bigger than legal chars.
        // TODO: switch to binarySearch version that takes start/end in Java6
        endTermIndex = Arrays.binarySearch(terms,prefix+"\uffff\uffff\uffff\uffff",nullStrComparator);
        endTermIndex = -endTermIndex-1;
      } else {
        startTermIndex=0;
        endTermIndex=terms.length;
      }

      final int nTerms=endTermIndex-startTermIndex;
      if (nTerms>0) {
        // count collection array only needs to be as big as the number of terms we are
        // going to collect counts for.
        final int[] counts = this.counts = new int[nTerms];
        DocIdSet idSet = baseSet.getDocIdSet(reader);
        DocIdSetIterator iter = idSet.iterator();

        if (startTermIndex==0 && endTermIndex==terms.length) {
          // specialized version when collecting counts for all terms
          int doc;
          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
            counts[termNum[doc]]++;
          }
        } else {
          // version that adjusts term numbers because we aren't collecting the full range
          int doc;
          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
            int term = termNum[doc];
            int arrIdx = term-startTermIndex;
            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
          }
        }
      }
    }

