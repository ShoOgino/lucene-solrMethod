  /**
   * Returns a list of terms in the specified field along with the 
   * corresponding count of documents in the set that match that constraint.
   * This method uses the FilterCache to get the intersection count between <code>docs</code>
   * and the DocSet for each term in the filter.
   *
   * @see FacetParams#FACET_LIMIT
   * @see FacetParams#FACET_ZEROS
   * @see FacetParams#FACET_MISSING
   */
  public NamedList getFacetTermEnumCounts(SolrIndexSearcher searcher, DocSet docs, String field, int offset, int limit, int mincount, boolean missing, String sort, String prefix)
    throws IOException {

    /* :TODO: potential optimization...
    * cache the Terms with the highest docFreq and try them first
    * don't enum if we get our max from them
    */

    // Minimum term docFreq in order to use the filterCache for that term.
    int minDfFilterCache = params.getFieldInt(field, FacetParams.FACET_ENUM_CACHE_MINDF, 0);

    // make sure we have a set that is fast for random access, if we will use it for that
    DocSet fastForRandomSet = docs;
    if (minDfFilterCache>0 && docs instanceof SortedIntDocSet) {
      SortedIntDocSet sset = (SortedIntDocSet)docs;
      fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());
    }


    IndexSchema schema = searcher.getSchema();
    IndexReader r = searcher.getReader();
    FieldType ft = schema.getFieldType(field);

    boolean sortByCount = sort.equals("count") || sort.equals("true");
    final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;
    final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;
    final NamedList res = new NamedList();

    int min=mincount-1;  // the smallest value in the top 'N' values    
    int off=offset;
    int lim=limit>=0 ? limit : Integer.MAX_VALUE;

    BytesRef startTermBytes = null;
    if (prefix != null) {
      String indexedPrefix = ft.toInternal(prefix);
      startTermBytes = new BytesRef(indexedPrefix);
    }

    Fields fields = MultiFields.getFields(r);
    Terms terms = fields==null ? null : fields.terms(field);
    TermsEnum termsEnum = null;
    SolrIndexSearcher.DocsEnumState deState = null;
    BytesRef term = null;
    if (terms != null) {
      termsEnum = terms.iterator();

      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for
      // facet.offset when sorting by index order.

      if (startTermBytes != null) {
        if (termsEnum.seek(startTermBytes, true) == TermsEnum.SeekStatus.END) {
          termsEnum = null;
        } else {
          term = termsEnum.term();
        }
      } else {
        // position termsEnum on first term
        term = termsEnum.next();
      }
    }

    Term template = new Term(field);
    DocsEnum docsEnum = null;
    CharArr spare = new CharArr();

    if (docs.size() >= mincount) {
      while (term != null) {

        if (startTermBytes != null && !term.startsWith(startTermBytes))
          break;

        int df = termsEnum.docFreq();

        // If we are sorting, we can use df>min (rather than >=) since we
        // are going in index order.  For certain term distributions this can
        // make a large difference (for example, many terms with df=1).
        if (df>0 && df>min) {
          int c;

          if (df >= minDfFilterCache) {
            // use the filter cache
            // TODO: need a term query that takes a BytesRef to handle binary terms
            spare.reset();
            ByteUtils.UTF8toUTF16(term, spare);
            Term t = template.createTerm(spare.toString());

            if (deState==null) {
              deState = new SolrIndexSearcher.DocsEnumState();
              deState.deletedDocs = MultiFields.getDeletedDocs(r);
              deState.termsEnum = termsEnum;
              deState.reuse = docsEnum;
            }

            c = searcher.numDocs(new TermQuery(t), docs, deState);

            docsEnum = deState.reuse;
          } else {
            // iterate over TermDocs to calculate the intersection

            // TODO: specialize when base docset is a bitset or hash set (skipDocs)?  or does it matter for this?
            // TODO: do this per-segment for better efficiency (MultiDocsEnum just uses base class impl)
            // TODO: would passing deleted docs lead to better efficiency over checking the fastForRandomSet?
            docsEnum = termsEnum.docs(null, docsEnum);
            c=0;

            if (docsEnum instanceof MultiDocsEnum) {
              MultiDocsEnum.EnumWithSlice[] subs = ((MultiDocsEnum)docsEnum).getSubs();
              int numSubs = ((MultiDocsEnum)docsEnum).getNumSubs();
              for (int subindex = 0; subindex<numSubs; subindex++) {
                MultiDocsEnum.EnumWithSlice sub = subs[subindex];
                if (sub.docsEnum == null) continue;
                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();
                int base = sub.slice.start;
                for (;;) {
                  int nDocs = sub.docsEnum.read();
                  if (nDocs == 0) break;
                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
                  int end = bulk.docs.offset + nDocs;
                  for (int i=bulk.docs.offset; i<end; i++) {
                    if (fastForRandomSet.exists(docArr[i]+base)) c++;
                  }
                }
              }
            } else {

              // this should be the same bulk result object if sharing of the docsEnum succeeded
              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();

              for (;;) {
                int nDocs = docsEnum.read();
                if (nDocs == 0) break;
                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
                int end = bulk.docs.offset + nDocs;
                for (int i=bulk.docs.offset; i<end; i++) {
                  if (fastForRandomSet.exists(docArr[i])) c++;
                }
              }
            }
            

          }

          if (sortByCount) {
            if (c>min) {
              BytesRef termCopy = new BytesRef(term);
              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));
              if (queue.size()>=maxsize) min=queue.last().val;
            }
          } else {
            if (c >= mincount && --off<0) {
              if (--lim<0) break;
              spare.reset();
              ft.indexedToReadable(term, spare);
              res.add(spare.toString(), c);
            }
          }
        }

        term = termsEnum.next();
      }
    }

    if (sortByCount) {
      for (CountPair<BytesRef,Integer> p : queue) {
        if (--off>=0) continue;
        if (--lim<0) break;
        spare.reset();
        ft.indexedToReadable(p.key, spare);
        res.add(spare.toString(), p.val);
      }
    }

    if (missing) {
      res.add(null, getFieldMissingCount(searcher,docs,field));
    }

    return res;
  }

