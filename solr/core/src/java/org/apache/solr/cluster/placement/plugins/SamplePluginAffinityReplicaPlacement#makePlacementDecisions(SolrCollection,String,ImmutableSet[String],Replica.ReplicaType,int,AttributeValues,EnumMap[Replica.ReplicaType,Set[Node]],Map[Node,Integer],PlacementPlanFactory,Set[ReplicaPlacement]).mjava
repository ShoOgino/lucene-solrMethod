  /**
   * <p>Picks nodes from {@code targetNodes} for placing {@code numReplicas} replicas.
   *
   * <p>The criteria used in this method are, in this order:
   * <ol>
   *     <li>No more than one replica of a given shard on a given node (strictly enforced)</li>
   *     <li>Balance as much as possible the number of replicas of the given {@link org.apache.solr.cluster.Replica.ReplicaType} over available AZ's.
   *     This balancing takes into account existing replicas <b>of the corresponding replica type</b>, if any.</li>
   *     <li>Place replicas is possible on nodes having more than a certain amount of free disk space (note that nodes with a too small
   *     amount of free disk space were eliminated as placement targets earlier, in {@link #getNodesPerReplicaType}). There's
   *     a threshold here rather than sorting on the amount of free disk space, because sorting on that value would in
   *     practice lead to never considering the number of cores on a node.</li>
   *     <li>Place replicas on nodes having a smaller number of cores (the number of cores considered
   *     for this decision includes decisions made during the processing of the placement request)</li>
   * </ol>
   */
  @SuppressForbidden(reason = "Ordering.arbitrary() has no equivalent in Comparator class. Rather reuse than copy.")
  private void makePlacementDecisions(SolrCollection solrCollection, String shardName, ImmutableSet<String> availabilityZones,
                                      Replica.ReplicaType replicaType, int numReplicas, final AttributeValues attrValues,
                                      EnumMap<Replica.ReplicaType, Set<Node>> replicaTypeToNodes, Map<Node, Integer> coresOnNodes,
                                      PlacementPlanFactory placementPlanFactory, Set<ReplicaPlacement> replicaPlacements) throws PlacementException {
    // Build the set of candidate nodes, i.e. nodes not having (yet) a replica of the given shard
    Set<Node> candidateNodes = new HashSet<>(replicaTypeToNodes.get(replicaType));

    // Count existing replicas per AZ. We count only instances the type of replica for which we need to do placement. This
    // can be changed in the loop below if we want to count all replicas for the shard.
    Map<String, Integer> azToNumReplicas = Maps.newHashMap();
    // Add all "interesting" AZ's, i.e. AZ's for which there's a chance we can do placement.
    for (String az : availabilityZones) {
      azToNumReplicas.put(az, 0);
    }

    Shard shard = solrCollection.getShard(shardName);
    if (shard != null) {
      // shard is non null if we're adding replicas to an already existing collection.
      // If we're creating the collection, the shards do not exist yet.
      for (Replica replica : shard.replicas()) {
        // Nodes already having any type of replica for the shard can't get another replica.
        candidateNodes.remove(replica.getNode());
        // The node's AZ has to be counted as having a replica if it has a replica of the same type as the one we need
        // to place here (remove the "if" below to balance the number of replicas per AZ across all replica types rather
        // than within each replica type, but then there's a risk that all NRT replicas for example end up on the same AZ).
        // Note that if in the cluster nodes are configured to accept a single replica type and not multiple ones, the
        // two options are equivalent (governed by system property AVAILABILITY_ZONE_SYSPROP on each node)
        if (replica.getType() == replicaType) {
          final String az = getNodeAZ(replica.getNode(), attrValues);
          if (azToNumReplicas.containsKey(az)) {
            // We do not count replicas on AZ's for which we don't have any node to place on because it would not help
            // the placement decision. If we did want to do that, note the dereferencing below can't be assumed as the
            // entry will not exist in the map.
            azToNumReplicas.put(az, azToNumReplicas.get(az) + 1);
          }
        }
      }
    }

    // We now have the set of real candidate nodes, we've enforced "No more than one replica of a given shard on a given node".
    // We also counted for the shard and replica type under consideration how many replicas were per AZ, so we can place
    // (or try to place) replicas on AZ's that have fewer replicas

    // Get the candidate nodes per AZ in order to build (further down) a mapping of AZ to placement candidates.
    Map<String, List<Node>> nodesPerAz = Maps.newHashMap();
    for (Node node : candidateNodes) {
      String nodeAz = getNodeAZ(node, attrValues);
      List<Node> nodesForAz = nodesPerAz.computeIfAbsent(nodeAz, k -> new ArrayList<>());
      nodesForAz.add(node);
    }

    // Build a treeMap sorted by the number of replicas per AZ and including candidates nodes suitable for placement on the
    // AZ, so we can easily select the next AZ to get a replica assignment and quickly (constant time) decide if placement
    // on this AZ is possible or not.
    TreeMultimap<Integer, AzWithNodes> azByExistingReplicas = TreeMultimap.create(Comparator.naturalOrder(), Ordering.arbitrary());
    for (Map.Entry<String, List<Node>> e : nodesPerAz.entrySet()) {
      azByExistingReplicas.put(azToNumReplicas.get(e.getKey()), new AzWithNodes(e.getKey(), e.getValue()));
    }

    CoresAndDiskComparator coresAndDiskComparator = new CoresAndDiskComparator(attrValues, coresOnNodes, deprioritizedFreeDiskGB);

    // Now we have for each AZ on which we might have a chance of placing a replica, the list of candidate nodes for replicas
    // (candidate: does not already have a replica of this shard and is in the corresponding AZ).
    // We must now select those of the nodes on which we actually place the replicas, and will do that based on the total
    // number of cores already present on these nodes as well as the free disk space.
    // We sort once by the order related to number of cores and disk space each list of nodes on an AZ. We do not sort all
    // of them ahead of time because we might be placing a small number of replicas and it might be wasted work.
    for (int i = 0; i < numReplicas; i++) {
      // Pick the AZ having the lowest number of replicas for this shard, and if that AZ has available nodes, pick the
      // most appropriate one (based on number of cores and disk space constraints). In the process, remove entries (AZ's)
      // that do not have nodes to place replicas on because these are useless to us.
      Map.Entry<Integer, AzWithNodes> azWithNodesEntry = null;
      for (Iterator<Map.Entry<Integer, AzWithNodes>> it = azByExistingReplicas.entries().iterator(); it.hasNext(); ) {
        Map.Entry<Integer, AzWithNodes> entry = it.next();
        if (!entry.getValue().availableNodesForPlacement.isEmpty()) {
          azWithNodesEntry = entry;
          // Remove this entry. Will add it back after a node has been removed from the list of available nodes and the number
          // of replicas on the AZ has been increased by one (search for "azByExistingReplicas.put" below).
          it.remove();
          break;
        } else {
          it.remove();
        }
      }

      if (azWithNodesEntry == null) {
        // This can happen because not enough nodes for the placement request or already too many nodes with replicas of
        // the shard that can't accept new replicas or not enough nodes with enough free disk space.
        throw new PlacementException("Not enough nodes to place " + numReplicas + " replica(s) of type " + replicaType +
                " for shard " + shardName + " of collection " + solrCollection.getName());
      }

      AzWithNodes azWithNodes = azWithNodesEntry.getValue();
      List<Node> nodes = azWithNodes.availableNodesForPlacement;

      if (!azWithNodes.hasBeenSorted) {
        // Make sure we do not tend to use always the same nodes (within an AZ) if all conditions are identical (well, this
        // likely is not the case since after having added a replica to a node its number of cores increases for the next
        // placement decision, but let's be defensive here, given that multiple concurrent placement decisions might see
        // the same initial cluster state, and we want placement to be reasonable even in that case without creating an
        // unnecessary imbalance).
        // For example, if all nodes have 0 cores and same amount of free disk space, ideally we want to pick a random node
        // for placement, not always the same one due to some internal ordering.
        Collections.shuffle(nodes, new Random());

        // Sort by increasing number of cores but pushing nodes with low free disk space to the end of the list
        nodes.sort(coresAndDiskComparator);

        azWithNodes.hasBeenSorted = true;
      }

      Node assignTarget = nodes.remove(0);

      // Insert back a corrected entry for the AZ: one more replica living there and one less node that can accept new replicas
      // (the remaining candidate node list might be empty, in which case it will be cleaned up on the next iteration).
      azByExistingReplicas.put(azWithNodesEntry.getKey() + 1, azWithNodes);

      // Track that the node has one more core. These values are only used during the current run of the plugin.
      coresOnNodes.merge(assignTarget, 1, Integer::sum);

      // Register the replica assignment just decided
      replicaPlacements.add(placementPlanFactory.createReplicaPlacement(solrCollection, shardName, assignTarget, replicaType));
    }
  }

