  public void split() throws IOException {

    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
    List<OpenBitSet[]> segmentDocSets = new ArrayList<OpenBitSet[]>(leaves.size());

    log.info("SolrIndexSplitter: partitions=" + ranges.size() + " segments="+leaves.size());

    for (AtomicReaderContext readerContext : leaves) {
      assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order
      OpenBitSet[] docSets = split(readerContext);
      segmentDocSets.add( docSets );
    }


    // would it be more efficient to write segment-at-a-time to each new index?
    // - need to worry about number of open descriptors
    // - need to worry about if IW.addIndexes does a sync or not...

    IndexReader[] subReaders = new IndexReader[leaves.size()];
    for (int partitionNumber=0; partitionNumber<ranges.size(); partitionNumber++) {
      log.info("SolrIndexSplitter: partition #" + partitionNumber + " range=" + ranges.get(partitionNumber));

      for (int segmentNumber = 0; segmentNumber<subReaders.length; segmentNumber++) {
        subReaders[segmentNumber] = new LiveDocsReader( leaves.get(segmentNumber), segmentDocSets.get(segmentNumber)[partitionNumber] );
      }

      String path = paths.get(partitionNumber);
      boolean success = false;
      SolrCore core = searcher.getCore();
      IndexWriter iw = new SolrIndexWriter("SplittingIndexWriter"+partitionNumber + " " + ranges.get(partitionNumber), path,
          core.getDirectoryFactory(), true, core.getSchema(),
          core.getSolrConfig().indexConfig, core.getDeletionPolicy(), core.getCodec(), true);

      try {
        iw.addIndexes(subReaders);
        // TODO: will many deletes have been removed, or should we optimize?
        success = true;
      } finally {
        if (success) {
          IOUtils.close(iw);
        } else {
          IOUtils.closeWhileHandlingException(iw);
        }
      }

    }

  }

