    /** 
     * Uses the previously popuated stats to compute a score for the specified field.
     *
     * <p>
     * The current implementation returns scores in the range of <code>[0,1]</code>, but this 
     * may change in future implementations.  The only current garuntees are:
     * </p>
     * 
     * <ul>
     * <li>0 (or less) means this is garunteed to not be a phrase</li>
     * <li>larger numbers are higher confidence</li>
     * </li>
     * 
     * @see #populateStats
     * @see #populateScores
     * @see #getFieldScore(String)
     * @return a score value
     */
    private static double computeFieldScore(final Phrase input,
                                            final String field,
                                            final int maxIndexedPositionLength,
                                            final int maxQueryPositionLength) {
      final long num_indexed_sub_phrases = input.getLargestIndexedSubPhrases().size();
      assert 0 <= num_indexed_sub_phrases; // should be impossible

      if (input.getIndividualIndexedTerms().size() < input.getPositionLength()) {
        // there are "gaps" in our input, where individual words have not been indexed (stop words, 
        // or multivalue position gap) which means we are not a viable candidate for being a valid Phrase.
        return -1.0D;
      }
      
      final long phrase_conj_count = input.getConjunctionDocCount(field);
      // if there isn't a single document containing all the terms in our
      // phrase, then it is 100% not a phrase
      if (phrase_conj_count <= 0) {
        return -1.0D;
      }
      
      // single words automatically score 0.0 (unless they already scored less for not existing
      if (input.getPositionLength() <= 1) {
        return 0.0D;
      }
      
      double field_score = 0.0D;
      long max_sub_conj_count = phrase_conj_count;
      
      // At the moment, the contribution of each "words" sub-Phrase to the field score to the input
      // Phrase is independent of any context of "input".  Depending on if/how sub-phrase scoring
      // changes, we might consider computing the scores of all the indexed phrases first, and
      // aching the portions of their values that are re-used when computing the scores of
      // longer phrases?
      //
      // This would make the overall scoring of all phrases a lot more complicated,
      // but could save CPU cycles? 
      // (particularly when maxIndexedPositionLength <<< maxQueryPositionLength ???)
      //
      // My gut says that knowing the conj_count(input) "context" should help us score the 
      // sub-phrases better, but i can't yet put my finger on why/how.  maybe by comparing
      // the conj_count(input) to the max(conj_count(parent of words)) ?
      
      // for each of the longest indexed phrases, aka indexed sub-sequence of "words", we have...
      for (Phrase words : input.getLargestIndexedSubPhrases()) {
        // we're going to compute scores in range of [-1:1] to indicate the likelihood that our
        // "words" should be used as a "phrase", based on a bayesian document categorization model,
        // where the "words as a phrase" (aka: phrase) is our candidate category.
        //
        //  P(words|phrase) * P(phrase) - P(words|not phrase) * P(not phrase)
        //
        // Where...
        //  P(words|phrase)     =  phrase_ttf / min(word_ttf)
        //  P(phrase)           =~ phrase_docFreq / conj_count(words in phrase)      *SEE NOTE BELOW*
        //  P(words|not phrase) =  phrase_ttf / max(word_ttf) 
        //  P(not a phrase)     =  1 - P(phrase)
        //
        //       ... BUT! ...
        //
        // NOTE: we're going to reduce our "P(phrase) by the max "P(phrase)" of all the (indexed)
        // candidate phrases we are a sub-phrase of, to try to offset the inherent bias in favor 
        // of small indexed phrases -- because anytime the super-phrase exists, the sub-phrase exists

        
        // IDEA: consider replacing this entire baysian model with LLR (or rootLLR)...
        //  http://mahout.apache.org/docs/0.13.0/api/docs/mahout-math/org/apache/mahout/math/stats/LogLikelihood.html
        // ...where we compute LLR over each of the TTF of the pairs of adjacent sub-phrases of each 
        // indexed phrase and take the min|max|avg of the LLR scores.
        //
        // ie: for indexed shingle "quick brown fox" compute LLR(ttf("quick"), ttf("brown fox")) &
        // LLR(ttf("quick brown"), ttf("fox")) using ttf("quick brown fox") as the co-occurance
        // count, and sumTTF-ttf("quick")-ttf("brown")-ttf("fox") as the "something else"
        //
        // (we could actually compute LLR stats over TTF and DF and combine them)
        //
        // NOTE: Going the LLR/rootLLR route would require building a full "tree" of every (indexed)
        // sub-phrase of every other phrase (or at least: all siblings of diff sizes that add up to
        // an existing phrase).  As well as require us to give up on a predictible "range" of
        // legal values for scores (IIUC from the LLR docs)
        
        final long phrase_ttf = words.getTTF(field);
        final long phrase_df = words.getDocFreq(field);
        final long words_conj_count = words.getConjunctionDocCount(field);
        max_sub_conj_count = Math.max(words_conj_count, max_sub_conj_count);
        
        final double max_wrapper_phrase_probability = 
          words.getIndexedSuperPhrases().stream()
          .mapToDouble(p -> p.getConjunctionDocCount(field) <= 0 ?
                       // special case check -- we already know *our* conj count > 0,
                       // but we need a similar check for wrapper phrases: if <= 0, their probability is 0
                       0.0D : ((double)p.getDocFreq(field) / p.getConjunctionDocCount(field))).max().orElse(0.0D);
        
        final LongSummaryStatistics words_ttfs = 
          words.getIndividualIndexedTerms().stream()
          .collect(Collectors.summarizingLong(t -> t.getTTF(field)));
        
        final double words_phrase_prob = (phrase_ttf / (double)words_ttfs.getMin());
        final double words_not_phrase_prob = (phrase_ttf / (double)words_ttfs.getMax());
        
        final double phrase_prob = (phrase_conj_count / (double)words_conj_count);
        
          
        final double phrase_score = words_phrase_prob * (phrase_prob - max_wrapper_phrase_probability);
        final double not_phrase_score =  words_not_phrase_prob * (1 - (phrase_prob - max_wrapper_phrase_probability));
        final double words_score = phrase_score - not_phrase_score;
        
        field_score += words_score;
      }

      // NOTE: the "scaling" factors below can "increase" negative scores (by reducing the unsigned value)
      // when they should ideally be penalizing the scores further, but since we currently don't care
      // about any score lower then 0, it's not worth worrying about.
      
      // Average the accumulated score over the number of actual indexed sub-phrases that contributed
      //
      // NOTE: since we subsequently want to multiply the score by a fraction with num_indexed_sub_phrases
      // in the numerator, we can skip this...
      // SEE BELOW // field_score /= (double) num_indexed_sub_phrases;
      
      // If we leave field_score as is, then a phrase longer then the maxIndexedPositionLength
      // will never score higher then the highest scoring sub-phrase it has (because we've averaged them)
      // so we scale the scores against the longest possible phrase length we're considering
      //
      // NOTE: We don't use num_indexed_sub_phrases in the numerator since we skipped it when
      // averating above...
      field_score *= ( 1.0D // SEE ABOVE // * ( (double)num_indexed_sub_phrases )
                       / (1 + maxQueryPositionLength - maxIndexedPositionLength) );
      
      // scale the field_score based on the ratio of the conjunction docCount for the whole phrase
      // realtive to the largest conjunction docCount of it's (largest indexed) sub phrases, to penalize
      // the scores of very long phrases that exist very rarely relative to the how often their
      // sub phrases exist in the index
      field_score *= ( ((double) phrase_conj_count) / max_sub_conj_count);

      return field_score;
    }

