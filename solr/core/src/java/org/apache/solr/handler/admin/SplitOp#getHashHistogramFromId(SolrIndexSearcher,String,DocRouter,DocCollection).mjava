  /**
   *   Returns a list of range counts sorted by the range lower bound, using the indexed "id" field (i.e. the terms are full IDs, not just prefixes)
   */
  static Collection<RangeCount> getHashHistogramFromId(SolrIndexSearcher searcher, String idField, DocRouter router, DocCollection collection) throws IOException {
    RTimer timer = new RTimer();

    TreeMap<DocRouter.Range, RangeCount> counts = new TreeMap<>();

    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), idField);
    if (terms == null) {
      return counts.values();
    }

    int numPrefixes = 0;
    int numCollisions = 0;
    long sumBuckets = 0;


    byte sep = (byte) CompositeIdRouter.SEPARATOR.charAt(0);
    TermsEnum termsEnum = terms.iterator();
    BytesRef currPrefix = new BytesRef();  // prefix of the previous "id" term
    int bucketCount = 0; // count of the number of docs in the current bucket

    // We're going to iterate over all terms, so do the minimum amount of work per term.
    // Terms are sorted, so all terms sharing a prefix will be grouped together.  The extra work
    // is really just limited to stepping over all the terms in the id field.
    for (;;) {
      BytesRef term = termsEnum.next();

      // compare to current prefix bucket and see if this new term shares the same prefix
      if (term != null && term.length >= currPrefix.length && currPrefix.length > 0) {
        if (StringHelper.startsWith(term, currPrefix)) {
          bucketCount++;  // use 1 since we are dealing with unique ids
          continue;
        }
      }

      // At this point the prefix did not match, so if we had a bucket we were working on, record it.
      if (currPrefix.length > 0) {
        numPrefixes++;
        sumBuckets += bucketCount;
        String currPrefixStr = currPrefix.utf8ToString();
        DocRouter.Range range = router.getSearchRangeSingle(currPrefixStr, null, collection);

        RangeCount rangeCount = new RangeCount(range, bucketCount);
        bucketCount = 0;

        RangeCount prev = counts.put(rangeCount.range, rangeCount);
        if (prev != null) {
          // we hit a hash collision, so add the buckets together.
          rangeCount.count += prev.count;
          numCollisions++;
        }
      }

      // if the current term is null, we ran out of values
      if (term == null) break;

      // find the new prefix (if any)

      // resize if needed
      if (currPrefix.length < term.length) {
        currPrefix.bytes = new byte[term.length+10];
      }

      // Copy the bytes up to and including the separator, and set the length if the separator is found.
      // If there was no separator, then length remains 0 and it's the indicator that we have no prefix bucket
      currPrefix.length = 0;
      for (int i=0; i<term.length; i++) {
        byte b = term.bytes[i + term.offset];
        currPrefix.bytes[i] = b;
        if (b == sep) {
          currPrefix.length = i + 1;
          bucketCount++;
          break;
        }
      }
    }

    if (log.isInfoEnabled()) {
      log.info("Split histogram from idField {}: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numCollisions={}"
          , idField, timer.getTime(), counts.size(), sumBuckets, numPrefixes, numCollisions);
    }

    return counts.values();
  }

