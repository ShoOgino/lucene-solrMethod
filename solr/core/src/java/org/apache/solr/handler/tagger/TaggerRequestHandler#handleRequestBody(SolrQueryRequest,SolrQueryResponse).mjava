  @Override
  public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {

    //--Read params
    final String indexedField = req.getParams().get("field");
    if (indexedField == null)
      throw new RuntimeException("required param 'field'");

    final TagClusterReducer tagClusterReducer =
            chooseTagClusterReducer(req.getParams().get(OVERLAPS));
    final int rows = req.getParams().getInt(CommonParams.ROWS, 10000);
    final int tagsLimit = req.getParams().getInt(TAGS_LIMIT, 1000);
    final boolean addMatchText = req.getParams().getBool(MATCH_TEXT, false);
    final SchemaField idSchemaField = req.getSchema().getUniqueKeyField();
    if (idSchemaField == null) {
      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "The tagger requires a" +
              "uniqueKey in the schema.");//TODO this could be relaxed
    }
    final boolean skipAltTokens = req.getParams().getBool(SKIP_ALT_TOKENS, false);
    final boolean ignoreStopWords = req.getParams().getBool(IGNORE_STOPWORDS,
            fieldHasIndexedStopFilter(indexedField, req));

    //--Get posted data
    Reader inputReader = null;
    Iterable<ContentStream> streams = req.getContentStreams();
    if (streams != null) {
      Iterator<ContentStream> iter = streams.iterator();
      if (iter.hasNext()) {
        inputReader = iter.next().getReader();
      }
      if (iter.hasNext()) {
        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
            getClass().getSimpleName()+" does not support multiple ContentStreams"); //TODO support bulk tagging?
      }
    }
    if (inputReader == null) {
      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
          getClass().getSimpleName()+" requires text to be POSTed to it");
    }

    // We may or may not need to read the input into a string
    final InputStringLazy inputStringFuture = new InputStringLazy(inputReader);

    final OffsetCorrector offsetCorrector = getOffsetCorrector(req.getParams(), inputStringFuture);

    final String inputString;//only populated if needed
    if (addMatchText || inputStringFuture.inputString != null) {
      //Read the input fully into a String buffer that we'll need later,
      // then replace the input with a reader wrapping the buffer.
      inputString = inputStringFuture.call();
      inputReader.close();
      inputReader = new StringReader(inputString);
    } else {
      inputString = null;//not used
    }

    final SolrIndexSearcher searcher = req.getSearcher();
    final FixedBitSet matchDocIdsBS = new FixedBitSet(searcher.maxDoc());
    @SuppressWarnings({"rawtypes"})
    final List tags = new ArrayList(2000);

    try {
      Analyzer analyzer = req.getSchema().getField(indexedField).getType().getQueryAnalyzer();
      try (TokenStream tokenStream = analyzer.tokenStream("", inputReader)) {
        Terms terms = searcher.getSlowAtomicReader().terms(indexedField);
        if (terms != null) {
          Tagger tagger = new Tagger(terms, computeDocCorpus(req), tokenStream, tagClusterReducer,
              skipAltTokens, ignoreStopWords) {
            @SuppressWarnings("unchecked")
            @Override
            protected void tagCallback(int startOffset, int endOffset, Object docIdsKey) {
              if (tags.size() >= tagsLimit)
                return;
              if (offsetCorrector != null) {
                int[] offsetPair = offsetCorrector.correctPair(startOffset, endOffset);
                if (offsetPair == null) {
                  log.debug("Discarded offsets [{}, {}] because couldn't balance XML.",
                      startOffset, endOffset);
                  return;
                }
                startOffset = offsetPair[0];
                endOffset = offsetPair[1];
              }

              @SuppressWarnings({"rawtypes"})
              NamedList tag = new NamedList();
              tag.add("startOffset", startOffset);
              tag.add("endOffset", endOffset);
              if (addMatchText)
                tag.add("matchText", inputString.substring(startOffset, endOffset));
              //below caches, and also flags matchDocIdsBS
              tag.add("ids", lookupSchemaDocIds(docIdsKey));
              tags.add(tag);
            }

            @SuppressWarnings({"rawtypes"})
            Map<Object, List> docIdsListCache = new HashMap<>(2000);

            ValueSourceAccessor uniqueKeyCache = new ValueSourceAccessor(searcher,
                idSchemaField.getType().getValueSource(idSchemaField, null));

            @SuppressWarnings({"unchecked", "rawtypes"})
            private List lookupSchemaDocIds(Object docIdsKey) {
              List schemaDocIds = docIdsListCache.get(docIdsKey);
              if (schemaDocIds != null)
                return schemaDocIds;
              IntsRef docIds = lookupDocIds(docIdsKey);
              //translate lucene docIds to schema ids
              schemaDocIds = new ArrayList<>(docIds.length);
              for (int i = docIds.offset; i < docIds.offset + docIds.length; i++) {
                int docId = docIds.ints[i];
                assert i == docIds.offset || docIds.ints[i - 1] < docId : "not sorted?";
                matchDocIdsBS.set(docId);//also, flip docid in bitset
                try {
                  schemaDocIds.add(uniqueKeyCache.objectVal(docId));//translates here
                } catch (IOException e) {
                  throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
                }
              }
              assert !schemaDocIds.isEmpty();

              docIdsListCache.put(docIds, schemaDocIds);
              return schemaDocIds;
            }

          };
          tagger.enableDocIdsCache(2000);//TODO configurable
          tagger.process();
        }
      }
    } finally {
      inputReader.close();
    }
    rsp.add("tagsCount",tags.size());
    rsp.add("tags", tags);

    rsp.setReturnFields(new SolrReturnFields( req ));

    //Solr's standard name for matching docs in response
    rsp.add("response", getDocList(rows, matchDocIdsBS));
  }

