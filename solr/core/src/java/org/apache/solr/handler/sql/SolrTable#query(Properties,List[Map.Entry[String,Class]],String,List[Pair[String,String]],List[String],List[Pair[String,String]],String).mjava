  /** Executes a Solr query on the underlying table.
   *
   * @param properties Connections properties
   * @param fields List of fields to project
   * @param query A string for the query
   * @return Enumerator of results
   */
  private Enumerable<Object> query(final Properties properties, final List<Map.Entry<String, Class>> fields,
                                   final String query, final List<Pair<String, String>> orders, final List<String> buckets,
                                   final List<Pair<String, String>> metricPairs, final String limit) {
    // SolrParams should be a ModifiableParams instead of a map
    ModifiableSolrParams solrParams = new ModifiableSolrParams();
    solrParams.add(CommonParams.OMIT_HEADER, "true");

    if (query == null) {
      solrParams.add(CommonParams.Q, DEFAULT_QUERY);
    } else {
      solrParams.add(CommonParams.Q, DEFAULT_QUERY + " AND " + query);
    }

    // List<String> doesn't have add so must make a new ArrayList
    List<String> fieldsList = new ArrayList<>(fields.size());
    fieldsList.addAll(fields.stream().map(Map.Entry::getKey).collect(Collectors.toList()));
    LinkedHashMap<String,String> ordersMap = new LinkedHashMap<>();
    for (Pair<String,String> order : orders) {
      ordersMap.put(order.getKey(), order.getValue());
    }
    List<Metric> metrics = buildMetrics(metricPairs);
    List<Bucket> bucketsList = buckets.stream().map(Bucket::new).collect(Collectors.toList());

    for(int i = buckets.size()-1; i >= 0; i--) {
      if (!ordersMap.containsKey(buckets.get(i))) {
        ordersMap.put(buckets.get(i), "asc");
      }
    }

    boolean isReOrder = false;

    for(Metric metric : metrics) {
      String metricIdentifier = metric.getIdentifier();

      ordersMap.remove(metricIdentifier);

      if(fieldsList.contains(metricIdentifier)) {
        fieldsList.remove(metricIdentifier);
        isReOrder = true;
      }

      for(String column : metric.getColumns()) {
        if (!fieldsList.contains(column)) {
          fieldsList.add(column);
        }

        if (!ordersMap.containsKey(column)) {
          ordersMap.put(column, "asc");
        }
      }
    }

    if (ordersMap.size() < 4) {
      ordersMap.put(DEFAULT_VERSION_FIELD, "desc");

      // Make sure the default sort field is in the field list
      if (!fieldsList.contains(DEFAULT_VERSION_FIELD)) {
        fieldsList.add(DEFAULT_VERSION_FIELD);
      }
    }

    if(!ordersMap.isEmpty()) {
      List<String> orderList = new ArrayList<>(ordersMap.size());
      for(Map.Entry<String, String> order : ordersMap.entrySet()) {
        String column = order.getKey();
        if(!fieldsList.contains(column)) {
          fieldsList.add(column);
        }
        orderList.add(column + " " + order.getValue());
      }
      solrParams.add(CommonParams.SORT, String.join(",", orderList));
    }

    if (fieldsList.isEmpty()) {
      solrParams.add(CommonParams.FL, "*");
    } else {
      solrParams.add(CommonParams.FL, String.join(",", fieldsList));
    }

    TupleStream tupleStream;
    String zk = properties.getProperty("zk");
    try {
      if (metrics.isEmpty() && bucketsList.isEmpty()) {
        solrParams.add(CommonParams.QT, "/export");
        if (limit != null) {
          solrParams.add(CommonParams.ROWS, limit);
          tupleStream = new LimitStream(new CloudSolrStream(zk, collection, solrParams), Integer.parseInt(limit));
        } else {
          tupleStream = new CloudSolrStream(zk, collection, solrParams);
        }
      } else {
        Metric[] metricsArray = metrics.toArray(new Metric[metrics.size()]);
        if(bucketsList.isEmpty()) {
          solrParams.remove(CommonParams.FL);
          solrParams.remove(CommonParams.SORT);
          tupleStream = new StatsStream(zk, collection, solrParams, metricsArray);
        } else {
          solrParams.add(CommonParams.QT, "/export");

          int numWorkers = Integer.parseInt(properties.getProperty("numWorkers", "1"));
          if (numWorkers > 1) solrParams.add("partitionKeys",String.join(",", buckets));

          tupleStream = new CloudSolrStream(zk, collection, solrParams);
          tupleStream = new RollupStream(tupleStream, bucketsList.toArray(new Bucket[bucketsList.size()]), metricsArray);

          if(numWorkers > 1) {
            String workerZkHost = properties.getProperty("workerZkhost");
            String workerCollection = properties.getProperty("workerCollection");
            // Do the rollups in parallel
            // Maintain the sort of the Tuples coming from the workers.
            StreamComparator comp = bucketSortComp(bucketsList, ordersMap);

            ParallelStream parallelStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);

            StreamFactory factory = new StreamFactory()
                .withFunctionName("search", CloudSolrStream.class)
                .withFunctionName("parallel", ParallelStream.class)
                .withFunctionName("rollup", RollupStream.class)
                .withFunctionName("sum", SumMetric.class)
                .withFunctionName("min", MinMetric.class)
                .withFunctionName("max", MaxMetric.class)
                .withFunctionName("avg", MeanMetric.class)
                .withFunctionName("count", CountMetric.class);

            parallelStream.setStreamFactory(factory);
            tupleStream = parallelStream;
            isReOrder = true;
          }

          if (isReOrder) {
            int limitVal = limit == null ? 100 : Integer.parseInt(limit);
            StreamComparator comp = getComp(orders);
            if (orders.isEmpty() && !ordersMap.isEmpty()) {
              // default order
              comp = getComp(new ArrayList<>(ordersMap.entrySet()));
            }
            tupleStream = new RankStream(tupleStream, limitVal, comp);
          } else {
            // Sort is the same as the same as the underlying stream
            // Only need to limit the result, not Rank the result
            if (limit != null) {
              solrParams.add(CommonParams.ROWS, limit);
              tupleStream = new LimitStream(new CloudSolrStream(zk, collection, solrParams), Integer.parseInt(limit));
            }
          }
        }
      }
    } catch (IOException e) {
      throw new RuntimeException(e);
    }

    final TupleStream finalStream = tupleStream;

    return new AbstractEnumerable<Object>() {
      // Use original fields list to make sure only the fields specified are enumerated
      public Enumerator<Object> enumerator() {
        return new SolrEnumerator(finalStream, fields);
      }
    };
  }

