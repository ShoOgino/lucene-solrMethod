  /*
   * Returns a list of range counts sorted by the range lower bound
   */
  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {
    RTimer timer = new RTimer();
    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();

    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);
    if (terms == null) {
      return counts.values();
    }

    int numPrefixes = 0;
    int numTriLevel = 0;
    int numCollisions = 0;
    long sumBuckets = 0;

    TermsEnum termsEnum = terms.iterator();
    BytesRef term;
    while ((term = termsEnum.next()) != null) {
      numPrefixes++;

      String termStr = term.utf8ToString();
      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);
      // truncate to first separator since we don't support multiple levels currently
      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id
      // and 8 for a 3 part id!
      if (firstSep != termStr.length()-1 && firstSep > 0) {
        numTriLevel++;
        termStr = termStr.substring(0, firstSep+1);
      }

      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);
      int numDocs = termsEnum.docFreq();
      sumBuckets += numDocs;

      RangeCount rangeCount = new RangeCount(range, numDocs);

      RangeCount prev = counts.put(rangeCount.range, rangeCount);
      if (prev != null) {
        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.
        rangeCount.count += prev.count;
        numCollisions++;
      }
    }

    log.info("Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);

    return counts.values();
  }

