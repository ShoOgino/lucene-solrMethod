  private void processBackupAction(ZkNodeProps message, NamedList results) throws IOException, KeeperException, InterruptedException {
    String collectionName =  message.getStr(COLLECTION_PROP);
    String backupName =  message.getStr(NAME);
    String location = message.getStr(ZkStateReader.BACKUP_LOCATION);
    ShardHandler shardHandler = shardHandlerFactory.getShardHandler();
    String asyncId = message.getStr(ASYNC);
    Map<String, String> requestMap = new HashMap<>();
    Instant startTime = Instant.now();

    // note: we assume a shared files system to backup a collection, since a collection is distributed
    Path backupPath = Paths.get(location).resolve(backupName).toAbsolutePath();

    //Validating if the directory already exists.
    if (Files.exists(backupPath)) {
      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
          "Backup directory already exists: " + backupPath);
    }
    Files.createDirectory(backupPath); // create now

    log.info("Starting backup of collection={} with backupName={} at location={}", collectionName, backupName,
        backupPath);

    for (Slice slice : zkStateReader.getClusterState().getCollection(collectionName).getActiveSlices()) {
      Replica replica = slice.getLeader();

      String coreName = replica.getStr(CORE_NAME_PROP);

      ModifiableSolrParams params = new ModifiableSolrParams();
      params.set(CoreAdminParams.ACTION, CoreAdminAction.BACKUPCORE.toString());
      params.set(NAME, slice.getName());
      params.set("location", backupPath.toString()); // note: index dir will be here then the "snapshot." + slice name
      params.set(CORE_NAME_PROP, coreName);

      sendShardRequest(replica.getNodeName(), params, shardHandler, asyncId, requestMap);
      log.debug("Sent backup request to core={} for backupName={}", coreName, backupName);
    }
    log.debug("Sent backup requests to all shard leaders for backupName={}", backupName);

    processResponses(results, shardHandler, true, "Could not backup all replicas", asyncId, requestMap);

    log.info("Starting to backup ZK data for backupName={}", backupName);

    //Download the configs
    String configName = zkStateReader.readConfigName(collectionName);
    Path zkBackup =  backupPath.resolve("zk_backup");
    zkStateReader.getConfigManager().downloadConfigDir(configName, zkBackup.resolve("configs").resolve(configName));

    //Save the collection's state. Can be part of the monolithic clusterstate.json or a individual state.json
    //Since we don't want to distinguish we extract the state and back it up as a separate json
    DocCollection collection = zkStateReader.getClusterState().getCollection(collectionName);
    Files.write(zkBackup.resolve("collection_state.json"),
        Utils.toJSON(Collections.singletonMap(collectionName, collection)));

    Path propertiesPath = backupPath.resolve("backup.properties");
    Properties properties = new Properties();

    properties.put("backupName", backupName);
    properties.put("collection", collectionName);
    properties.put("collection.configName", configName);
    properties.put("startTime", startTime.toString());
    //TODO: Add MD5 of the configset. If during restore the same name configset exists then we can compare checksums to see if they are the same.
    //if they are not the same then we can throw an error or have an 'overwriteConfig' flag
    //TODO save numDocs for the shardLeader. We can use it to sanity check the restore.

    try (Writer os = Files.newBufferedWriter(propertiesPath, StandardCharsets.UTF_8)) {
      properties.store(os, "Snapshot properties file");
    }

    log.info("Completed backing up ZK data for backupName={}", backupName);
  }

