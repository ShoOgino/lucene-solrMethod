  /**
   * Register shard with ZooKeeper.
   * 
   * @param coreName
   * @param desc
   * @param recoverReloadedCores
   * @return the shardId for the SolrCore
   * @throws Exception
   */
  public String register(String coreName, final CoreDescriptor desc, boolean recoverReloadedCores) throws Exception {  
    final String baseUrl = getBaseUrl();
    
    final CloudDescriptor cloudDesc = desc.getCloudDescriptor();
    final String collection = cloudDesc.getCollectionName();

    final String coreZkNodeName = getNodeName() + "_" + coreName;
    
    String shardId = cloudDesc.getShardId();

    Map<String,String> props = new HashMap<String,String>();
 // we only put a subset of props into the leader node
    props.put(ZkStateReader.BASE_URL_PROP, baseUrl);
    props.put(ZkStateReader.CORE_NAME_PROP, coreName);
    props.put(ZkStateReader.NODE_NAME_PROP, getNodeName());


    if (log.isInfoEnabled()) {
        log.info("Register shard - core:" + coreName + " address:"
            + baseUrl + " shardId:" + shardId);
    }

    ZkNodeProps leaderProps = new ZkNodeProps(props);
    
    // rather than look in the cluster state file, we go straight to the zknodes
    // here, because on cluster restart there could be stale leader info in the
    // cluster state node that won't be updated for a moment
    String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();
    
    // now wait until our currently cloud state contains the latest leader
    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, cloudDesc.getShardId(), 30000);
    int tries = 0;
    while (!leaderUrl.equals(cloudStateLeader)) {
      if (tries == 60) {
        throw new SolrException(ErrorCode.SERVER_ERROR,
            "There is conflicting information about the leader of shard: "
                + cloudDesc.getShardId());
      }
      Thread.sleep(1000);
      tries++;
      cloudStateLeader = zkStateReader.getLeaderUrl(collection,
          cloudDesc.getShardId(), 30000);
    }
    
    String ourUrl = ZkCoreNodeProps.getCoreUrl(baseUrl, coreName);
    log.info("We are " + ourUrl + " and leader is " + leaderUrl);
    boolean isLeader = leaderUrl.equals(ourUrl);
    

    SolrCore core = null;
    if (cc != null) { // CoreContainer only null in tests
      try {
        core = cc.getCore(desc.getName());

        if (isLeader) {
          // recover from local transaction log and wait for it to complete before
          // going active
          // TODO: should this be moved to another thread? To recoveryStrat?
          // TODO: should this actually be done earlier, before (or as part of)
          // leader election perhaps?
          // TODO: ensure that a replica that is trying to recover waits until I'm
          // active (or don't make me the
          // leader until my local replay is done. But this replay is only needed
          // on the leader - replicas
          // will do recovery anyway
          
          UpdateLog ulog = core.getUpdateHandler().getUpdateLog();
          if (!core.isReloaded() && ulog != null) {
            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()
                .getUpdateLog().recoverFromLog();
            if (recoveryFuture != null) {
              recoveryFuture.get(); // NOTE: this could potentially block for
                                    // minutes or more!
              // TODO: public as recovering in the mean time?
            }
          }
        }
        
        boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,
            collection, coreZkNodeName, shardId, leaderProps, core, cc);
        if (!didRecovery) {
          publishAsActive(baseUrl, desc, coreZkNodeName, coreName);
        }
      } finally {
        if (core != null) {
          core.close();
        }
      }
    } else {
      publishAsActive(baseUrl, desc, coreZkNodeName, coreName);
    }
    
    // make sure we have an update cluster state right away
    zkStateReader.updateCloudState(true);

    return shardId;
  }

