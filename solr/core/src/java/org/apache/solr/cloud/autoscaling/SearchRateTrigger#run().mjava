  @Override
  public void run() {
    AutoScaling.TriggerEventProcessor processor = processorRef.get();
    if (processor == null) {
      return;
    }

    // collection, shard, list(replica + rate)
    Map<String, Map<String, List<ReplicaInfo>>> collectionRates = new HashMap<>();
    // node, rate
    Map<String, AtomicDouble> nodeRates = new HashMap<>();
    // this replication factor only considers replica types that are searchable
    // collection, shard, RF
    Map<String, Map<String, AtomicInteger>> searchableReplicationFactors = new HashMap<>();

    ClusterState clusterState = null;
    try {
      clusterState = cloudManager.getClusterStateProvider().getClusterState();
    } catch (IOException e) {
      log.warn("Error getting ClusterState", e);
      return;
    }
    for (String node : cloudManager.getClusterStateProvider().getLiveNodes()) {
      Map<String, ReplicaInfo> metricTags = new HashMap<>();
      // coll, shard, replica
      Map<String, Map<String, List<ReplicaInfo>>> infos = cloudManager.getNodeStateProvider().getReplicaInfo(node, Collections.emptyList());
      infos.forEach((coll, shards) -> {
        Map<String, AtomicInteger> replPerShard = searchableReplicationFactors.computeIfAbsent(coll, c -> new HashMap<>());
        shards.forEach((sh, replicas) -> {
          AtomicInteger repl = replPerShard.computeIfAbsent(sh, s -> new AtomicInteger());
          replicas.forEach(replica -> {
            // skip non-active replicas
            if (replica.getState() != Replica.State.ACTIVE) {
              return;
            }
            repl.incrementAndGet();
            // we have to translate to the metrics registry name, which uses "_replica_nN" as suffix
            String replicaName = Utils.parseMetricsReplicaName(coll, replica.getCore());
            if (replicaName == null) { // should never happen???
              replicaName = replica.getName(); // which is actually coreNode name...
            }
            String registry = SolrCoreMetricManager.createRegistryName(true, coll, sh, replicaName, null);
            String tag = "metrics:" + registry + ":" + metric;
            metricTags.put(tag, replica);
          });
        });
      });
      if (metricTags.isEmpty()) {
        continue;
      }
      Map<String, Object> rates = cloudManager.getNodeStateProvider().getNodeValues(node, metricTags.keySet());
      if (log.isDebugEnabled()) {
        log.debug("### rates for node {}", node);
        rates.forEach((tag, rate) -> log.debug("###  " + tag + "\t" + rate)); // logOk
      }
      rates.forEach((tag, rate) -> {
        ReplicaInfo info = metricTags.get(tag);
        if (info == null) {
          log.warn("Missing replica info for response tag {}", tag);
        } else {
          Map<String, List<ReplicaInfo>> perCollection = collectionRates.computeIfAbsent(info.getCollection(), s -> new HashMap<>());
          List<ReplicaInfo> perShard = perCollection.computeIfAbsent(info.getShard(), s -> new ArrayList<>());
          info = (ReplicaInfo)info.clone();
          info.getVariables().put(AutoScalingParams.RATE, ((Number)rate).doubleValue());
          perShard.add(info);
          AtomicDouble perNode = nodeRates.computeIfAbsent(node, s -> new AtomicDouble());
          perNode.addAndGet(((Number)rate).doubleValue());
        }
      });
    }

    if (log.isDebugEnabled()) {
      collectionRates.forEach((coll, collRates) -> {
        log.debug("## Collection: {}", coll);
        collRates.forEach((s, replicas) -> {
          log.debug("##  - {}", s);
          replicas.forEach(ri -> log.debug("##     {}  {}", ri.getCore(), ri.getVariable(AutoScalingParams.RATE))); //logOk
        });
      });
    }
    long now = cloudManager.getTimeSource().getTimeNs();
    Map<String, Double> hotNodes = new HashMap<>();
    Map<String, Double> coldNodes = new HashMap<>();

    // check for exceeded rates and filter out those with less than waitFor from previous events
    nodeRates.entrySet().stream()
        .filter(entry -> node.equals(Policy.ANY) || node.equals(entry.getKey()))
        .forEach(entry -> {
          if (entry.getValue().get() > aboveNodeRate) {
            if (waitForElapsed(entry.getKey(), now, lastNodeEvent)) {
              hotNodes.put(entry.getKey(), entry.getValue().get());
            }
          } else if (entry.getValue().get() < belowNodeRate) {
            if (waitForElapsed(entry.getKey(), now, lastNodeEvent)) {
              coldNodes.put(entry.getKey(), entry.getValue().get());
            }
          } else {
            // no violation - clear waitForElapsed
            // (violation is only valid if it persists throughout waitFor)
            lastNodeEvent.remove(entry.getKey());
          }
        });

    Map<String, Map<String, Double>> hotShards = new HashMap<>();
    Map<String, Map<String, Double>> coldShards = new HashMap<>();
    List<ReplicaInfo> hotReplicas = new ArrayList<>();
    List<ReplicaInfo> coldReplicas = new ArrayList<>();
    collectionRates.forEach((coll, shardRates) -> {
      shardRates.forEach((sh, replicaRates) -> {
        double totalShardRate = replicaRates.stream()
            .map(r -> {
              String elapsedKey = r.getCollection() + "." + r.getCore();
              if ((Double)r.getVariable(AutoScalingParams.RATE) > aboveRate) {
                if (waitForElapsed(elapsedKey, now, lastReplicaEvent)) {
                  hotReplicas.add(r);
                }
              } else if ((Double)r.getVariable(AutoScalingParams.RATE) < belowRate) {
                if (waitForElapsed(elapsedKey, now, lastReplicaEvent)) {
                  coldReplicas.add(r);
                }
              } else {
                // no violation - clear waitForElapsed
                lastReplicaEvent.remove(elapsedKey);
              }
              return r;
            })
            .mapToDouble(r -> (Double)r.getVariable(AutoScalingParams.RATE)).sum();
        // calculate average shard rate over all searchable replicas (see SOLR-12470)
        double shardRate = totalShardRate / searchableReplicationFactors.get(coll).get(sh).doubleValue();
        String elapsedKey = coll + "." + sh;
        log.debug("-- {}: totalShardRate={}, shardRate={}", elapsedKey, totalShardRate, shardRate);
        if ((collections.isEmpty() || collections.contains(coll)) &&
            (shard.equals(Policy.ANY) || shard.equals(sh))) {
          if (shardRate > aboveRate) {
            if (waitForElapsed(elapsedKey, now, lastShardEvent)) {
              hotShards.computeIfAbsent(coll, s -> new HashMap<>()).put(sh, shardRate);
            }
          } else if (shardRate < belowRate) {
            if (waitForElapsed(elapsedKey, now, lastShardEvent)) {
              coldShards.computeIfAbsent(coll, s -> new HashMap<>()).put(sh, shardRate);
              log.debug("-- coldShard waitFor elapsed {}", elapsedKey);
            } else {
              if (log.isDebugEnabled()) {
                Long lastTime = lastShardEvent.computeIfAbsent(elapsedKey, s -> now);
                long elapsed = TimeUnit.SECONDS.convert(now - lastTime, TimeUnit.NANOSECONDS);
                if (log.isDebugEnabled()) {
                  log.debug("-- waitFor didn't elapse for {}, waitFor={}, elapsed={}", elapsedKey, getWaitForSecond(), elapsed);
                }
              }
            }
          } else {
            // no violation - clear waitForElapsed
            lastShardEvent.remove(elapsedKey);
          }
        }
      });
    });

    Map<String, Double> hotCollections = new HashMap<>();
    Map<String, Double> coldCollections = new HashMap<>();
    collectionRates.forEach((coll, shardRates) -> {
      double total = shardRates.entrySet().stream()
          .mapToDouble(e -> e.getValue().stream()
              .mapToDouble(r -> (Double)r.getVariable(AutoScalingParams.RATE)).sum()).sum();
      if (collections.isEmpty() || collections.contains(coll)) {
        if (total > aboveRate) {
          if (waitForElapsed(coll, now, lastCollectionEvent)) {
            hotCollections.put(coll, total);
          }
        } else if (total < belowRate) {
          if (waitForElapsed(coll, now, lastCollectionEvent)) {
            coldCollections.put(coll, total);
          }
        } else {
          // no violation - clear waitForElapsed
          lastCollectionEvent.remove(coll);
        }
      }
    });

    if (hotCollections.isEmpty() &&
        hotShards.isEmpty() &&
        hotReplicas.isEmpty() &&
        hotNodes.isEmpty() &&
        coldCollections.isEmpty() &&
        coldShards.isEmpty() &&
        coldReplicas.isEmpty() &&
        coldNodes.isEmpty()) {
      return;
    }

    // generate event

    // find the earliest time when a condition was exceeded
    final AtomicLong eventTime = new AtomicLong(now);
    hotCollections.forEach((c, r) -> {
      long time = lastCollectionEvent.get(c);
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });
    coldCollections.forEach((c, r) -> {
      long time = lastCollectionEvent.get(c);
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });
    hotShards.forEach((c, shards) -> {
      shards.forEach((s, r) -> {
        long time = lastShardEvent.get(c + "." + s);
        if (eventTime.get() > time) {
          eventTime.set(time);
        }
      });
    });
    coldShards.forEach((c, shards) -> {
      shards.forEach((s, r) -> {
        long time = lastShardEvent.get(c + "." + s);
        if (eventTime.get() > time) {
          eventTime.set(time);
        }
      });
    });
    hotReplicas.forEach(r -> {
      long time = lastReplicaEvent.get(r.getCollection() + "." + r.getCore());
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });
    coldReplicas.forEach(r -> {
      long time = lastReplicaEvent.get(r.getCollection() + "." + r.getCore());
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });
    hotNodes.forEach((n, r) -> {
      long time = lastNodeEvent.get(n);
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });
    coldNodes.forEach((n, r) -> {
      long time = lastNodeEvent.get(n);
      if (eventTime.get() > time) {
        eventTime.set(time);
      }
    });

    final List<TriggerEvent.Op> ops = new ArrayList<>();
    final Set<String> violations = new HashSet<>();

    calculateHotOps(ops, violations, searchableReplicationFactors, hotNodes, hotCollections, hotShards, hotReplicas);
    calculateColdOps(ops, violations, clusterState, searchableReplicationFactors, coldNodes, coldCollections, coldShards, coldReplicas);

    if (ops.isEmpty()) {
      return;
    }

    if (processor.process(new SearchRateEvent(getName(), eventTime.get(), ops,
        hotNodes, hotCollections, hotShards, hotReplicas,
        coldNodes, coldCollections, coldShards, coldReplicas, violations))) {
      // update lastEvent times
      hotNodes.keySet().forEach(node -> lastNodeEvent.put(node, now));
      coldNodes.keySet().forEach(node -> lastNodeEvent.put(node, now));
      hotCollections.keySet().forEach(coll -> lastCollectionEvent.put(coll, now));
      coldCollections.keySet().forEach(coll -> lastCollectionEvent.put(coll, now));
      hotShards.entrySet().forEach(e -> e.getValue()
          .forEach((sh, rate) -> lastShardEvent.put(e.getKey() + "." + sh, now)));
      coldShards.entrySet().forEach(e -> e.getValue()
          .forEach((sh, rate) -> lastShardEvent.put(e.getKey() + "." + sh, now)));
      hotReplicas.forEach(r -> lastReplicaEvent.put(r.getCollection() + "." + r.getCore(), now));
      coldReplicas.forEach(r -> lastReplicaEvent.put(r.getCollection() + "." + r.getCore(), now));
    }
  }

