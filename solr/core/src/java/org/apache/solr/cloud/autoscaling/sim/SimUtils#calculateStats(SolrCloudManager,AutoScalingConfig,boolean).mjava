  /**
   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.
   * @param cloudManager manager
   * @param config autoscaling config, or null if the one from the provided manager should be used
   * @param verbose if true then add more details about replicas.
   * @return a map containing detailed statistics
   */
  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {
    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();
    Map<String, Map<String, Number>> collStats = new TreeMap<>();
    Policy.Session session = config.getPolicy().createSession(cloudManager);
    clusterState.forEachCollection(coll -> {
      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());
      AtomicInteger numCores = new AtomicInteger();
      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();
      coll.getSlices().forEach(s -> {
        numCores.addAndGet(s.getReplicas().size());
        s.getReplicas().forEach(r -> {
          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())
              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();
        });
      });
      int maxCoresPerNode = 0;
      int minCoresPerNode = 0;
      int maxActualShardsPerNode = 0;
      int minActualShardsPerNode = 0;
      int maxShardReplicasPerNode = 0;
      int minShardReplicasPerNode = 0;
      if (!nodes.isEmpty()) {
        minCoresPerNode = Integer.MAX_VALUE;
        minActualShardsPerNode = Integer.MAX_VALUE;
        minShardReplicasPerNode = Integer.MAX_VALUE;
        for (Map<String, AtomicInteger> counts : nodes.values()) {
          int total = counts.values().stream().mapToInt(c -> c.get()).sum();
          for (AtomicInteger count : counts.values()) {
            if (count.get() > maxShardReplicasPerNode) {
              maxShardReplicasPerNode = count.get();
            }
            if (count.get() < minShardReplicasPerNode) {
              minShardReplicasPerNode = count.get();
            }
          }
          if (total > maxCoresPerNode) {
            maxCoresPerNode = total;
          }
          if (total < minCoresPerNode) {
            minCoresPerNode = total;
          }
          if (counts.size() > maxActualShardsPerNode) {
            maxActualShardsPerNode = counts.size();
          }
          if (counts.size() < minActualShardsPerNode) {
            minActualShardsPerNode = counts.size();
          }
        }
      }
      perColl.put("activeShards", coll.getActiveSlices().size());
      perColl.put("inactiveShards", coll.getSlices().size() - coll.getActiveSlices().size());
      perColl.put("rf", coll.getReplicationFactor());
      perColl.put("maxActualShardsPerNode", maxActualShardsPerNode);
      perColl.put("minActualShardsPerNode", minActualShardsPerNode);
      perColl.put("maxShardReplicasPerNode", maxShardReplicasPerNode);
      perColl.put("minShardReplicasPerNode", minShardReplicasPerNode);
      perColl.put("numCores", numCores.get());
      perColl.put("numNodes", nodes.size());
      perColl.put("maxCoresPerNode", maxCoresPerNode);
      perColl.put("minCoresPerNode", minCoresPerNode);
    });
    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();
    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();
    List<Row> rows = session.getSortedNodes();
    // check consistency
    if (rows.size() != clusterState.getLiveNodes().size()) {
      throw new Exception("Mismatch between autoscaling matrix size (" + rows.size() + ") and liveNodes size (" + clusterState.getLiveNodes().size() + ")");
    }
    for (Row row : rows) {
      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());
      nodeStat.put("isLive", row.isLive());
      for (Cell cell : row.getCells()) {
        nodeStat.put(cell.getName(), cell.getValue());
      }
//      nodeStat.put("freedisk", row.getVal("freedisk", 0));
//      nodeStat.put("totaldisk", row.getVal("totaldisk", 0));
      int cores = ((Number)row.getVal("cores", 0)).intValue();
//      nodeStat.put("cores", cores);
      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();
      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();
      // check consistency
      AtomicInteger rowCores = new AtomicInteger();
      row.forEachReplica(ri -> rowCores.incrementAndGet());
      if (cores != rowCores.get()) {
        throw new Exception("Mismatch between autoscaling matrix row replicas (" + rowCores.get() + ") and number of cores (" + cores + ")");
      }
      row.forEachReplica(ri -> {
        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())
            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());
//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {
//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));
//            }
        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {
          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));
          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {
            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));
          } else {
            perReplica.put(Variable.Type.CORE_IDX.tagName,
                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));
          }
        }
        perReplica.put("coreNode", ri.getName());
        if (ri.isLeader || ri.getBool("leader", false)) {
          perReplica.put("leader", true);
          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())
              .computeIfAbsent("avgShardSize", size -> 0.0);
          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);
          if (riSize != null) {
            totalSize += riSize.doubleValue();
            collStats.get(ri.getCollection()).put("avgShardSize", totalSize);
            Double max = (Double)collStats.get(ri.getCollection()).get("maxShardSize");
            if (max == null) max = 0.0;
            if (riSize.doubleValue() > max) {
              collStats.get(ri.getCollection()).put("maxShardSize", riSize.doubleValue());
            }
            Double min = (Double)collStats.get(ri.getCollection()).get("minShardSize");
            if (min == null) min = Double.MAX_VALUE;
            if (riSize.doubleValue() < min) {
              collStats.get(ri.getCollection()).put("minShardSize", riSize.doubleValue());
            }
          } else {
            throw new RuntimeException("ReplicaInfo without size information: " + ri);
          }
        }
        if (verbose) {
          nodeStat.put("replicas", collReplicas);
        }
      });
    }

    // calculate average per shard and convert the units
    for (Map<String, Number> perColl : collStats.values()) {
      Number avg = perColl.get("avgShardSize");
      if (avg != null) {
        avg = avg.doubleValue() / perColl.get("activeShards").doubleValue();
        perColl.put("avgShardSize", (Number)Variable.Type.CORE_IDX.convertVal(avg));
      }
      Number num = perColl.get("maxShardSize");
      if (num != null) {
        perColl.put("maxShardSize", (Number)Variable.Type.CORE_IDX.convertVal(num));
      }
      num = perColl.get("minShardSize");
      if (num != null) {
        perColl.put("minShardSize", (Number)Variable.Type.CORE_IDX.convertVal(num));
      }
    }
    Map<String, Object> stats = new LinkedHashMap<>();
    stats.put("coresPerNodes", coreStats);
    stats.put("sortedNodeStats", nodeStats);
    stats.put("collectionStats", collStats);
    return stats;
  }

