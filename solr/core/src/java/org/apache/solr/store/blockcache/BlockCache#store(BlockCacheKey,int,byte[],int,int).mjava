  /**
   * This is only best-effort... it's possible for false to be returned.
   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.
   *
   * @param blockCacheKey the key for the block
   * @param blockOffset the offset within the block
   * @param data source data to write to the block
   * @param offset offset within the source data array
   * @param length the number of bytes to write.
   * @return true if the block was cached/updated
   */
  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,
      byte[] data, int offset, int length) {
    if (length + blockOffset > blockSize) {
      throw new RuntimeException("Buffer size exceeded, expecting max ["
          + blockSize + "] got length [" + length + "] with blockOffset ["
          + blockOffset + "]");
    }
    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
    boolean newLocation = false;
    if (location == null) {
      newLocation = true;
      location = new BlockCacheLocation();
      if (!findEmptyLocation(location)) {
        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing
        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.
        // TODO: simplest fix would be to leave more than one block empty
        return false;
      }
    }

    // YCS: I think this means that the block existed, but it is in the process of being
    // concurrently removed.  This flag is set in the releaseLocation eviction listener.
    if (location.isRemoved()) {
      return false;
    }

    int bankId = location.getBankId();
    int bankOffset = location.getBlock() * blockSize;
    ByteBuffer bank = getBank(bankId);
    bank.position(bankOffset + blockOffset);
    bank.put(data, offset, length);
    if (newLocation) {
      cache.put(blockCacheKey.clone(), location);
      metrics.blockCacheSize.incrementAndGet();
    }
    return true;
  }

