  /**
   * This is only best-effort... it's possible for false to be returned, meaning the block was not able to be cached.
   * NOTE: blocks may not currently be updated (false will be returned if the block is already cached)
   * The blockCacheKey is cloned before it is inserted into the map, so it may be reused by clients if desired.
   *
   * @param blockCacheKey the key for the block
   * @param blockOffset the offset within the block
   * @param data source data to write to the block
   * @param offset offset within the source data array
   * @param length the number of bytes to write.
   * @return true if the block was cached/updated
   */
  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,
      byte[] data, int offset, int length) {
    if (length + blockOffset > blockSize) {
      throw new RuntimeException("Buffer size exceeded, expecting max ["
          + blockSize + "] got length [" + length + "] with blockOffset ["
          + blockOffset + "]");
    }
    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
    if (location == null) {
      location = new BlockCacheLocation();
      if (!findEmptyLocation(location)) {
        // YCS: it looks like when the cache is full (a normal scenario), then two concurrent writes will result in one of them failing
        // because no eviction is done first.  The code seems to rely on leaving just a single block empty.
        // TODO: simplest fix would be to leave more than one block empty
        return false;
      }
    } else {
      // If we allocated a new block, then it has never been published and is thus never in danger of being concurrently removed.
      // On the other hand, if this is an existing block we are updating, it may concurrently be removed and reused for another
      // purpose (and then our write may overwrite that).  This can happen even if clients never try to update existing blocks,
      // since two clients can try to cache the same block concurrently.  Because of this, the ability to update an existing
      // block has been removed for the time being (see SOLR-10121).
      return false;
    }

    int bankId = location.getBankId();
    int bankOffset = location.getBlock() * blockSize;
    ByteBuffer bank = getBank(bankId);
    bank.position(bankOffset + blockOffset);
    bank.put(data, offset, length);

    // make sure all modifications to the block have been completed before we publish it.
    cache.put(blockCacheKey.clone(), location);
    metrics.blockCacheSize.incrementAndGet();
    return true;
  }

