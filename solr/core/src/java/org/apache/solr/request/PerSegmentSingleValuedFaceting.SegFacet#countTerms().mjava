    void countTerms() throws IOException {
      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);
      // SolrCore.log.info("reader= " + reader + "  FC=" + System.identityHashCode(si));

      if (prefix!=null) {
        BytesRef prefixRef = new BytesRef(prefix);
        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);
        if (startTermIndex<0) startTermIndex=-startTermIndex-1;
        prefixRef.append(UnicodeUtil.BIG_TERM);
        // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end
        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);
        assert endTermIndex < 0;
        endTermIndex = -endTermIndex-1;
      } else {
        startTermIndex=0;
        endTermIndex=si.numOrd();
      }

      final int nTerms=endTermIndex-startTermIndex;
      if (nTerms>0) {
        // count collection array only needs to be as big as the number of terms we are
        // going to collect counts for.
        final int[] counts = this.counts = new int[nTerms];
        DocIdSet idSet = baseSet.getDocIdSet(context, null);  // this set only includes live docs
        DocIdSetIterator iter = idSet.iterator();


        ////
        PackedInts.Reader ordReader = si.getDocToOrd();
        int doc;

        final Object arr;
        if (ordReader.hasArray()) {
          arr = ordReader.getArray();
        } else {
          arr = null;
        }

        if (arr instanceof int[]) {
          int[] ords = (int[]) arr;
          if (prefix==null) {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              counts[ords[doc]]++;
            }
          } else {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              int term = ords[doc];
              int arrIdx = term-startTermIndex;
              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
            }
          }
        } else if (arr instanceof short[]) {
          short[] ords = (short[]) arr;
          if (prefix==null) {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              counts[ords[doc] & 0xffff]++;
            }
          } else {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              int term = ords[doc] & 0xffff;
              int arrIdx = term-startTermIndex;
              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
            }
          }
        } else if (arr instanceof byte[]) {
          byte[] ords = (byte[]) arr;
          if (prefix==null) {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              counts[ords[doc] & 0xff]++;
            }
          } else {
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              int term = ords[doc] & 0xff;
              int arrIdx = term-startTermIndex;
              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
            }
          }
        } else {
          if (prefix==null) {
            // specialized version when collecting counts for all terms
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              counts[si.getOrd(doc)]++;
            }
          } else {
            // version that adjusts term numbers because we aren't collecting the full range
            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
              int term = si.getOrd(doc);
              int arrIdx = term-startTermIndex;
              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
            }
          }
        }
      }
    }

