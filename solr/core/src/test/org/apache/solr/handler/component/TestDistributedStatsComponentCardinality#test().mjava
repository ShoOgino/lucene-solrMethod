  public void test() throws Exception {
    buildIndex();
    
    { // simple sanity checks - don't leak variables
      QueryResponse rsp = null;
      rsp = query(params("rows", "0", "q", "id:42")); 
      assertEquals(1, rsp.getResults().getNumFound());
      
      rsp = query(params("rows", "0", "q", "*:*", 
                         "stats","true", "stats.field", "{!min=true max=true}long_l"));
      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());
      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get("long_l").getMin()));
      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get("long_l").getMax()));
    }

    final int NUM_QUERIES = atLeast(100);

    // Some Randomized queries with randomized log2m and max regwidth
    for (int i = 0; i < NUM_QUERIES; i++) {

      // testing shows that on random data, at the size we're dealing with, 
      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the 
      // the theoretically expected relative error.
      //
      // So we have to use a slightly higher lower bound on what log2m values we randomly test
      final int log2m = TestUtil.nextInt(random(), 
                                         2 + HLL.MINIMUM_LOG2M_PARAM, 
                                         HLL.MAXIMUM_LOG2M_PARAM);

      // use max regwidth to try and prevent hash collisions from introducing problems
      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;

      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);
      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);
      final int numMatches = 1+highId-lowId;

      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);
      QueryResponse rsp = query(p);
      assertEquals("sanity check num matches, p="+p, numMatches, rsp.getResults().getNumFound());

      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();

      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {
        log.warn("SOLR-10918: can't relying on exact match with pre-hashed values when using points");
      } else {
        for (String f : STAT_FIELDS) {
          // regardless of log2m and regwidth, the estimated cardinality of the 
          // hashed vs prehashed values should be exactly the same for each field
          
          assertEquals(f + ": hashed vs prehashed, real="+ numMatches + ", p=" + p,
                       stats.get(f).getCardinality().longValue(),
                       stats.get(f+"_prehashed_l").getCardinality().longValue());
        }
      }

      for (String f : STAT_FIELDS) {
        // check the relative error of the estimate returned against the known truth

        final double relErr = expectedRelativeError(log2m);
        final long estimate = stats.get(f).getCardinality().longValue();
        assertTrue(f + ": relativeErr="+relErr+", estimate="+estimate+", real="+numMatches+", p=" + p,
                   (Math.abs(numMatches - estimate) / numMatches) < relErr);
        
      }
    }
    
    // Some Randomized queries with both low and high accuracy options
    for (int i = 0; i < NUM_QUERIES; i++) {

      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);
      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);
      final int numMatches = 1+highId-lowId;

      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15
      // 
      // aparently we can't rely on estimates always being more accurate with higher log2m values?
      // so for now, just try testing accuracy values that differ by at least 0.5
      //
      // (that should give us a significant enough log2m diff that the "highAccuracy" is always
      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)
      // 
      final double lowAccuracy = random().nextDouble() / 2;
      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));
      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);

      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);
      QueryResponse rsp = query(p);
      assertEquals("sanity check num matches, p="+p, numMatches, rsp.getResults().getNumFound());

      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();

      // can't use STAT_FIELDS here ...
      //
      // hueristic differences for regwidth on 32 bit values mean we get differences 
      // between estimates for the normal field vs the prehashed (long) field
      //
      // so we settle for only testing things where the regwidth is consistent 
      // w/the prehashed long...
      for (String f : new String[] { "long_l", "string_s" }) {

        // regardless of accuracy, the estimated cardinality of the 
        // hashed vs prehashed values should be exactly the same for each field

        assertEquals(f + ": hashed vs prehashed (low), real="+ numMatches + ", p=" + p,
                     stats.get("low_"+f).getCardinality().longValue(),
                     stats.get("low_"+f+"_prehashed_l").getCardinality().longValue());
        assertEquals(f + ": hashed vs prehashed (high), real="+ numMatches + ", p=" + p,
                     stats.get("high_"+f).getCardinality().longValue(),
                     stats.get("high_"+f+"_prehashed_l").getCardinality().longValue());
      }
      
      for (String f : STAT_FIELDS) {
        for (String ff : new String[] { f, f+"_prehashed_l"}) {
          // for both the prehashed and regular fields, the high accuracy option 
          // should always produce an estimate at least as good as the low accuracy option
          
          long poorEst = stats.get("low_"+ff).getCardinality();
          long goodEst = stats.get("high_"+ff).getCardinality();
          assertTrue(ff + ": goodEst="+goodEst+", poorEst="+poorEst+", real="+numMatches+", p=" + p,
                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));
        }
      }
    }
  }

