  @BeforeClass
  public static void setupCluster() throws Exception {
    final int numShards = TestUtil.nextInt(random(),1,5);
    final int numReplicas = 1;
    final int maxShardsPerNode = 1;
    final int nodeCount = numShards * numReplicas;

    configureCluster(nodeCount)
      .addConfig(CONF, Paths.get(TEST_HOME(), "collection1", "conf"))
      .configure();

    assertEquals(0, (CollectionAdminRequest.createCollection(COLLECTION, CONF, numShards, numReplicas)
                     .setMaxShardsPerNode(maxShardsPerNode)
                     .setProperties(Collections.singletonMap(CoreAdminParams.CONFIG, "solrconfig-minimal.xml"))
                     .process(cluster.getSolrClient())).getStatus());
    
    cluster.getSolrClient().setDefaultCollection(COLLECTION);

    final int numDocs = atLeast(1000);
    final int maxTermId = atLeast(TERM_VALUES_RANDOMIZER);

    // clear the RANGE_MODEL
    Arrays.fill(RANGE_MODEL, 0);
    // seed the TERM_MODEL Maps so we don't have null check later
    for (int i = 0; i < NUM_RANGE_VALUES; i++) {
      TERM_MODEL[i] = new LinkedHashMap<>();
    }

    // build our index & our models
    for (int id = 0; id < numDocs; id++) {
      final int rangeVal = random().nextInt(NUM_RANGE_VALUES);
      final String termVal = "x" + random().nextInt(maxTermId);
      final SolrInputDocument doc = sdoc("id", ""+id,
                                         INT_FIELD, ""+rangeVal,
                                         STR_FIELD, termVal);
      RANGE_MODEL[rangeVal]++;
      TERM_MODEL[rangeVal].merge(termVal, 1, Integer::sum);

      assertEquals(0, (new UpdateRequest().add(doc)).process(cluster.getSolrClient()).getStatus());
    }
    assertEquals(0, cluster.getSolrClient().commit().getStatus());
    
  }

