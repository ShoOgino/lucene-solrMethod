  private void runTest() throws Exception {
    final int totalNumDocs = atLeast(50);
    
    // Add a bunch of docs; some with extremely short expiration, some with no expiration
    // these should be randomly distributed to each shard
    long numDocsThatNeverExpire = 0;
    {
      final UpdateRequest req = setAuthIfNeeded(new UpdateRequest());
      for (int i = 1; i <= totalNumDocs; i++) {
        final SolrInputDocument doc = sdoc("id", i);

        if (random().nextBoolean()) {
          doc.addField("should_expire_s","yup");
          doc.addField("tTl_s","+1SECONDS");
        } else {
          numDocsThatNeverExpire++;
        }
        
        req.add(doc);
      }
      req.commit(cluster.getSolrClient(), COLLECTION);
    }
    
    // NOTE: don't assume we can find exactly totalNumDocs right now, some may have already been deleted...
    
    // it should not take long for us to get to the point where all 'should_expire_s:yup' docs are gone
    waitForNoResults(30, params("q","should_expire_s:yup","rows","0","_trace","init_batch_check"));

    {
      // ...*NOW* we can assert that exactly numDocsThatNeverExpire should exist...
      final QueryRequest req = setAuthIfNeeded(new QueryRequest
                                               (params("q", "*:*",
                                                       "rows", "0",
                                                       "_trace", "count_non_expire_docs")));

      // NOTE: it's possible that replicas could be out of sync but this query may get lucky and
      // only hit leaders.  we'll compare the counts of every replica in every shard later on...
      assertEquals(numDocsThatNeverExpire,
                   req.process(cluster.getSolrClient(), COLLECTION).getResults().getNumFound());
    }
    
    //
    // now that we've confrmed the basics work, let's check some fine grain stuff...
    //
    
    // first off, sanity check that this special docId doesn't some how already exist
    waitForNoResults(0, params("q","id:special99","rows","0","_trace","sanity_check99"));

    {
      // force a hard commit on all shards (the prior auto-expire would have only done a soft commit)
      // so we can ensure our indexVersion won't change uncessisarily on the un-affected
      // shard when we add & (hard) commit our special doc...
      final UpdateRequest req = setAuthIfNeeded(new UpdateRequest());
      req.commit(cluster.getSolrClient(), COLLECTION);
    }
    
    
    // record important data for each replica core so we can check later
    // that it only changes for the replicas of a single shard after we add/expire a single special doc
    log.info("Fetching ReplicaData BEFORE special doc addition/expiration");
    final Map<String,ReplicaData> initReplicaData = getTestDataForAllReplicas();
    assertTrue("WTF? no replica data?", 0 < initReplicaData.size());

    // add & hard commit a special doc with a short TTL 
    setAuthIfNeeded(new UpdateRequest()).add(sdoc("id", "special99", "should_expire_s","yup","tTl_s","+30SECONDS"))
      .commit(cluster.getSolrClient(), COLLECTION);

    // wait for our special docId to be deleted
    waitForNoResults(180, params("q","id:special99","rows","0","_trace","did_special_doc_expire_yet"));

    // now check all of the replicas to verify a few things:
    // - only the replicas of one shard changed -- no unneccessary churn on other shards
    // - every replica of each single shard should have the same number of docs
    // - the total number of docs should match numDocsThatNeverExpire
    log.info("Fetching ReplicaData AFTER special doc addition/expiration");
    final Map<String,ReplicaData> finalReplicaData = getTestDataForAllReplicas();
    assertEquals("WTF? not same num replicas?", 
                 initReplicaData.size(),
                 finalReplicaData.size());

    final Set<String> coresThatChange = new HashSet<>();
    final Set<String> shardsThatChange = new HashSet<>();
    
    int coresCompared = 0;
    int totalDocsOnAllShards = 0;
    final DocCollection collectionState = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);
    for (Slice shard : collectionState) {
      boolean firstReplica = true;
      for (Replica replica : shard) {
        coresCompared++;
        assertEquals(shard.getName(), replica.getSlice()); // sanity check
        final String core = replica.getCoreName();
        final ReplicaData initData = initReplicaData.get(core);
        final ReplicaData finalData = finalReplicaData.get(core);
        assertNotNull(shard.getName() + ": no init data for core: " + core, initData);
        assertNotNull(shard.getName() + ": no final data for core: " + core, finalData);

        if (!initData.equals(finalData)) {
          log.error("ReplicaData changed: {} != {}", initData, finalData);
          coresThatChange.add(core + "("+shard.getName()+")");
          shardsThatChange.add(shard.getName());
        }
        
        if (firstReplica) {
          totalDocsOnAllShards += finalData.numDocs;
          firstReplica = false;
        }
      }
    }

    assertEquals("Exactly one shard should have changed, instead: " + shardsThatChange
                 + " cores=(" + coresThatChange + ")",
                 1, shardsThatChange.size());
    assertEquals("somehow we missed some cores?", 
                 initReplicaData.size(), coresCompared);

    assertEquals("Final tally has incorrect numDocsThatNeverExpire",
                 numDocsThatNeverExpire, totalDocsOnAllShards);
    
    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...
    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify 
    // that *only* one node is sending the deletes ?
    // (ie: no flood of redundant deletes?)

  }

