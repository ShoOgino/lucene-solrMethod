  public void testManyDocsInTlogReplayWhileReplicaIsTryingToRecover() throws Exception {
    // TODO: One the basic problem in SOLR-13486 is fixed, this test can be made more robust by:
    // 1) randomizing the number of committedDocs (pre net split) & uncommittedDocs (post net split)
    //    to trigger diff recovery strategies & shutdown behavior
    // 2) replace "committedDocs + uncommittedDocs" with 4 variables:
    //    a: docs committed before network split (add + commit)
    //    b: docs not committed before network split (add w/o commit)
    //    c: docs committed after network split (add + commit)
    //    d: docs not committed after network split (add w/o commit)
    final int committedDocs = 3;
    final int uncommittedDocs = 50;
    
    log.info("Create Collection...");
    assertEquals(RequestStatusState.COMPLETED,
                 CollectionAdminRequest.createCollection(COLLECTION, 1, 2)
                 .setCreateNodeSet("")
                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));
    assertEquals(RequestStatusState.COMPLETED,
                 CollectionAdminRequest.addReplicaToShard(COLLECTION, "shard1")
                 .setNode(NODE0.getNodeName())
                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));
    
    waitForState("Timeout waiting for shard leader", COLLECTION, clusterShape(1, 1));

    assertEquals(RequestStatusState.COMPLETED,
                 CollectionAdminRequest.addReplicaToShard(COLLECTION, "shard1")
                 .setNode(NODE1.getNodeName())
                 .processAndWait(cluster.getSolrClient(), DEFAULT_TIMEOUT));
    
    cluster.waitForActiveCollection(COLLECTION, 1, 2);
    
    waitForState("Timeout waiting for 1x2 collection", COLLECTION, clusterShape(1, 2));
    
    final Replica leader = getCollectionState(COLLECTION).getSlice("shard1").getLeader();
    assertEquals("Sanity check failed", NODE0.getNodeName(), leader.getNodeName());

    log.info("Add and commit {} docs...", committedDocs);
    addDocs(true, committedDocs, 1);
    assertDocsExistInBothReplicas(1, committedDocs);

    log.info("Partition nodes...");
    proxies.get(NODE0).close();
    proxies.get(NODE1).close();

    log.info("Adding {} (uncommitted) docs during network partition....", uncommittedDocs);
    addDocs(false, uncommittedDocs, committedDocs + 1);

    log.info("Stopping leader node...");
    assertEquals("Something broke our expected skipIndexWriterCommitOnClose",
                 TEST_VALUE_FOR_SKIP_COMMIT_ON_CLOSE, TestInjection.skipIndexWriterCommitOnClose);
    NODE0.stop();
    cluster.waitForJettyToStop(NODE0);

    log.info("Un-Partition replica (NODE1)...");
    proxies.get(NODE1).reopen();
    
    waitForState("Timeout waiting for leader goes DOWN", COLLECTION, (liveNodes, collectionState)
                 -> collectionState.getReplica(leader.getName()).getState() == Replica.State.DOWN);

    // Sanity check that a new (out of sync) replica doesn't come up in our place...
    expectThrows(TimeoutException.class,
                 "Did not time out waiting for new leader, out of sync replica became leader",
                 () -> {
                   cluster.getSolrClient().waitForState(COLLECTION, 10, TimeUnit.SECONDS, (state) -> {
            Replica newLeader = state.getSlice("shard1").getLeader();
            if (newLeader != null && !newLeader.getName().equals(leader.getName()) && newLeader.getState() == Replica.State.ACTIVE) {
              // this is is the bad case, our "bad" state was found before timeout
              log.error("WTF: New Leader={}", newLeader);
              return true;
            }
            return false; // still no bad state, wait for timeout
          });
      });

    log.info("Enabling TestInjection.updateLogReplayRandomPause");
    TestInjection.updateLogReplayRandomPause = "true:100";
      
    log.info("Un-Partition & restart leader (NODE0)...");
    proxies.get(NODE0).reopen();
    NODE0.start();

    log.info("Waiting for all nodes and active collection...");
    
    cluster.waitForAllNodes(30);;
    waitForState("Timeout waiting for leader", COLLECTION, (liveNodes, collectionState) -> {
      Replica newLeader = collectionState.getLeader("shard1");
      return newLeader != null && newLeader.getName().equals(leader.getName());
    });
    waitForState("Timeout waiting for active collection", COLLECTION, clusterShape(1, 2));
    
    cluster.waitForActiveCollection(COLLECTION, 1, 2);

    log.info("Check docs on both replicas...");
    assertDocsExistInBothReplicas(1, committedDocs + uncommittedDocs);
    
    log.info("Test ok, delete collection...");
    CollectionAdminRequest.deleteCollection(COLLECTION).process(cluster.getSolrClient());
  }

