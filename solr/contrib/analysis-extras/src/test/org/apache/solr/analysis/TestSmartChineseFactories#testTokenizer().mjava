  /** Test showing the behavior with whitespace */
  public void testTokenizer() throws Exception {
    String sentence = "我购买了道具和服装。我购买了道具和服装。";
    SmartChineseSentenceTokenizerFactory tokenizerFactory = new SmartChineseSentenceTokenizerFactory();
    Tokenizer tokenizer = tokenizerFactory.create(new StringReader(sentence));
    SmartChineseWordTokenFilterFactory factory = new SmartChineseWordTokenFilterFactory();
    TokenStream ts = factory.create(tokenizer);
    // TODO: fix smart chinese to not emit punctuation tokens
    // at the moment: you have to clean up with WDF, or use the stoplist, etc
    assertTokenStreamContents(ts, 
       new String[] { "我", "购买", "了", "道具", "和", "服装", ",", 
        "我", "购买", "了", "道具", "和", "服装", ","
        });
  }

