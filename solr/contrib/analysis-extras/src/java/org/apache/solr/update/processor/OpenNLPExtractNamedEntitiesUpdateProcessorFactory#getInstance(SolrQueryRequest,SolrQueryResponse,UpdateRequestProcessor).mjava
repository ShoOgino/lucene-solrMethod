  @Override
  public final UpdateRequestProcessor getInstance
      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {
    final FieldNameSelector srcSelector = getSourceSelector();
    return new UpdateRequestProcessor(next) {
      private final NLPNERTaggerOp nerTaggerOp;
      private Analyzer analyzer = null;
      {
        try {
          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);
          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);
          if (fieldType == null) {
            throw new SolrException
                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + " '" + analyzerFieldType + "' not found in the schema.");
          }
          analyzer = fieldType.getIndexAnalyzer();
        } catch (IOException e) {
          throw new IllegalArgumentException(e);
        }
      }

      @Override
      public void processAdd(AddUpdateCommand cmd) throws IOException {

        final SolrInputDocument doc = cmd.getSolrInputDocument();

        // Destination may be regex replace string, or "{EntityType}" replaced by
        // each entity's type, both of which can cause multiple output fields.
        Map<String,SolrInputField> destMap = new HashMap<>();

        // preserve initial values
        for (final String fname : doc.getFieldNames()) {
          if ( ! srcSelector.shouldMutate(fname)) continue;

          Collection<Object> srcFieldValues = doc.getFieldValues(fname);
          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;

          String resolvedDest = dest;

          if (pattern != null) {
            Matcher matcher = pattern.matcher(fname);
            if (matcher.find()) {
              resolvedDest = matcher.replaceAll(dest);
            } else {
              log.debug("srcSelector.shouldMutate(\"{}\") returned true, " +
                  "but replacement pattern did not match, field skipped.", fname);
              continue;
            }
          }

          for (Object val : srcFieldValues) {
            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {
              SolrInputField destField = null;
              String entityName = entity.first();
              String entityType = entity.second();
              resolvedDest = resolvedDest.replace(ENTITY_TYPE, entityType);
              if (doc.containsKey(resolvedDest)) {
                destField = doc.getField(resolvedDest);
              } else {
                SolrInputField targetField = destMap.get(resolvedDest);
                if (targetField == null) {
                  destField = new SolrInputField(resolvedDest);
                } else {
                  destField = targetField;
                }
              }
              destField.addValue(entityName);

              // put it in map to avoid concurrent modification...
              destMap.put(resolvedDest, destField);
            }
          }
        }

        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {
          doc.put(entry.getKey(), entry.getValue());
        }
        super.processAdd(cmd);
      }

      /** Using configured NER model, extracts (name, type) pairs from the given source field value */
      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {
        List<Pair<String,String>> entitiesWithType = new ArrayList<>();
        List<String> terms = new ArrayList<>();
        List<Integer> startOffsets = new ArrayList<>();
        List<Integer> endOffsets = new ArrayList<>();
        String fullText = srcFieldValue.toString();
        TokenStream tokenStream = analyzer.tokenStream("", fullText);
        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);
        tokenStream.reset();
        synchronized (nerTaggerOp) {
          while (tokenStream.incrementToken()) {
            terms.add(termAtt.toString());
            startOffsets.add(offsetAtt.startOffset());
            endOffsets.add(offsetAtt.endOffset());
            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);
            if (endOfSentence) {    // extract named entities one sentence at a time
              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);
            }
          }
          tokenStream.end();
          tokenStream.close();
          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT
            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);
          }
          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls
        }
        return entitiesWithType;
      }

      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,
                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {
        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {
          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));
          entitiesWithType.add(new Pair<>(text, span.getType()));
        }
        terms.clear();
        startOffsets.clear();
        endOffsets.clear();
      }
    };
  }

