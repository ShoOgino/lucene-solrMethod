  /** Does the actual (time-consuming) work of the merge,
   *  but without holding synchronized lock on IndexWriter
   *  instance */
  final private int mergeMiddle(MergePolicy.OneMerge merge) 
    throws CorruptIndexException, IOException {
    
    merge.checkAborted(directory);

    final String mergedName = merge.info.name;
    
    SegmentMerger merger = null;

    int mergedDocCount = 0;

    SegmentInfos sourceSegments = merge.segments;
    final int numSegments = sourceSegments.size();

    if (infoStream != null)
      message("merging " + merge.segString(directory));

    merger = new SegmentMerger(this, mergedName, merge);

    merge.readers = new SegmentReader[numSegments];
    merge.readersClone = new SegmentReader[numSegments];

    boolean mergeDocStores = false;

    final Set dss = new HashSet();
    
    // This is try/finally to make sure merger's readers are
    // closed:
    boolean success = false;
    try {
      int totDocCount = 0;

      for (int i = 0; i < numSegments; i++) {

        final SegmentInfo info = sourceSegments.info(i);

        // Hold onto the "live" reader; we will use this to
        // commit merged deletes
        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,
                                                                 MERGE_READ_BUFFER_SIZE,
                                                                 -1);

        // We clone the segment readers because other
        // deletes may come in while we're merging so we
        // need readers that will not change
        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);
        merger.add(clone);

        if (clone.hasDeletions()) {
          mergeDocStores = true;
        }
        
        if (info.getDocStoreOffset() != -1) {
          dss.add(info.getDocStoreSegment());
        }

        totDocCount += clone.numDocs();
      }

      if (infoStream != null) {
        message("merge: total "+totDocCount+" docs");
      }

      merge.checkAborted(directory);

      // If deletions have arrived and it has now become
      // necessary to merge doc stores, go and open them:
      if (mergeDocStores && !merge.mergeDocStores) {
        merge.mergeDocStores = true;
        synchronized(this) {
          if (dss.contains(docWriter.getDocStoreSegment())) {
            if (infoStream != null)
              message("now flush at mergeMiddle");
            doFlush(true, false);
          }
        }

        for(int i=0;i<numSegments;i++) {
          merge.readersClone[i].openDocStores();
        }

        // Clear DSS
        synchronized(this) {
          merge.info.setDocStore(-1, null, false);
        }
      }

      // This is where all the work happens:
      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);

      assert mergedDocCount == totDocCount;

      // TODO: in the non-realtime case, we may want to only
      // keep deletes (it's costly to open entire reader
      // when we just need deletes)

      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);
      try {
        if (poolReaders && mergedSegmentWarmer != null) {
          mergedSegmentWarmer.warm(mergedReader);
        }
        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))
          // commitMerge will return false if this merge was aborted
          return 0;
      } finally {
        synchronized(this) {
          readerPool.release(mergedReader);
        }
      }

      success = true;
    } finally {
      synchronized(this) {
        if (!success) {
          // Suppress any new exceptions so we throw the
          // original cause
          for (int i=0;i<numSegments;i++) {
            if (merge.readers[i] != null) {
              try {
                readerPool.release(merge.readers[i], true);
              } catch (Throwable t) {
              }
            }

            if (merge.readersClone[i] != null) {
              try {
                merge.readersClone[i].close();
              } catch (Throwable t) {
              }
              // This was a private clone and we had the only reference
              assert merge.readersClone[i].getRefCount() == 0;
            }
          }
        } else {
          for (int i=0;i<numSegments;i++) {
            if (merge.readers[i] != null) {
              readerPool.release(merge.readers[i], true);
            }

            if (merge.readersClone[i] != null) {
              merge.readersClone[i].close();
              // This was a private clone and we had the only reference
              assert merge.readersClone[i].getRefCount() == 0;
            }
          }
        }
      }
    }

    // Must checkpoint before decrefing so any newly
    // referenced files in the new merge.info are incref'd
    // first:
    synchronized(this) {
      deleter.checkpoint(segmentInfos, false);
    }
    decrefMergeSegments(merge);

    if (merge.useCompoundFile) {

      // Maybe force a sync here to allow reclaiming of the
      // disk space used by the segments we just merged:
      if (autoCommit && doCommitBeforeMergeCFS(merge)) {
        final long size;
        synchronized(this) {
          size = merge.info.sizeInBytes();
        }
        commit(size);
      }
      
      success = false;
      final String compoundFileName = mergedName + "." + IndexFileNames.COMPOUND_FILE_EXTENSION;

      try {
        merger.createCompoundFile(compoundFileName);
        success = true;
      } catch (IOException ioe) {
        synchronized(this) {
          if (merge.isAborted()) {
            // This can happen if rollback or close(false)
            // is called -- fall through to logic below to
            // remove the partially created CFS:
            success = true;
          } else
            handleMergeException(ioe, merge);
        }
      } catch (Throwable t) {
        handleMergeException(t, merge);
      } finally {
        if (!success) {
          if (infoStream != null)
            message("hit exception creating compound file during merge");
          synchronized(this) {
            deleter.deleteFile(compoundFileName);
          }
        }
      }

      if (merge.isAborted()) {
        if (infoStream != null)
          message("abort merge after building CFS");
        deleter.deleteFile(compoundFileName);
        return 0;
      }

      synchronized(this) {
        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {
          // Our segment (committed in non-compound
          // format) got merged away while we were
          // building the compound format.
          deleter.deleteFile(compoundFileName);
        } else {
          merge.info.setUseCompoundFile(true);
          checkpoint();
        }
      }
    }

    // Force a sync after commiting the merge.  Once this
    // sync completes then all index files referenced by the
    // current segmentInfos are on stable storage so if the
    // OS/machine crashes, or power cord is yanked, the
    // index will be intact.  Note that this is just one
    // (somewhat arbitrary) policy; we could try other
    // policies like only sync if it's been > X minutes or
    // more than Y bytes have been written, etc.
    if (autoCommit) {
      final long size;
      synchronized(this) {
        size = merge.info.sizeInBytes();
      }
      commit(size);
    }

    return mergedDocCount;
  }

