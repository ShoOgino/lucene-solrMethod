  synchronized SegmentReader reopenSegment(SegmentInfo si, boolean doClone, boolean openReadOnly) throws CorruptIndexException, IOException {
    boolean deletionsUpToDate = (this.si.hasDeletions() == si.hasDeletions()) 
                                  && (!si.hasDeletions() || this.si.getDelFileName().equals(si.getDelFileName()));
    boolean normsUpToDate = true;
    
    boolean[] fieldNormsChanged = new boolean[fieldInfos.size()];
    final int fieldCount = fieldInfos.size();
    for (int i = 0; i < fieldCount; i++) {
      if (!this.si.getNormFileName(i).equals(si.getNormFileName(i))) {
        normsUpToDate = false;
        fieldNormsChanged[i] = true;
      }
    }

    // if we're cloning we need to run through the reopenSegment logic
    if (normsUpToDate && deletionsUpToDate && !doClone && openReadOnly == readOnly) {
      return this;
    }    

    // clone reader
    SegmentReader clone;
    try {
      if (openReadOnly)
        clone = (SegmentReader) READONLY_IMPL.newInstance();
      else
        clone = (SegmentReader) IMPL.newInstance();
    } catch (Exception e) {
      throw new RuntimeException("cannot load SegmentReader class: " + e, e);
    }

    boolean success = false;
    try {
      clone.readOnly = openReadOnly;
      clone.directory = directory;
      clone.si = si;
      clone.segment = segment;
      clone.readBufferSize = readBufferSize;
      clone.cfsReader = cfsReader;
      clone.storeCFSReader = storeCFSReader;

      clone.fieldInfos = fieldInfos;
      clone.tis = tis;
      clone.freqStream = freqStream;
      clone.proxStream = proxStream;
      clone.termVectorsReaderOrig = termVectorsReaderOrig;
  
      if (fieldsReaderOrig != null) {
        clone.fieldsReaderOrig = (FieldsReader) fieldsReaderOrig.clone();
      }      
      
      if (deletedDocsRef != null) {
        deletedDocsRef.incRef();
      }
      if (doClone) {
        clone.deletedDocs = deletedDocs;
        clone.deletedDocsRef = deletedDocsRef;
      } else {
        if (!deletionsUpToDate) {
          // load deleted docs
          clone.deletedDocs = null;
          clone.deletedDocsRef = null;
          clone.loadDeletedDocs();
        } else {
          clone.deletedDocs = deletedDocs;
          clone.deletedDocsRef = deletedDocsRef;
        }
      }

      clone.norms = new HashMap();

      // Clone norms
      for (int i = 0; i < fieldNormsChanged.length; i++) {

        // Clone unchanged norms to the cloned reader
        if (doClone || !fieldNormsChanged[i]) {
          final String curField = fieldInfos.fieldInfo(i).name;
          Norm norm = (Norm) this.norms.get(curField);
          if (norm != null)
            clone.norms.put(curField, norm.clone());
        }
      }
      
      // If we are not cloning, then this will open anew
      // any norms that have changed:
      clone.openNorms(si.getUseCompoundFile() ? cfsReader : directory(), readBufferSize);

      success = true;
    } finally {
      if (this.referencedSegmentReader != null) {
        // This reader shares resources with another SegmentReader,
        // so we increment the other reader's refCount.
        clone.referencedSegmentReader = this.referencedSegmentReader;
      } else {
        // We are the original SegmentReader
        clone.referencedSegmentReader = this;
      }
      clone.referencedSegmentReader.incRefReaderNotNorms();
      
      if (!success) {
        // An exception occured during reopen, we have to decRef the norms
        // that we incRef'ed already and close singleNormsStream and FieldsReader
        clone.decRef();
      }
    }
    
    return clone;
  }

