  /**
   * Merge the TermVectors from each of the segments into the new one.
   * @throws IOException
   */
  private final void mergeVectors() throws IOException {
    TermVectorsWriter termVectorsWriter = 
      new TermVectorsWriter(directory, segment, fieldInfos);

    try {
      for (int r = 0; r < readers.size(); r++) {
        final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];
        TermVectorsReader matchingVectorsReader;
        final boolean hasMatchingReader;
        if (matchingSegmentReader != null) {
          matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;

          // If the TV* files are an older format then they
          // cannot read raw docs:
          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {
            matchingVectorsReader = null;
            hasMatchingReader = false;
          } else
            hasMatchingReader = matchingVectorsReader != null;

        } else {
          hasMatchingReader = false;
          matchingVectorsReader = null;
        }
        IndexReader reader = (IndexReader) readers.get(r);
        final boolean hasDeletions = reader.hasDeletions();
        int maxDoc = reader.maxDoc();
        for (int docNum = 0; docNum < maxDoc;) {
          // skip deleted docs
          if (!hasDeletions || !reader.isDeleted(docNum)) {
            if (hasMatchingReader) {
              // We can optimize this case (doing a bulk
              // byte copy) since the field numbers are
              // identical
              int start = docNum;
              int numDocs = 0;
              do {
                docNum++;
                numDocs++;
                if (docNum >= maxDoc)
                  break;
                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {
                  docNum++;
                  break;
                }
              } while(numDocs < MAX_RAW_MERGE_DOCS);

              matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
              termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
              if (checkAbort != null)
                checkAbort.work(300*numDocs);
            } else {
              // NOTE: it's very important to first assign
              // to vectors then pass it to
              // termVectorsWriter.addAllDocVectors; see
              // LUCENE-1282
              TermFreqVector[] vectors = reader.getTermFreqVectors(docNum);
              termVectorsWriter.addAllDocVectors(vectors);
              docNum++;
              if (checkAbort != null)
                checkAbort.work(300);
            }
          } else
            docNum++;
        }
      }
    } finally {
      termVectorsWriter.close();
    }

    final long tvxSize = directory.fileLength(segment + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);

    if (4+((long) mergedDocs)*16 != tvxSize)
      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
      // we detect that the bug has struck, here, and
      // throw an exception to prevent the corruption from
      // entering the index.  See LUCENE-1282 for
      // details.
      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + mergedDocs + " but tvx size is " + tvxSize + "; now aborting this merge to prevent index corruption");
  }

