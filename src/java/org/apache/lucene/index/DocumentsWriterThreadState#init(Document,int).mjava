  /** Initializes shared state for this new document */
  void init(Document doc, int docID) throws IOException, AbortException {

    assert !isIdle;
    assert docWriter.writer.testPoint("DocumentsWriter.ThreadState.init start");

    this.docID = docID;
    docBoost = doc.getBoost();
    numStoredFields = 0;
    numFieldData = 0;
    numVectorFields = 0;
    maxTermPrefix = null;

    assert 0 == fdtLocal.length();
    assert 0 == fdtLocal.getFilePointer();
    assert 0 == tvfLocal.length();
    assert 0 == tvfLocal.getFilePointer();
    final int thisFieldGen = fieldGen++;

    List docFields = doc.getFields();
    final int numDocFields = docFields.size();
    boolean docHasVectors = false;

    // Absorb any new fields first seen in this document.
    // Also absorb any changes to fields we had already
    // seen before (eg suddenly turning on norms or
    // vectors, etc.):

    for(int i=0;i<numDocFields;i++) {
      Fieldable field = (Fieldable) docFields.get(i);

      FieldInfo fi = docWriter.fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),
                                              field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
                                              field.getOmitNorms(), false);
      if (fi.isIndexed && !fi.omitNorms) {
        // Maybe grow our buffered norms
        if (docWriter.norms.length <= fi.number) {
          int newSize = (int) ((1+fi.number)*1.25);
          BufferedNorms[] newNorms = new BufferedNorms[newSize];
          System.arraycopy(docWriter.norms, 0, newNorms, 0, docWriter.norms.length);
          docWriter.norms = newNorms;
        }
          
        if (docWriter.norms[fi.number] == null)
          docWriter.norms[fi.number] = new BufferedNorms();

        docWriter.hasNorms = true;
      }

      // Make sure we have a FieldData allocated
      int hashPos = fi.name.hashCode() & fieldDataHashMask;
      DocumentsWriterFieldData fp = fieldDataHash[hashPos];
      while(fp != null && !fp.fieldInfo.name.equals(fi.name))
        fp = fp.next;

      if (fp == null) {

        fp = new DocumentsWriterFieldData(this, fi);
        fp.next = fieldDataHash[hashPos];
        fieldDataHash[hashPos] = fp;

        if (numAllFieldData == allFieldDataArray.length) {
          int newSize = (int) (allFieldDataArray.length*1.5);
          int newHashSize = fieldDataHash.length*2;

          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];
          DocumentsWriterFieldData newHashArray[] = new DocumentsWriterFieldData[newHashSize];
          System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);

          // Rehash
          fieldDataHashMask = newSize-1;
          for(int j=0;j<fieldDataHash.length;j++) {
            DocumentsWriterFieldData fp0 = fieldDataHash[j];
            while(fp0 != null) {
              hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;
              DocumentsWriterFieldData nextFP0 = fp0.next;
              fp0.next = newHashArray[hashPos];
              newHashArray[hashPos] = fp0;
              fp0 = nextFP0;
            }
          }

          allFieldDataArray = newArray;
          fieldDataHash = newHashArray;
        }
        allFieldDataArray[numAllFieldData++] = fp;
      } else {
        assert fp.fieldInfo == fi;
      }

      if (thisFieldGen != fp.lastGen) {

        // First time we're seeing this field for this doc
        fp.lastGen = thisFieldGen;
        fp.fieldCount = 0;
        fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;
        fp.doNorms = fi.isIndexed && !fi.omitNorms;

        if (numFieldData == fieldDataArray.length) {
          int newSize = fieldDataArray.length*2;
          DocumentsWriterFieldData newArray[] = new DocumentsWriterFieldData[newSize];
          System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);
          fieldDataArray = newArray;

        }
        fieldDataArray[numFieldData++] = fp;
      }

      if (field.isTermVectorStored()) {
        if (!fp.doVectors && numVectorFields++ == vectorFieldPointers.length) {
          final int newSize = (int) (numVectorFields*1.5);
          vectorFieldPointers = new long[newSize];
          vectorFieldNumbers = new int[newSize];
        }
        fp.doVectors = true;
        docHasVectors = true;

        fp.doVectorPositions |= field.isStorePositionWithTermVector();
        fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();
      }

      if (fp.fieldCount == fp.docFields.length) {
        Fieldable[] newArray = new Fieldable[fp.docFields.length*2];
        System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);
        fp.docFields = newArray;
      }

      // Lazily allocate arrays for postings:
      if (field.isIndexed() && fp.postingsHash == null)
        fp.initPostingArrays();

      fp.docFields[fp.fieldCount++] = field;
    }

    // Maybe init the local & global fieldsWriter
    if (localFieldsWriter == null) {
      if (docWriter.fieldsWriter == null) {
        assert docWriter.docStoreSegment == null;
        assert docWriter.segment != null;
        docWriter.docStoreSegment = docWriter.segment;
        // If we hit an exception while init'ing the
        // fieldsWriter, we must abort this segment
        // because those files will be in an unknown
        // state:
        try {
          docWriter.fieldsWriter = new FieldsWriter(docWriter.directory, docWriter.docStoreSegment, docWriter.fieldInfos);
        } catch (Throwable t) {
          throw new AbortException(t, docWriter);
        }
        docWriter.files = null;
      }
      localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);
    }

    // First time we see a doc that has field(s) with
    // stored vectors, we init our tvx writer
    if (docHasVectors) {
      if (docWriter.tvx == null) {
        assert docWriter.docStoreSegment != null;
        // If we hit an exception while init'ing the term
        // vector output files, we must abort this segment
        // because those files will be in an unknown
        // state:
        try {
          docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);
          docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  "." + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);
          docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  "." + IndexFileNames.VECTORS_FIELDS_EXTENSION);
          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);

          // We must "catch up" for all docs before us
          // that had no vectors:
          for(int i=0;i<docWriter.numDocsInStore;i++) {
            docWriter.tvx.writeLong(docWriter.tvd.getFilePointer());
            docWriter.tvd.writeVInt(0);
            docWriter.tvx.writeLong(0);
          }
        } catch (Throwable t) {
          throw new AbortException(t, docWriter);
        }
        docWriter.files = null;
      }
      numVectorFields = 0;
    }
  }

