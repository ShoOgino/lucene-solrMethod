  /**
   * Flush all in-memory buffered udpates (adds and deletes)
   * to the Directory.
   * @param triggerMerge if true, we may merge segments (if
   *  deletes or docs were flushed) if necessary
   * @param flushDocStores if false we are allowed to keep
   *  doc stores open to share with the next segment
   */
  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {
    ensureOpen();

    // Make sure no threads are actively adding a document
    docWriter.pauseAllThreads();

    try {

      SegmentInfo newSegment = null;

      final int numDocs = docWriter.getNumDocsInRAM();

      // Always flush docs if there are any
      boolean flushDocs = numDocs > 0;

      // With autoCommit=true we always must flush the doc
      // stores when we flush
      flushDocStores |= autoCommit;
      String docStoreSegment = docWriter.getDocStoreSegment();
      if (docStoreSegment == null)
        flushDocStores = false;

      // Always flush deletes if there are any delete terms.
      // TODO: when autoCommit=false we don't have to flush
      // deletes with every flushed segment; we can save
      // CPU/IO by buffering longer & flushing deletes only
      // when they are full or writer is being closed.  We
      // have to fix the "applyDeletesSelectively" logic to
      // apply to more than just the last flushed segment
      boolean flushDeletes = bufferedDeleteTerms.size() > 0;

      if (infoStream != null)
        infoStream.println("  flush: flushDocs=" + flushDocs +
                           " flushDeletes=" + flushDeletes +
                           " flushDocStores=" + flushDocStores +
                           " numDocs=" + numDocs);

      int docStoreOffset = docWriter.getDocStoreOffset();
      boolean docStoreIsCompoundFile = false;

      // Check if the doc stores must be separately flushed
      // because other segments, besides the one we are about
      // to flush, reference it
      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {
        // We must separately flush the doc store
        if (infoStream != null)
          infoStream.println("  flush shared docStore segment " + docStoreSegment);
      
        flushDocStores();
        flushDocStores = false;
        docStoreIsCompoundFile = useCompoundFile;
      }

      String segment = docWriter.getSegment();

      if (flushDocs || flushDeletes) {

        SegmentInfos rollback = null;

        if (flushDeletes)
          rollback = (SegmentInfos) segmentInfos.clone();

        boolean success = false;

        try {
          if (flushDocs) {

            if (0 == docStoreOffset && flushDocStores) {
              // This means we are flushing private doc stores
              // with this segment, so it will not be shared
              // with other segments
              assert docStoreSegment != null;
              assert docStoreSegment.equals(segment);
              docStoreOffset = -1;
              docStoreIsCompoundFile = false;
              docStoreSegment = null;
            }

            int flushedDocCount = docWriter.flush(flushDocStores);
          
            newSegment = new SegmentInfo(segment,
                                         flushedDocCount,
                                         directory, false, true,
                                         docStoreOffset, docStoreSegment,
                                         docStoreIsCompoundFile);
            segmentInfos.addElement(newSegment);
          }

          if (flushDeletes) {
            // we should be able to change this so we can
            // buffer deletes longer and then flush them to
            // multiple flushed segments, when
            // autoCommit=false
            applyDeletes(flushDocs);
            doAfterFlush();
          }

          checkpoint();
          success = true;
        } finally {
          if (!success) {
            if (flushDeletes) {
              // Fully replace the segmentInfos since flushed
              // deletes could have changed any of the
              // SegmentInfo instances:
              segmentInfos.clear();
              segmentInfos.addAll(rollback);
            } else {
              // Remove segment we added, if any:
              if (newSegment != null && 
                  segmentInfos.size() > 0 && 
                  segmentInfos.info(segmentInfos.size()-1) == newSegment)
                segmentInfos.remove(segmentInfos.size()-1);
            }
            if (flushDocs)
              docWriter.abort();
            deleter.checkpoint(segmentInfos, false);
            deleter.refresh();
          }
        }

        deleter.checkpoint(segmentInfos, autoCommit);

        if (flushDocs && useCompoundFile) {
          success = false;
          try {
            docWriter.createCompoundFile(segment);
            newSegment.setUseCompoundFile(true);
            checkpoint();
            success = true;
          } finally {
            if (!success) {
              newSegment.setUseCompoundFile(false);
              deleter.refresh();
            }
          }

          deleter.checkpoint(segmentInfos, autoCommit);
        }

        /* new merge policy
        if (0 == docWriter.getMaxBufferedDocs())
          maybeMergeSegments(mergeFactor * numDocs / 2);
        else
          maybeMergeSegments(docWriter.getMaxBufferedDocs());
        */
        if (triggerMerge)
          maybeMergeSegments(docWriter.getMaxBufferedDocs());
      }
    } finally {
      docWriter.clearFlushPending();
      docWriter.resumeAllThreads();
    }
  }

