  /** This is the hotspot of indexing: it's called once
   *  for every term of every document.  Its job is to *
   *  update the postings byte stream (Postings hash) *
   *  based on the occurence of a single term. */
  private void addPosition(Token token) throws AbortException {

    final Payload payload = token.getPayload();

    // Get the text of this term.  Term can either
    // provide a String token or offset into a char[]
    // array
    final char[] tokenText = token.termBuffer();
    final int tokenTextLen = token.termLength();

    int code = 0;

    // Compute hashcode & replace any invalid UTF16 sequences
    int downto = tokenTextLen;
    while (downto > 0) {
      char ch = tokenText[--downto];

      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {
        if (0 == downto) {
          // Unpaired
          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
        } else {
          final char ch2 = tokenText[downto-1];
          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {
            // OK: high followed by low.  This is a valid
            // surrogate pair.
            code = ((code*31) + ch)*31+ch2;
            downto--;
            continue;
          } else {
            // Unpaired
            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
          }            
        }
      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)
        // Unpaired
        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;

      code = (code*31) + ch;
    }

    // System.out.println("  addPosition: field=" + fieldInfo.name + " buffer=" + new String(tokenText, 0, tokenTextLen) + " pos=" + position + " offsetStart=" + (offset+token.startOffset()) + " offsetEnd=" + (offset + token.endOffset()) + " docID=" + docID + " doPos=" + doVectorPositions + " doOffset=" + doVectorOffsets);

    int hashPos = code & postingsHashMask;

    assert !postingsCompacted;

    // Locate Posting in hash
    p = postingsHash[hashPos];

    if (p != null && !postingEquals(tokenText, tokenTextLen)) {
      // Conflict: keep searching different locations in
      // the hash table.
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        hashPos = code & postingsHashMask;
        p = postingsHash[hashPos];
      } while (p != null && !postingEquals(tokenText, tokenTextLen));
    }

    final int proxCode;

    // If we hit an exception below, it's possible the
    // posting list or term vectors data will be
    // partially written and thus inconsistent if
    // flushed, so we have to abort all documents
    // since the last flush:

    try {

      if (p != null) {       // term seen since last flush

        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc
            
          // System.out.println("    seen before (new docID=" + docID + ") freqUpto=" + p.freqUpto +" proxUpto=" + p.proxUpto);

          assert p.docFreq > 0;

          // Now that we know doc freq for previous doc,
          // write it & lastDocCode
          sliceWriter.init(p.freqUpto);

          if (1 == p.docFreq)
            sliceWriter.writeVInt(p.lastDocCode|1);
          else {
            sliceWriter.writeVInt(p.lastDocCode);
            sliceWriter.writeVInt(p.docFreq);
          }
          p.freqUpto = sliceWriter.getAddress();

          if (doVectors) {
            vector = addNewVector();
            if (doVectorOffsets) {
              offsetStartCode = offsetStart = offset + token.startOffset();
              offsetEnd = offset + token.endOffset();
            }
          }

          proxCode = position;

          p.docFreq = 1;

          // Store code so we can write this after we're
          // done with this new doc
          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;
          p.lastDocID = threadState.docID;

        } else {                                // term already seen in this doc
          // System.out.println("    seen before (same docID=" + docID + ") proxUpto=" + p.proxUpto);
          p.docFreq++;

          proxCode = position-p.lastPosition;

          if (doVectors) {
            vector = p.vector;
            if (vector == null)
              vector = addNewVector();
            if (doVectorOffsets) {
              offsetStart = offset + token.startOffset();
              offsetEnd = offset + token.endOffset();
              offsetStartCode = offsetStart-vector.lastOffset;
            }
          }
        }
      } else {					  // term not seen before
        // System.out.println("    never seen docID=" + docID);

        // Refill?
        if (0 == threadState.postingsFreeCount) {
          threadState.docWriter.getPostings(threadState.postingsFreeList);
          threadState.postingsFreeCount = threadState.postingsFreeList.length;
        }

        final int textLen1 = 1+tokenTextLen;
        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {
          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {
            // Just skip this term, to remain as robust as
            // possible during indexing.  A TokenFilter
            // can be inserted into the analyzer chain if
            // other behavior is wanted (pruning the term
            // to a prefix, throwing an exception, etc).
            if (threadState.maxTermPrefix == null)
              threadState.maxTermPrefix = new String(tokenText, 0, 30);

            // Still increment position:
            position++;
            return;
          }
          threadState.charPool.nextBuffer();
        }

        final char[] text = threadState.charPool.buffer;
        final int textUpto = threadState.charPool.byteUpto;

        // Pull next free Posting from free list
        p = threadState.postingsFreeList[--threadState.postingsFreeCount];

        p.textStart = textUpto + threadState.charPool.byteOffset;
        threadState.charPool.byteUpto += textLen1;

        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);

        text[textUpto+tokenTextLen] = 0xffff;
          
        assert postingsHash[hashPos] == null;

        postingsHash[hashPos] = p;
        numPostings++;

        if (numPostings == postingsHashHalfSize)
          rehashPostings(2*postingsHashSize);

        // Init first slice for freq & prox streams
        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;

        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;

        p.lastDocCode = threadState.docID << 1;
        p.lastDocID = threadState.docID;
        p.docFreq = 1;

        if (doVectors) {
          vector = addNewVector();
          if (doVectorOffsets) {
            offsetStart = offsetStartCode = offset + token.startOffset();
            offsetEnd = offset + token.endOffset();
          }
        }

        proxCode = position;
      }

      sliceWriter.init(p.proxUpto);

      if (payload != null && payload.length > 0) {
        sliceWriter.writeVInt((proxCode<<1)|1);
        sliceWriter.writeVInt(payload.length);
        sliceWriter.writeBytes(payload.data, payload.offset, payload.length);
        fieldInfo.storePayloads = true;
      } else
        sliceWriter.writeVInt(proxCode<<1);

      p.proxUpto = sliceWriter.getAddress();
      p.lastPosition = position++;

      if (doVectorPositions) {
        vectorsSliceWriter.init(vector.posUpto);
        vectorsSliceWriter.writeVInt(proxCode);
        vector.posUpto = vectorsSliceWriter.getAddress();
      }

      if (doVectorOffsets) {
        vectorsSliceWriter.init(vector.offsetUpto);
        vectorsSliceWriter.writeVInt(offsetStartCode);
        vectorsSliceWriter.writeVInt(offsetEnd-offsetStart);
        vector.lastOffset = offsetEnd;
        vector.offsetUpto = vectorsSliceWriter.getAddress();
      }
    } catch (Throwable t) {
      throw new AbortException(t, threadState.docWriter);
    }
  }

