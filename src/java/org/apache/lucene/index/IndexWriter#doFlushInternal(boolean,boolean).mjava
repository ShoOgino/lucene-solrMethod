  // TODO: this method should not have to be entirely
  // synchronized, ie, merges should be allowed to commit
  // even while a flush is happening
  private synchronized final boolean doFlushInternal(boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {

    if (hitOOM) {
      throw new IllegalStateException("this writer hit an OutOfMemoryError; cannot flush");
    }

    ensureOpen(false);

    assert testPoint("startDoFlush");

    doBeforeFlush();
    
    flushCount++;

    // If we are flushing because too many deletes
    // accumulated, then we should apply the deletes to free
    // RAM:
    flushDeletes |= docWriter.doApplyDeletes();

    // Make sure no threads are actively adding a document.
    // Returns true if docWriter is currently aborting, in
    // which case we skip flushing this segment
    if (docWriter.pauseAllThreads()) {
      docWriter.resumeAllThreads();
      return false;
    }

    try {

      SegmentInfo newSegment = null;

      final int numDocs = docWriter.getNumDocsInRAM();

      // Always flush docs if there are any
      boolean flushDocs = numDocs > 0;

      String docStoreSegment = docWriter.getDocStoreSegment();

      assert docStoreSegment != null || numDocs == 0: "dss=" + docStoreSegment + " numDocs=" + numDocs;

      if (docStoreSegment == null)
        flushDocStores = false;

      int docStoreOffset = docWriter.getDocStoreOffset();

      boolean docStoreIsCompoundFile = false;

      if (infoStream != null) {
        message("  flush: segment=" + docWriter.getSegment() +
                " docStoreSegment=" + docWriter.getDocStoreSegment() +
                " docStoreOffset=" + docStoreOffset +
                " flushDocs=" + flushDocs +
                " flushDeletes=" + flushDeletes +
                " flushDocStores=" + flushDocStores +
                " numDocs=" + numDocs +
                " numBufDelTerms=" + docWriter.getNumBufferedDeleteTerms());
        message("  index before flush " + segString());
      }

      // Check if the doc stores must be separately flushed
      // because other segments, besides the one we are about
      // to flush, reference it
      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {
        // We must separately flush the doc store
        if (infoStream != null)
          message("  flush shared docStore segment " + docStoreSegment);
      
        docStoreIsCompoundFile = flushDocStores();
        flushDocStores = false;
      }

      String segment = docWriter.getSegment();

      // If we are flushing docs, segment must not be null:
      assert segment != null || !flushDocs;

      if (flushDocs) {

        boolean success = false;
        final int flushedDocCount;

        try {
          flushedDocCount = docWriter.flush(flushDocStores);
          success = true;
        } finally {
          if (!success) {
            if (infoStream != null)
              message("hit exception flushing segment " + segment);
            deleter.refresh(segment);
          }
        }
        
        if (0 == docStoreOffset && flushDocStores) {
          // This means we are flushing private doc stores
          // with this segment, so it will not be shared
          // with other segments
          assert docStoreSegment != null;
          assert docStoreSegment.equals(segment);
          docStoreOffset = -1;
          docStoreIsCompoundFile = false;
          docStoreSegment = null;
        }

        // Create new SegmentInfo, but do not add to our
        // segmentInfos until deletes are flushed
        // successfully.
        newSegment = new SegmentInfo(segment,
                                     flushedDocCount,
                                     directory, false, true,
                                     docStoreOffset, docStoreSegment,
                                     docStoreIsCompoundFile,    
                                     docWriter.hasProx());
        setDiagnostics(newSegment, "flush");
      }

      docWriter.pushDeletes();

      if (flushDocs) {
        segmentInfos.add(newSegment);
        checkpoint();
      }

      if (flushDocs && mergePolicy.useCompoundFile(segmentInfos, newSegment)) {
        // Now build compound file
        boolean success = false;
        try {
          docWriter.createCompoundFile(segment);
          success = true;
        } finally {
          if (!success) {
            if (infoStream != null)
              message("hit exception creating compound file for newly flushed segment " + segment);
            deleter.deleteFile(IndexFileNames.segmentFileName(segment, IndexFileNames.COMPOUND_FILE_EXTENSION));
          }
        }

        newSegment.setUseCompoundFile(true);
        checkpoint();
      }

      if (flushDeletes) {
        applyDeletes();
      }
      
      if (flushDocs)
        checkpoint();

      doAfterFlush();

      return flushDocs;

    } catch (OutOfMemoryError oom) {
      handleOOM(oom, "doFlush");
      // never hit
      return false;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

