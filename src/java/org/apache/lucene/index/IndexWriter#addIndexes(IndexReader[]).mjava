  /** Merges the provided indexes into this index.
   * <p>After this completes, the index is optimized. </p>
   * <p>The provided IndexReaders are not closed.</p>

   * <p><b>NOTE:</b> the index in each Directory must not be
   * changed (opened by a writer) while this method is
   * running.  This method does not acquire a write lock in
   * each input Directory, so it is up to the caller to
   * enforce this.
   *
   * <p><b>NOTE:</b> while this is running, any attempts to
   * add or delete documents (with another thread) will be
   * paused until this method completes.
   *
   * <p>See {@link #addIndexes(Directory[])} for
   * details on transactional semantics, temporary free
   * space required in the Directory, and non-CFS segments
   * on an Exception.</p>
   * @throws CorruptIndexException if the index is corrupt
   * @throws IOException if there is a low-level IO error
   */
  public void addIndexes(IndexReader[] readers)
    throws CorruptIndexException, IOException {

    ensureOpen();

    // Do not allow add docs or deletes while we are running:
    docWriter.pauseAllThreads();

    try {
      optimize();					  // start with zero or 1 seg

      final String mergedName = newSegmentName();
      SegmentMerger merger = new SegmentMerger(this, mergedName, null);

      SegmentInfo info;

      IndexReader sReader = null;
      try {
        synchronized(this) {
          if (segmentInfos.size() == 1){ // add existing index, if any
            sReader = SegmentReader.get(segmentInfos.info(0));
            merger.add(sReader);
          }
        }

        for (int i = 0; i < readers.length; i++)      // add new indexes
          merger.add(readers[i]);

        boolean success = false;

        startTransaction();

        try {
          int docCount = merger.merge();                // merge 'em

          if(sReader != null) {
            sReader.close();
            sReader = null;
          }

          synchronized(this) {
            segmentInfos.setSize(0);                      // pop old infos & add new
            info = new SegmentInfo(mergedName, docCount, directory, false, true,
                                   -1, null, false);
            segmentInfos.addElement(info);
          }

          // Notify DocumentsWriter that the flushed count just increased
          docWriter.updateFlushedDocCount(docCount);

          success = true;

        } finally {
          if (!success) {
            if (infoStream != null)
              message("hit exception in addIndexes during merge");

            rollbackTransaction();
          } else {
            commitTransaction();
          }
        }
      } finally {
        if (sReader != null) {
          sReader.close();
        }
      }
    
      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {

        boolean success = false;

        startTransaction();

        try {
          merger.createCompoundFile(mergedName + ".cfs");
          synchronized(this) {
            info.setUseCompoundFile(true);
          }
        } finally {
          if (!success) {
            if (infoStream != null)
              message("hit exception building compound file in addIndexes during merge");

            rollbackTransaction();
          } else {
            commitTransaction();
          }
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

