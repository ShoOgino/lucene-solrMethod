  /** Merges the provided indexes into this index.
   * <p>After this completes, the index is optimized. </p>
   * <p>The provided IndexReaders are not closed.</p>

   * <p><b>NOTE:</b> the index in each Directory must not be
   * changed (opened by a writer) while this method is
   * running.  This method does not acquire a write lock in
   * each input Directory, so it is up to the caller to
   * enforce this.
   *
   * <p><b>NOTE:</b> while this is running, any attempts to
   * add or delete documents (with another thread) will be
   * paused until this method completes.
   *
   * <p>See {@link #addIndexesNoOptimize(Directory[])} for
   * details on transactional semantics, temporary free
   * space required in the Directory, and non-CFS segments
   * on an Exception.</p>
   *
   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError
   * you should immediately close the writer.  See <a
   * href="#OOME">above</a> for details.</p>
   *
   * @throws CorruptIndexException if the index is corrupt
   * @throws IOException if there is a low-level IO error
   */
  public void addIndexes(IndexReader[] readers)
    throws CorruptIndexException, IOException {

    ensureOpen();

    // Do not allow add docs or deletes while we are running:
    docWriter.pauseAllThreads();

    // We must pre-acquire a read lock here (and upgrade to
    // write lock in startTransaction below) so that no
    // other addIndexes is allowed to start up after we have
    // flushed & optimized but before we then start our
    // transaction.  This is because the merging below
    // requires that only one segment is present in the
    // index:
    acquireRead();

    try {

      SegmentInfo info = null;
      String mergedName = null;
      SegmentMerger merger = null;

      boolean success = false;

      try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the read lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseRead();
      }

      // true means we already have a read lock; if this
      // call hits an exception it will release the write
      // lock:
      startTransaction(true);

      try {
        mergedName = newSegmentName();
        merger = new SegmentMerger(this, mergedName, null);

        IndexReader sReader = null;
        synchronized(this) {
          if (segmentInfos.size() == 1) { // add existing index, if any
            sReader = SegmentReader.get(true, segmentInfos.info(0));
          }
        }

        try {
          if (sReader != null)
            merger.add(sReader);

          for (int i = 0; i < readers.length; i++)      // add new indexes
            merger.add(readers[i]);

          int docCount = merger.merge();                // merge 'em

          if(sReader != null) {
            sReader.close();
            sReader = null;
          }

          synchronized(this) {
            segmentInfos.clear();                      // pop old infos & add new
            info = new SegmentInfo(mergedName, docCount, directory, false, true,
                                   -1, null, false, merger.hasProx());
            segmentInfos.add(info);
          }

          // Notify DocumentsWriter that the flushed count just increased
          docWriter.updateFlushedDocCount(docCount);

          success = true;

        } finally {
          if (sReader != null) {
            sReader.close();
          }
        }
      } finally {
        if (!success) {
          if (infoStream != null)
            message("hit exception in addIndexes during merge");
          rollbackTransaction();
        } else {
          commitTransaction();
        }
      }
    
      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {

        List files = null;

        synchronized(this) {
          // Must incRef our files so that if another thread
          // is running merge/optimize, it doesn't delete our
          // segment's files before we have a change to
          // finish making the compound file.
          if (segmentInfos.contains(info)) {
            files = info.files();
            deleter.incRef(files);
          }
        }

        if (files != null) {

          success = false;

          startTransaction(false);

          try {
            merger.createCompoundFile(mergedName + ".cfs");
            synchronized(this) {
              info.setUseCompoundFile(true);
            }
          
            success = true;
          
          } finally {

            deleter.decRef(files);

            if (!success) {
              if (infoStream != null)
                message("hit exception building compound file in addIndexes during merge");

              rollbackTransaction();
            } else {
              commitTransaction();
            }
          }
        }
      }
    } catch (OutOfMemoryError oom) {
      hitOOM = true;
      throw oom;
    } finally {
      docWriter.resumeAllThreads();
    }
  }

