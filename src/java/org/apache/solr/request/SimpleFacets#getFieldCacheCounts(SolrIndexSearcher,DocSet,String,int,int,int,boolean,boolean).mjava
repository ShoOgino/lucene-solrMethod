  /**
   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.
   * The field must have at most one indexed token per document.
   */
  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, boolean sort) throws IOException {
    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,
    //  we should use an alternate strategy to avoid
    //  1) creating another huge int[] for the counts
    //  2) looping over that huge int[] looking for the rare non-zeros.
    //
    // Yet another variation: if docs.size() is small and termvectors are stored,
    // then use them instead of the FieldCache.
    //

    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);
    final int[] count = new int[si.lookup.length];
    DocIterator iter = docs.iterator();
    while (iter.hasNext()) {
      count[si.order[iter.nextDoc()]]++;
    }

    FieldType ft = searcher.getSchema().getFieldType(fieldName);
    NamedList res = new NamedList();

    // IDEA: we could also maintain a count of "other"... everything that fell outside
    // of the top 'N'

    int off=offset;
    int lim=limit>=0 ? limit : Integer.MAX_VALUE;

    if (sort) {
      // TODO: compare performance of BoundedTreeSet compare against priority queue?
      int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;
      final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);
      int min=mincount-1;  // the smallest value in the top 'N' values
      for (int i=1; i<count.length; i++) {
        int c = count[i];
        if (c>min) {
          // NOTE: we use c>min rather than c>=min as an optimization because we are going in
          // index order, so we already know that the keys are ordered.  This can be very
          // important if a lot of the counts are repeated (like zero counts would be).
          queue.add(new CountPair<String,Integer>(ft.indexedToReadable(si.lookup[i]), c));
          if (queue.size()>=maxsize) min=queue.last().val;
        }
      }
      for (CountPair<String,Integer> p : queue) {
        if (--off>=0) continue;
        if (--lim<0) break;
        res.add(p.key, p.val);
      }
    } else if (mincount<=0) {
      // This is an optimization... if mincount<=0 and we aren't sorting then
      // we know exactly where to start and end in the fieldcache.
      for (int i=offset+1; i<offset+1+limit; i++) {
        res.add(ft.indexedToReadable(si.lookup[i]),count[i]);
      }
    } else {
      for (int i=1; i<count.length; i++) {
        int c = count[i];
        if (c<mincount || --off>=0) continue;
        if (--lim<0) break;
        res.add(ft.indexedToReadable(si.lookup[i]), c);      
      }
    }

    if (missing) res.add(null, count[0]);
    return res;
  }

