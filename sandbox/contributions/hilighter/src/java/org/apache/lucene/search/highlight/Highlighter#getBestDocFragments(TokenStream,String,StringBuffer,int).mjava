	/**
	 * Low level api to get the most relevant sections of the document
	 * @param tokenStream
	 * @param text
	 * @param maxNumFragments
	 * @return 
	 * @throws IOException
	 */
	private final TextFragment[] getBestDocFragments(
		TokenStream tokenStream,	
		String text,
		StringBuffer newText,
		int maxNumFragments)
		throws IOException
	{
		ArrayList docFrags = new ArrayList();

		TextFragment currentFrag =	new TextFragment(newText.length(), docFrags.size());
		fragmentScorer.startFragment(currentFrag);
		docFrags.add(currentFrag);
	
		FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);

		try
		{
			org.apache.lucene.analysis.Token token;
			String tokenText;
			int startOffset;
			int endOffset;
			int lastEndOffset = 0;
			textFragmenter.start(text);

			while ((token = tokenStream.next()) != null)
			{
				
				startOffset = token.startOffset();
				endOffset = token.endOffset();		
				//FIXME an issue was reported with CJKTokenizer that I couldnt reproduce
				// where the analyzer was producing overlapping tokens.
				// I suspect the fix is to make startOffset=Math.max(startOffset,lastEndOffset+1)
				// but cant be sure so I'll just leave this comment in for now
				tokenText = text.substring(startOffset, endOffset);


				// append text between end of last token (or beginning of text) and start of current token
				if (startOffset > lastEndOffset)
					newText.append(text.substring(lastEndOffset, startOffset));

				// does query contain current token?
				float score=fragmentScorer.getTokenScore(token);			
				newText.append(formatter.highlightTerm(tokenText, token.termText(), score, startOffset));
				

				if(textFragmenter.isNewFragment(token))
				{
					currentFrag.setScore(fragmentScorer.getFragmentScore());
					//record stats for a new fragment
					currentFrag.textEndPos = newText.length();
					currentFrag =new TextFragment(newText.length(), docFrags.size());
					fragmentScorer.startFragment(currentFrag);
					docFrags.add(currentFrag);
				}

				lastEndOffset = endOffset;
				if(lastEndOffset>maxDocBytesToAnalyze)
				{
					break;
				}
			}
			currentFrag.setScore(fragmentScorer.getFragmentScore());
			

			// append text after end of last token
			if (lastEndOffset < text.length())
				newText.append(text.substring(lastEndOffset));

			currentFrag.textEndPos = newText.length();

			//sort the most relevant sections of the text
			int minScore = 0;
			for (Iterator i = docFrags.iterator(); i.hasNext();)
			{
				currentFrag = (TextFragment) i.next();

				//If you are running with a version of Lucene before 11th Sept 03
				// you do not have PriorityQueue.insert() - so uncomment the code below					
				/*
									if (currentFrag.getScore() >= minScore)
									{
										fragQueue.put(currentFrag);
										if (fragQueue.size() > maxNumFragments)
										{ // if hit queue overfull
											fragQueue.pop(); // remove lowest in hit queue
											minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore
										}
										
					
									}
				*/
				//The above code caused a problem as a result of Christoph Goller's 11th Sept 03
				//fix to PriorityQueue. The correct method to use here is the new "insert" method
				// USE ABOVE CODE IF THIS DOES NOT COMPILE!
				fragQueue.insert(currentFrag);
			}

			//return the most relevant fragments
			TextFragment frag[] = new TextFragment[fragQueue.size()];
			for (int i = frag.length - 1; i >= 0; i--)
			{
				frag[i] = (TextFragment) fragQueue.pop();
			}
			return frag;

		}
		finally
		{
			if (tokenStream != null)
			{
				try
				{
					tokenStream.close();
				}
				catch (Exception e)
				{
				}
			}
		}
	}

