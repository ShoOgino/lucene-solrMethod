  /**
   * 生成两两词之间的二叉图表，将结果保存在一个MultiTokenPairMap中
   * 
   * @param segGraph 所有的Token列表
   * @param smooth 平滑系数
   * @param biDict 二叉词典
   * @return
   * 
   * @see MultiTokenPairMap
   */
  private void generateBiSegGraph(SegGraph segGraph) {
    double smooth = 0.1;
    int wordPairFreq = 0;
    int maxStart = segGraph.getMaxStart();
    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;

    int next;
    char[] idBuffer;
    // 为segGraph中的每个元素赋以一个下标
    segTokenList = segGraph.makeIndex();
    // 因为startToken（"始##始"）的起始位置是-1因此key为-1时可以取出startToken
    int key = -1;
    List nextTokens = null;
    while (key < maxStart) {
      if (segGraph.isStartExist(key)) {

        List tokenList = segGraph.getStartList(key);

        // 为某一个key对应的所有Token都计算一次
        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {
          SegToken t1 = (SegToken) iter.next();
          oneWordFreq = t1.weight;
          next = t1.endOffset;
          nextTokens = null;
          // 找到下一个对应的Token，例如“阳光海岸”，当前Token是“阳光”， 下一个Token可以是“海”或者“海岸”
          // 如果找不到下一个Token，则说明到了末尾，重新循环。
          while (next <= maxStart) {
            // 因为endToken的起始位置是sentenceLen，因此等于sentenceLen是可以找到endToken
            if (segGraph.isStartExist(next)) {
              nextTokens = segGraph.getStartList(next);
              break;
            }
            next++;
          }
          if (nextTokens == null) {
            break;
          }
          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {
            SegToken t2 = (SegToken) iter2.next();
            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];
            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);
            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;
            System.arraycopy(t2.charArray, 0, idBuffer,
                t1.charArray.length + 1, t2.charArray.length);

            // Two linked Words frequency
            wordPairFreq = bigramDict.getFrequency(idBuffer);

            // Smoothing

            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1
            weight = -Math
                .log(smooth
                    * (1.0 + oneWordFreq)
                    / (Utility.MAX_FREQUENCE + 0.0)
                    + (1.0 - smooth)
                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));

            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,
                t2.index, weight);
            this.addSegTokenPair(tokenPair);
          }
        }
      }
      key++;
    }

  }

