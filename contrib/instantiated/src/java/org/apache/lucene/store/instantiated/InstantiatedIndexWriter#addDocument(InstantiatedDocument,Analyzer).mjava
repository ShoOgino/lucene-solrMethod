  /**
   * Tokenizes a document and adds it to the buffer.
   * Try to do all calculations in this method rather than in commit, as this is a non locking method.
   * Remember, this index implementation expects unlimited memory for maximum speed.
   *
   * @param document
   * @param analyzer
   * @throws IOException
   */
  protected void addDocument(InstantiatedDocument document, Analyzer analyzer) throws IOException {

    if (document.getDocumentNumber() != null) {
      throw new RuntimeException("Document number already set! Are you trying to add a document that already is bound to this or another index?");
    }

    // todo: write lock

    // normalize settings per field name in document

    Map<String /* field name */, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();
    for (Field field : (List<Field>) document.getDocument().getFields()) {
      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());
      if (fieldSettings == null) {
        fieldSettings = new FieldSetting();
        fieldSettings.fieldName = field.name().intern();
        fieldSettingsByFieldName.put(fieldSettings.fieldName, fieldSettings);
        fieldNameBuffer.add(fieldSettings.fieldName);
      }

      // todo: fixme: multiple fields with the same name does not mean field boost += more boost.
      fieldSettings.boost *= field.getBoost();
      //fieldSettings.dimensions++;

      // once fieldSettings, always fieldSettings.
      if (field.getOmitNorms() != fieldSettings.omitNorms) {
        fieldSettings.omitNorms = true;
      }
      if (field.isIndexed() != fieldSettings.isIndexed) {
        fieldSettings.isIndexed = true;
      }
      if (field.isTokenized() != fieldSettings.isTokenized) {
        fieldSettings.isTokenized = true;
      }
      if (field.isCompressed() != fieldSettings.isCompressed) {
        fieldSettings.isCompressed = true;
      }
      if (field.isStored() != fieldSettings.isStored) {
        fieldSettings.isStored = true;
      }
      if (field.isBinary() != fieldSettings.isBinary) {
        fieldSettings.isBinary = true;
      }
      if (field.isTermVectorStored() != fieldSettings.storeTermVector) {
        fieldSettings.storeTermVector = true;
      }
      if (field.isStorePositionWithTermVector() != fieldSettings.storePositionWithTermVector) {
        fieldSettings.storePositionWithTermVector = true;
      }
      if (field.isStoreOffsetWithTermVector() != fieldSettings.storeOffsetWithTermVector) {
        fieldSettings.storeOffsetWithTermVector = true;
      }
    }

    Map<Field, LinkedList<Token>> tokensByField = new LinkedHashMap<Field, LinkedList<Token>>(20);

    // tokenize indexed fields.
    for (Iterator<Field> it = (Iterator<Field>) document.getDocument().getFields().iterator(); it.hasNext();) {

      Field field = it.next();

      FieldSetting fieldSettings = fieldSettingsByFieldName.get(field.name());

      if (field.isIndexed()) {

        LinkedList<Token> tokens = new LinkedList<Token>();
        tokensByField.put(field, tokens);

        if (field.isTokenized()) {
          int termCounter = 0;
          final TokenStream tokenStream;
          // todo readerValue(), binaryValue()
          if (field.tokenStreamValue() != null) {
            tokenStream = field.tokenStreamValue();
          } else {
            tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));
          }
          Token next = tokenStream.next();

          while (next != null) {
            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?
            tokens.add(next); // the vector will be built on commit.
            next = tokenStream.next();
            fieldSettings.fieldLength++;
            if (fieldSettings.fieldLength > maxFieldLength) {
              break;
            }
          }
        } else {
          // untokenized
          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), "untokenized"));
          fieldSettings.fieldLength++;
        }
      }

      if (!field.isStored()) {
        it.remove();
      }
    }


    Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> termDocumentInformationFactoryByTermTextAndFieldSetting = new HashMap<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>();
    termDocumentInformationFactoryByDocument.put(document, termDocumentInformationFactoryByTermTextAndFieldSetting);

    // build term vector, term positions and term offsets
    for (Map.Entry<Field, LinkedList<Token>> eField_Tokens : tokensByField.entrySet()) {
      FieldSetting fieldSettings = fieldSettingsByFieldName.get(eField_Tokens.getKey().name());

      Map<String, TermDocumentInformationFactory> termDocumentInformationFactoryByTermText = termDocumentInformationFactoryByTermTextAndFieldSetting.get(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()));
      if (termDocumentInformationFactoryByTermText == null) {
        termDocumentInformationFactoryByTermText = new HashMap<String /*text*/, TermDocumentInformationFactory>();
        termDocumentInformationFactoryByTermTextAndFieldSetting.put(fieldSettingsByFieldName.get(eField_Tokens.getKey().name()), termDocumentInformationFactoryByTermText);
      }

      int lastOffset = 0;

      // for each new field, move positions a bunch.
      if (fieldSettings.position > 0) {
        // todo what if no analyzer set, multiple fields with same name and index without tokenization?
        fieldSettings.position += analyzer.getPositionIncrementGap(fieldSettings.fieldName);
      }

      for (Token token : eField_Tokens.getValue()) {

        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());
        if (termDocumentInformationFactory == null) {
          termDocumentInformationFactory = new TermDocumentInformationFactory();
          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);
        }
        //termDocumentInformationFactory.termFrequency++;

        fieldSettings.position += (token.getPositionIncrement() - 1);
        termDocumentInformationFactory.termPositions.add(fieldSettings.position++);

        if (token.getPayload() != null && token.getPayload().length() > 0) {
          termDocumentInformationFactory.payloads.add(token.getPayload().toByteArray());
        } else {
          termDocumentInformationFactory.payloads.add(null);
        }

        if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {

          termDocumentInformationFactory.termOffsets.add(new TermVectorOffsetInfo(fieldSettings.offset + token.startOffset(), fieldSettings.offset + token.endOffset()));
          lastOffset = fieldSettings.offset + token.endOffset();
        }


      }

      if (eField_Tokens.getKey().isStoreOffsetWithTermVector()) {
        fieldSettings.offset = lastOffset + 1;
      }

    }


    unflushedDocuments.add(document);

    // if too many documents in buffer, commit.
    if (unflushedDocuments.size() >= getMergeFactor()) {
      commit(/*lock*/);
    }

    // todo: unlock write lock

  }

