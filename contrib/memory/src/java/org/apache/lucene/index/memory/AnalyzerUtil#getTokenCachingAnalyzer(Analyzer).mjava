  /**
   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
   * token stream, and delivers those cached tokens on subsequent matching calls to 
   * <code>tokenStream(String fieldName, Reader reader)</code>.
   * <p>
   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can 
   * help improve performance if the same document is added to multiple Lucene indexes, 
   * because the text analysis phase need not be performed more than once.
   * <p>
   * Caveats: 
   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. 
   * 3) The Token instances delivered by the underlying child analyzer must be immutable.
   * 
   * @param child
   *            the underlying child analyzer
   * @return a new analyzer
   */
  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {

    if (child == null)
      throw new IllegalArgumentException("child analyzer must not be null");

    return new Analyzer() {

      private final HashMap cache = new HashMap();

      public TokenStream tokenStream(String fieldName, Reader reader) {
        final ArrayList tokens = (ArrayList) cache.get(fieldName);
        if (tokens == null) { // not yet cached
          final ArrayList tokens2 = new ArrayList();
          cache.put(fieldName, tokens2);
          return new TokenFilter(child.tokenStream(fieldName, reader)) {

            public Token next() throws IOException {
              Token token = input.next(); // from filter super class
              if (token != null) tokens2.add(token);
              return token;
            }
          };
        } else { // already cached
          return new TokenStream() {

            private Iterator iter = tokens.iterator();

            public Token next() {
              if (!iter.hasNext()) return null;
              return (Token) iter.next();
            }
          };
        }
      }
    };
  }

