  /**
   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
   * token stream, and delivers those cached tokens on subsequent calls to 
   * <code>tokenStream(String fieldName, Reader reader)</code>.
   * <p>
   * This can help improve performance in the presence of expensive Analyzer / TokenFilter chains.
   * <p>
   * Caveats: 
   * 1) Caching only works if the methods equals() and hashCode() methods are properly 
   * implemented on the Reader passed to <code>tokenStream(String fieldName, Reader reader)</code>.
   * 2) Caching the tokens of large Lucene documents can lead to out of memory exceptions. 
   * 3) The Token instances delivered by the underlying child analyzer must be immutable.
   * 
   * @param child
   *            the underlying child analyzer
   * @return a new analyzer
   */
  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {

    if (child == null)
      throw new IllegalArgumentException("child analyzer must not be null");

    return new Analyzer() {

      private final HashMap cache = new HashMap();

      public TokenStream tokenStream(String fieldName, Reader reader) {
        Pair key = new Pair(fieldName, reader);
        final ArrayList tokens = (ArrayList) cache.get(key);
        if (tokens == null) { // not yet cached
          final ArrayList tokens2 = new ArrayList();
          cache.put(key, tokens2);
          return new TokenFilter(child.tokenStream(fieldName, reader)) {

            public Token next() throws IOException {
              Token token = input.next(); // from filter super class
              if (token != null) tokens2.add(token);
              return token;
            }
          };
        } else { // already cached
          return new TokenStream() {

            private Iterator iter = tokens.iterator();

            public Token next() {
              if (!iter.hasNext()) return null;
              return (Token) iter.next();
            }
          };
        }
      }
    };
  }

