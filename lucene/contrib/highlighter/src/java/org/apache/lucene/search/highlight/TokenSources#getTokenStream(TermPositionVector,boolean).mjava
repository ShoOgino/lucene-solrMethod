  /**
   * Low level api. Returns a token stream or null if no offset info available
   * in index. This can be used to feed the highlighter with a pre-parsed token
   * stream
   * 
   * In my tests the speeds to recreate 1000 token streams using this method
   * are: - with TermVector offset only data stored - 420 milliseconds - with
   * TermVector offset AND position data stored - 271 milliseconds (nb timings
   * for TermVector with position data are based on a tokenizer with contiguous
   * positions - no overlaps or gaps) The cost of not using TermPositionVector
   * to store pre-parsed content and using an analyzer to re-parse the original
   * content: - reanalyzing the original content - 980 milliseconds
   * 
   * The re-analyze timings will typically vary depending on - 1) The complexity
   * of the analyzer code (timings above were using a
   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene
   * reads ALL fields off the disk when accessing just one document field - can
   * cost dear!) 3) Use of compression on field storage - could be faster due to
   * compression (less disk IO) or slower (more CPU burn) depending on the
   * content.
   * 
   * @param tpv
   * @param tokenPositionsGuaranteedContiguous true if the token position
   *        numbers have no overlaps or gaps. If looking to eek out the last
   *        drops of performance, set to true. If in doubt, set to false.
   */
  public static TokenStream getTokenStream(TermPositionVector tpv,
      boolean tokenPositionsGuaranteedContiguous) {
    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {
      return new TokenStreamFromTermPositionVector(tpv);
    }

    // an object used to iterate across an array of tokens
    final class StoredTokenStream extends TokenStream {
      Token tokens[];

      int currentToken = 0;

      CharTermAttribute termAtt;

      OffsetAttribute offsetAtt;

      PositionIncrementAttribute posincAtt;

      StoredTokenStream(Token tokens[]) {
        this.tokens = tokens;
        termAtt = addAttribute(CharTermAttribute.class);
        offsetAtt = addAttribute(OffsetAttribute.class);
        posincAtt = addAttribute(PositionIncrementAttribute.class);
      }

      @Override
      public boolean incrementToken() throws IOException {
        if (currentToken >= tokens.length) {
          return false;
        }
        Token token = tokens[currentToken++];
        clearAttributes();
        termAtt.setEmpty().append(token);
        offsetAtt.setOffset(token.startOffset(), token.endOffset());
        posincAtt
            .setPositionIncrement(currentToken <= 1
                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]
                    .startOffset() ? 1 : 0);
        return true;
      }
    }
    // code to reconstruct the original sequence of Tokens
    BytesRef[] terms = tpv.getTerms();
    int[] freq = tpv.getTermFrequencies();
    int totalTokens = 0;
    for (int t = 0; t < freq.length; t++) {
      totalTokens += freq[t];
    }
    Token tokensInOriginalOrder[] = new Token[totalTokens];
    ArrayList<Token> unsortedTokens = null;
    for (int t = 0; t < freq.length; t++) {
      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);
      if (offsets == null) {
        throw new IllegalArgumentException(
            "Required TermVector Offset information was not found");
      }

      int[] pos = null;
      if (tokenPositionsGuaranteedContiguous) {
        // try get the token position info to speed up assembly of tokens into
        // sorted sequence
        pos = tpv.getTermPositions(t);
      }
      if (pos == null) {
        // tokens NOT stored with positions or not guaranteed contiguous - must
        // add to list and sort later
        if (unsortedTokens == null) {
          unsortedTokens = new ArrayList<Token>();
        }
        for (int tp = 0; tp < offsets.length; tp++) {
          Token token = new Token(terms[t].utf8ToString(),
              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
          unsortedTokens.add(token);
        }
      } else {
        // We have positions stored and a guarantee that the token position
        // information is contiguous

        // This may be fast BUT wont work if Tokenizers used which create >1
        // token in same position or
        // creates jumps in position numbers - this code would fail under those
        // circumstances

        // tokens stored with positions - can use this to index straight into
        // sorted array
        for (int tp = 0; tp < pos.length; tp++) {
          Token token = new Token(terms[t].utf8ToString(),
              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
          tokensInOriginalOrder[pos[tp]] = token;
        }
      }
    }
    // If the field has been stored without position data we must perform a sort
    if (unsortedTokens != null) {
      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens
          .size()]);
      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {
        public int compare(Token t1, Token t2) {
          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()
              - t2.endOffset();
          else return t1.startOffset() - t2.startOffset();
        }
      });
    }
    return new StoredTokenStream(tokensInOriginalOrder);
  }

