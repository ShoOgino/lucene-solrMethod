  public void testRandomVectors() throws IOException {
    Directory dir = newDirectory();
    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));
    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);
    String[] sampleTerms = new String[RandomInts.randomIntBetween(random(), 20, 50)];
    for (int i = 0; i < sampleTerms.length; ++i) {
      sampleTerms[i] = _TestUtil.randomUnicodeString(random());
    }
    FieldType ft = randomFieldType();
    // generate random documents and index them
    final String[] fieldNames = new String[_TestUtil.nextInt(random(), 1, 200)];
    for (int i = 0; i < fieldNames.length; ++i) {
      String fieldName;
      do {
        fieldName = _TestUtil.randomSimpleString(random());
      } while ("id".equals(fieldName));
      fieldNames[i] = fieldName;
    }
    final int numDocs = _TestUtil.nextInt(random(), 10, 100);
    @SuppressWarnings("unchecked")
    final Map<String, RandomTokenStream>[] fieldValues  = new Map[numDocs];
    for (int i = 0; i < numDocs; ++i) {
      fieldValues[i] = new HashMap<String, RandomTokenStream>();
      final int numFields = _TestUtil.nextInt(random(), 0, rarely() ? fieldNames.length : 5);
      for (int j = 0; j < numFields; ++j) {
        final String fieldName = fieldNames[(i+j*31) % fieldNames.length];
        final int tokenStreamLen = _TestUtil.nextInt(random(), 1, rarely() ? 300 : 5);
        fieldValues[i].put(fieldName, new RandomTokenStream(tokenStreamLen, sampleTerms, rarely()));
      }
    }

    // index them
    for (int i = 0; i < numDocs; ++i) {
      Document doc = new Document();
      doc.add(new IntField("id", i, Store.YES));
      for (Map.Entry<String, RandomTokenStream> entry : fieldValues[i].entrySet()) {
        doc.add(new Field(entry.getKey(), entry.getValue(), ft));
      }
      iw.addDocument(doc);
    }

    iw.commit();
    // make sure the format can merge
    iw.forceMerge(2);

    // read term vectors
    final DirectoryReader reader = DirectoryReader.open(dir);
    for (int i = 0; i < 100; ++i) {
      final int docID = random().nextInt(numDocs);
      final Map<String, RandomTokenStream> fvs = fieldValues[reader.document(docID).getField("id").numericValue().intValue()];
      final Fields fields = reader.getTermVectors(docID);
      if (fvs.isEmpty()) {
        assertNull(fields);
      } else {
        Set<String> fns = new HashSet<String>();
        for (String field : fields) {
          fns.add(field);
        }
        assertEquals(fields.size(), fns.size());
        assertEquals(fvs.keySet(), fns);
        for (String field : fields) {
          final RandomTokenStream tk = fvs.get(field);
          assert tk != null;
          final Terms terms = fields.terms(field);
          assertEquals(ft.storeTermVectorPositions(), terms.hasPositions());
          assertEquals(ft.storeTermVectorOffsets(), terms.hasOffsets());
          assertEquals(1, terms.getDocCount());
          final TermsEnum termsEnum = terms.iterator(null);
          while (termsEnum.next() != null) {
            assertEquals(1, termsEnum.docFreq());
            final DocsAndPositionsEnum docsAndPositionsEnum = termsEnum.docsAndPositions(null, null);
            final DocsEnum docsEnum = docsAndPositionsEnum == null ? termsEnum.docs(null, null) : docsAndPositionsEnum;
            if (ft.storeTermVectorOffsets() || ft.storeTermVectorPositions()) {
              assertNotNull(docsAndPositionsEnum);
            }
            assertEquals(0, docsEnum.nextDoc());
            if (terms.hasPositions() || terms.hasOffsets()) {
              final int freq = docsEnum.freq();
              assertTrue(freq >= 1);
              if (docsAndPositionsEnum != null) {
                for (int k = 0; k < freq; ++k) {
                  final int position = docsAndPositionsEnum.nextPosition();
                  final Set<Integer> indexes;
                  if (terms.hasPositions()) {
                    indexes = tk.positionToTerms.get(position);
                    assertNotNull(tk.positionToTerms.keySet().toString() + " does not contain " + position, indexes);
                  } else {
                    indexes = tk.startOffsetToTerms.get(docsAndPositionsEnum.startOffset());
                    assertNotNull(indexes);
                  }
                  if (terms.hasPositions()) {
                    boolean foundPosition = false;
                    for (int index : indexes) {
                      if (new BytesRef(tk.terms[index]).equals(termsEnum.term()) && tk.positions[index] == position) {
                        foundPosition = true;
                        break;
                      }
                    }
                    assertTrue(foundPosition);
                  }
                  if (terms.hasOffsets()) {
                    boolean foundOffset = false;
                    for (int index : indexes) {
                      if (new BytesRef(tk.terms[index]).equals(termsEnum.term()) && tk.startOffsets[index] == docsAndPositionsEnum.startOffset() && tk.endOffsets[index] == docsAndPositionsEnum.endOffset()) {
                        foundOffset = true;
                        break;
                      }
                    }
                    assertTrue(foundOffset);
                  }
                  if (terms.hasPayloads()) {
                    boolean foundPayload = false;
                    for (int index : indexes) {
                      if (new BytesRef(tk.terms[index]).equals(termsEnum.term()) && equals(tk.payloads[index], docsAndPositionsEnum.getPayload())) {
                        foundPayload = true;
                        break;
                      }
                    }
                    assertTrue(foundPayload);
                  }
                }
              }
            }
            assertEquals(DocsEnum.NO_MORE_DOCS, docsEnum.nextDoc());
          }
        }
      }
    }
    IOUtils.close(reader, iw, dir);
  }

