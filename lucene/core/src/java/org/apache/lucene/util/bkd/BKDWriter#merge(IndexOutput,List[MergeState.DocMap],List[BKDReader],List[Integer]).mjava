  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already
   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing
   *  dimensional values were deleted. */
  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {
    if (numDims != 1) {
      throw new UnsupportedOperationException("numDims must be 1 but got " + numDims);
    }
    if (pointCount != 0) {
      throw new IllegalStateException("cannot mix add and merge");
    }

    //System.out.println("BKDW.merge segs=" + readers.size());

    // Catch user silliness:
    if (heapPointWriter == null && tempInput == null) {
      throw new IllegalStateException("already finished");
    }

    // Mark that we already finished:
    heapPointWriter = null;

    assert docMaps == null || readers.size() == docMaps.size();

    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());

    for(int i=0;i<readers.size();i++) {
      BKDReader bkd = readers.get(i);
      MergeState.DocMap docMap;
      if (docMaps == null) {
        docMap = null;
      } else {
        docMap = docMaps.get(i);
      }
      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));
      if (reader.next()) {
        queue.add(reader);
      }
    }

    if (queue.size() == 0) {
      return -1;
    }

    int leafCount = 0;
    List<Long> leafBlockFPs = new ArrayList<>();
    List<byte[]> leafBlockStartValues = new ArrayList<>();

    // Target halfway between min and max allowed for the leaf:
    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);
    //System.out.println("POINTS PER: " + pointsPerLeafBlock);

    byte[] lastPackedValue = new byte[bytesPerDim];
    byte[] firstPackedValue = new byte[bytesPerDim];
    long valueCount = 0;

    // Buffer up each leaf block's docs and values
    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];
    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];
    for(int i=0;i<maxPointsInLeafNode;i++) {
      leafBlockPackedValues[i] = new byte[packedBytesLength];
    }
    Arrays.fill(commonPrefixLengths, bytesPerDim);

    while (queue.size() != 0) {
      MergeReader reader = queue.top();
      // System.out.println("iter reader=" + reader);

      // NOTE: doesn't work with subclasses (e.g. SimpleText!)
      int docID = reader.docIDBase + reader.docID;
      leafBlockDocIDs[leafCount] = docID;
      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);
      docsSeen.set(docID);

      if (valueCount == 0) {
        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);
      }
      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);

      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);
      valueCount++;
      if (pointCount > totalPointCount) {
        throw new IllegalStateException("totalPointCount=" + totalPointCount + " was passed when we were created, but we just hit " + pointCount + " values");
      }

      if (leafCount == 0) {
        if (leafBlockFPs.size() > 0) {
          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:
          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));
        }
        Arrays.fill(commonPrefixLengths, bytesPerDim);
        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);
      } else {
        // Find per-dim common prefix:
        for(int dim=0;dim<numDims;dim++) {
          int offset = dim * bytesPerDim;
          for(int j=0;j<commonPrefixLengths[dim];j++) {
            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {
              commonPrefixLengths[dim] = j;
              break;
            }
          }
        }
      }

      leafCount++;

      if (reader.next()) {
        queue.updateTop();
      } else {
        // This segment was exhausted
        queue.pop();
      }

      // We write a block once we hit exactly the max count ... this is different from
      // when we flush a new segment, where we write between max/2 and max per leaf block,
      // so merged segments will behave differently from newly flushed segments:
      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {
        leafBlockFPs.add(out.getFilePointer());
        checkMaxLeafNodeCount(leafBlockFPs.size());

        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);
        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);

        // Write the full values:
        for (int i=0;i<leafCount;i++) {
          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);
        }

        leafCount = 0;
      }
    }

    pointCount = valueCount;

    long indexFP = out.getFilePointer();

    int numInnerNodes = leafBlockStartValues.size();

    //System.out.println("BKDW: now rotate numInnerNodes=" + numInnerNodes + " leafBlockStarts=" + leafBlockStartValues.size());

    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];
    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);
    long[] arr = new long[leafBlockFPs.size()];
    for(int i=0;i<leafBlockFPs.size();i++) {
      arr[i] = leafBlockFPs.get(i);
    }
    writeIndex(out, arr, index);
    return indexFP;
  }

