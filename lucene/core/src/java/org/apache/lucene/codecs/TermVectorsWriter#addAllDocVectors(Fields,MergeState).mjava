  /** Safe (but, slowish) default method to write every
   *  vector field in the document.  This default
   *  implementation requires that the vectors implement
   *  both Fields.size and
   *  Terms.size. */
  protected final void addAllDocVectors(Fields vectors, MergeState mergeState) throws IOException {
    if (vectors == null) {
      startDocument(0);
      return;
    }

    final int numFields = vectors.size();
    if (numFields == -1) {
      throw new IllegalStateException("vectors.size() must be implemented (it returned -1)");
    }
    startDocument(numFields);
    
    String lastFieldName = null;
    
    TermsEnum termsEnum = null;
    DocsAndPositionsEnum docsAndPositionsEnum = null;
    
    final ReaderPayloadProcessor readerPayloadProcessor = mergeState.currentReaderPayloadProcessor;
    PayloadProcessor payloadProcessor = null;

    for(String fieldName : vectors) {
      final FieldInfo fieldInfo = mergeState.fieldInfos.fieldInfo(fieldName);

      assert lastFieldName == null || fieldName.compareTo(lastFieldName) > 0: "lastFieldName=" + lastFieldName + " fieldName=" + fieldName;
      lastFieldName = fieldName;

      final Terms terms = vectors.terms(fieldName);
      if (terms == null) {
        // FieldsEnum shouldn't lie...
        continue;
      }
      
      final boolean hasPositions = terms.hasPositions();
      final boolean hasOffsets = terms.hasOffsets();
      final boolean hasPayloads = terms.hasPayloads();
      assert !hasPayloads || hasPositions;
      
      final int numTerms = (int) terms.size();
      if (numTerms == -1) {
        throw new IllegalStateException("terms.size() must be implemented (it returned -1)");
      }
      
      startField(fieldInfo, numTerms, hasPositions, hasOffsets, hasPayloads);
      termsEnum = terms.iterator(termsEnum);

      int termCount = 0;
      while(termsEnum.next() != null) {
        termCount++;

        final int freq = (int) termsEnum.totalTermFreq();
        
        startTerm(termsEnum.term(), freq);
        
        if (hasPayloads && readerPayloadProcessor != null) {
          payloadProcessor = readerPayloadProcessor.getProcessor(fieldName, termsEnum.term());
        }

        if (hasPositions || hasOffsets) {
          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
          assert docsAndPositionsEnum != null;
          
          final int docID = docsAndPositionsEnum.nextDoc();
          assert docID != DocIdSetIterator.NO_MORE_DOCS;
          assert docsAndPositionsEnum.freq() == freq;

          for(int posUpto=0; posUpto<freq; posUpto++) {
            final int pos = docsAndPositionsEnum.nextPosition();
            final int startOffset = docsAndPositionsEnum.startOffset();
            final int endOffset = docsAndPositionsEnum.endOffset();
            
            BytesRef payload = docsAndPositionsEnum.getPayload();
                
            if (payloadProcessor != null && payload != null) {
              // to not violate the D&P api, we must give the processor a private copy
              payload = BytesRef.deepCopyOf(payload);
              payloadProcessor.processPayload(payload);
              if (payload.length == 0) {
                // don't let PayloadProcessors corrumpt the index
                payload = null;
              }
            }

            assert !hasPositions || pos >= 0;
            addPosition(pos, startOffset, endOffset, payload);
          }
        }
      }
      assert termCount == numTerms;
    }
  }

