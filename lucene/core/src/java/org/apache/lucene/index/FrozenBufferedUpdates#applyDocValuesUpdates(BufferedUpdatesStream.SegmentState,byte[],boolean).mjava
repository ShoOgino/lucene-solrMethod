  private long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,
                                     byte[] updates, boolean isNumeric) throws IOException {

    TermsEnum termsEnum = null;
    PostingsEnum postingsEnum = null;

    // TODO: we can process the updates per DV field, from last to first so that
    // if multiple terms affect same document for the same field, we add an update
    // only once (that of the last term). To do that, we can keep a bitset which
    // marks which documents have already been updated. So e.g. if term T1
    // updates doc 7, and then we process term T2 and it updates doc 7 as well,
    // we don't apply the update since we know T1 came last and therefore wins
    // the update.
    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so
    // that these documents aren't even returned.

    long updateCount = 0;

    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */
    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();

    ByteArrayDataInput in = new ByteArrayDataInput(updates);

    String termField = null;
    String updateField = null;
    BytesRef term = new BytesRef();
    term.bytes = new byte[16];
    
    BytesRef scratch = new BytesRef();
    scratch.bytes = new byte[16];
    
    while (in.getPosition() != updates.length) {
      int code = in.readVInt();
      int docIDUpto = in.readVInt();
      term.length = code >> 2;
      
      if ((code & 1) != 0) {
        termField = in.readString();
      }
      if ((code & 2) != 0) {
        updateField = in.readString();
      }

      if (term.bytes.length < term.length) {
        term.bytes = ArrayUtil.grow(term.bytes, term.length);
      }
      in.readBytes(term.bytes, 0, term.length);

      int limit;
      if (delGen == segState.delGen) {
        assert privateSegment != null;
        limit = docIDUpto;
      } else {
        limit = Integer.MAX_VALUE;
      }
        
      // TODO: we traverse the terms in update order (not term order) so that we
      // apply the updates in the correct order, i.e. if two terms udpate the
      // same document, the last one that came in wins, irrespective of the
      // terms lexical order.
      // we can apply the updates in terms order if we keep an updatesGen (and
      // increment it with every update) and attach it to each NumericUpdate. Note
      // that we cannot rely only on docIDUpto because an app may send two updates
      // which will get same docIDUpto, yet will still need to respect the order
      // those updates arrived.

      // TODO: we could at least *collate* by field?

      // This is the field used to resolve to docIDs, e.g. an "id" field, not the doc values field we are updating!
      if ((code & 1) != 0) {
        Terms terms = segState.reader.terms(termField);
        if (terms != null) {
          termsEnum = terms.iterator();
        } else {
          termsEnum = null;
        }
      }

      // TODO: can we avoid boxing here w/o fully forking this method?
      Object value;
      if (isNumeric) {
        value = Long.valueOf(in.readZLong());
      } else {
        value = scratch;
        scratch.length = in.readVInt();
        if (scratch.bytes.length < scratch.length) {
          scratch.bytes = ArrayUtil.grow(scratch.bytes, scratch.length);
        }
        in.readBytes(scratch.bytes, 0, scratch.length);
      }

      if (termsEnum == null) {
        // no terms in this segment for this field
        continue;
      }

      if (termsEnum.seekExact(term)) {

        // we don't need term frequencies for this
        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);

        DocValuesFieldUpdates dvUpdates = holder.get(updateField);
        if (dvUpdates == null) {
          if (isNumeric) {
            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());
          } else {
            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());
          }

          holder.put(updateField, dvUpdates);
        }

        if (segState.rld.sortMap != null && privateSegment != null) {
          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:
          int doc;
          final Bits acceptDocs = segState.rld.getLiveDocs();
          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {

            if (acceptDocs != null && acceptDocs.get(doc) == false) {
              continue;
            }
            
            // The limit is in the pre-sorted doc space:
            if (segState.rld.sortMap.newToOld(doc) < limit) {
              dvUpdates.add(doc, value);
              updateCount++;
            }
          }
        } else {
          int doc;
          final Bits acceptDocs = segState.rld.getLiveDocs();
          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
            if (doc >= limit) {
              break; // no more docs that can be updated for this term
            }
            if (acceptDocs != null && acceptDocs.get(doc) == false) {
              continue;
            }
            dvUpdates.add(doc, value);
            updateCount++;
          }
        }
      }
    }

    // now freeze & publish:
    for (DocValuesFieldUpdates update : holder.values()) {
      if (update.any()) {
        update.finish();
        segState.rld.addDVUpdate(update);
      }
    }

    return updateCount;
  }

