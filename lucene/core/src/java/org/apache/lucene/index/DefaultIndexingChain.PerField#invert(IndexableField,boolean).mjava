    /** Inverts one field for one document; first is true
     *  if this is the first time we are seeing this field
     *  name in this document. */
    public void invert(IndexableField field, boolean first) throws IOException {
      if (first) {
        // First time we're seeing this field (indexed) in
        // this document:
        invertState.reset();
      }

      IndexableFieldType fieldType = field.fieldType();

      // if the field omits norms, the boost cannot be indexed.
      if (fieldType.omitNorms() && field.boost() != 1.0f) {
        throw new UnsupportedOperationException("You cannot set an index-time boost: norms are omitted for field '" + field.name() + "'");
      }

      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;
        
      // only bother checking offsets if something will consume them.
      // TODO: after we fix analyzers, also check if termVectorOffsets will be indexed.
      final boolean checkOffsets = fieldType.indexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;

      int lastStartOffset = 0;
        
      /*
       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream
       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,
       * but rather a finally that takes note of the problem.
       */
      boolean aborting = false;
      boolean succeededInProcessingField = false;
      try (TokenStream stream = field.tokenStream(docState.analyzer)) {
        // reset the TokenStream to the first token
        stream.reset();

        if (invertState.attributeSource != stream) {
          // EmptyTokenStream gets angry otherwise:
          invertState.termAttribute = stream.getAttribute(TermToBytesRefAttribute.class);
          invertState.posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);
          invertState.offsetAttribute = stream.addAttribute(OffsetAttribute.class);
          invertState.payloadAttribute = stream.getAttribute(PayloadAttribute.class);
          invertState.attributeSource = stream;
        }

        termsHashPerField.start(field, first);

        while (stream.incrementToken()) {

          // If we hit an exception in stream.next below
          // (which is fairly common, e.g. if analyzer
          // chokes on a given document), then it's
          // non-aborting and (above) this one document
          // will be marked as deleted, but still
          // consume a docID

          final int posIncr = invertState.posIncrAttribute.getPositionIncrement();
          if (posIncr < 0) {
            throw new IllegalArgumentException("position increment must be >=0 (got " + posIncr + ") for field '" + field.name() + "'");
          }
          if (invertState.position == 0 && posIncr == 0) {
            throw new IllegalArgumentException("first position increment must be > 0 (got 0) for field '" + field.name() + "'");
          }
          int position = invertState.position + posIncr;
          if (position > 0) {
            // NOTE: confusing: this "mirrors" the
            // position++ we do below
            position--;
          } else if (position < 0) {
            throw new IllegalArgumentException("position overflow for field '" + field.name() + "'");
          }
              
          // position is legal, we can safely place it in invertState now.
          // not sure if anything will use invertState after non-aborting exc...
          invertState.position = position;

          if (posIncr == 0) {
            invertState.numOverlap++;
          }
              
          if (checkOffsets) {
            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();
            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();
            if (startOffset < 0 || endOffset < startOffset) {
              throw new IllegalArgumentException("startOffset must be non-negative, and endOffset must be >= startOffset, "
                                                 + "startOffset=" + startOffset + ",endOffset=" + endOffset + " for field '" + field.name() + "'");
            }
            if (startOffset < lastStartOffset) {
              throw new IllegalArgumentException("offsets must not go backwards startOffset=" 
                                                 + startOffset + " is < lastStartOffset=" + lastStartOffset + " for field '" + field.name() + "'");
            }
            lastStartOffset = startOffset;
          }

          //System.out.println("  term=" + invertState.termAttribute);

          // If we hit an exception in here, we abort
          // all buffered documents since the last
          // flush, on the likelihood that the
          // internal state of the terms hash is now
          // corrupt and should not be flushed to a
          // new segment:
          aborting = true;
          termsHashPerField.add();
          aborting = false;

          invertState.length++;
          invertState.position++;
        }

        // trigger streams to perform end-of-stream operations
        stream.end();

        // TODO: maybe add some safety? then again, its already checked 
        // when we come back around to the field...
        invertState.position += invertState.posIncrAttribute.getPositionIncrement();
        invertState.offset += invertState.offsetAttribute.endOffset();

        /* if there is an exception coming through, we won't set this to true here:*/
        succeededInProcessingField = true;
      } catch (MaxBytesLengthExceededException e) {
        aborting = false;
        byte[] prefix = new byte[30];
        BytesRef bigTerm = invertState.termAttribute.getBytesRef();
        System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);
        String msg = "Document contains at least one immense term in field=\"" + fieldInfo.name + "\" (whose UTF8 encoding is longer than the max length " + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '" + Arrays.toString(prefix) + "...'";
        if (docState.infoStream.isEnabled("IW")) {
          docState.infoStream.message("IW", "ERROR: " + msg);
        }
        // Document will be deleted above:
        throw new IllegalArgumentException(msg);
      } finally {
        if (succeededInProcessingField == false && aborting) {
          docState.docWriter.setAborting();
        }

        if (!succeededInProcessingField && docState.infoStream.isEnabled("DW")) {
          docState.infoStream.message("DW", "An exception was thrown while processing field " + fieldInfo.name);
        }
      }

      if (analyzed) {
        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);
        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);
      }

      invertState.boost *= field.boost();
    }

