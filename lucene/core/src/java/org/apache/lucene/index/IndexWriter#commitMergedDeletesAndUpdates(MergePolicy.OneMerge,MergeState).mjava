  /**
   * Carefully merges deletes and updates for the segments we just merged. This
   * is tricky because, although merging will clear all deletes (compacts the
   * documents) and compact all the updates, new deletes and updates may have
   * been flushed to the segments since the merge was started. This method
   * "carries over" such new deletes and updates onto the newly merged segment,
   * and saves the resulting deletes and updates files (incrementing the delete
   * and DV generations for merge.info). If no deletes were flushed, no new
   * deletes file is saved.
   */
  synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {

    mergeFinishedGen.incrementAndGet();
    
    testPoint("startCommitMergeDeletes");

    final List<SegmentCommitInfo> sourceSegments = merge.segments;

    if (infoStream.isEnabled("IW")) {
      infoStream.message("IW", "commitMergeDeletes " + segString(merge.segments));
    }

    // Carefully merge deletes that occurred after we
    // started merging:
    long minGen = Long.MAX_VALUE;

    // Lazy init (only when we find a delete or update to carry over):
    final ReadersAndUpdates mergedDeletesAndUpdates = getPooledInstance(merge.info, true);
    
    // field -> delGen -> dv field updates
    Map<String,Map<Long,DocValuesFieldUpdates>> mappedDVUpdates = new HashMap<>();

    boolean anyDVUpdates = false;

    assert sourceSegments.size() == mergeState.docMaps.length;
    for (int i = 0; i < sourceSegments.size(); i++) {
      SegmentCommitInfo info = sourceSegments.get(i);
      minGen = Math.min(info.getBufferedDeletesGen(), minGen);
      final int maxDoc = info.info.maxDoc();
      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();
      final ReadersAndUpdates rld = getPooledInstance(info, false);
      // We hold a ref, from when we opened the readers during mergeInit, so it better still be in the pool:
      assert rld != null: "seg=" + info.info.name;
      final Bits currentLiveDocs = rld.getLiveDocs();

      MergeState.DocMap segDocMap = mergeState.docMaps[i];
      MergeState.DocMap segLeafDocMap = mergeState.leafDocMaps[i];

      if (prevLiveDocs != null) {

        // If we had deletions on starting the merge we must
        // still have deletions now:
        assert currentLiveDocs != null;
        assert prevLiveDocs.length() == maxDoc;
        assert currentLiveDocs.length() == maxDoc;

        // There were deletes on this segment when the merge
        // started.  The merge has collapsed away those
        // deletes, but, if new deletes were flushed since
        // the merge started, we must now carefully keep any
        // newly flushed deletes but mapping them to the new
        // docIDs.

        // Since we copy-on-write, if any new deletes were
        // applied after merging has started, we can just
        // check if the before/after liveDocs have changed.
        // If so, we must carefully merge the liveDocs one
        // doc at a time:
        if (currentLiveDocs != prevLiveDocs) {
          // This means this segment received new deletes
          // since we started the merge, so we
          // must merge them:
          for (int j = 0; j < maxDoc; j++) {
            if (prevLiveDocs.get(j) == false) {
              // if the document was deleted before, it better still be deleted!
              assert currentLiveDocs.get(j) == false;
            } else if (currentLiveDocs.get(j) == false) {
              // the document was deleted while we were merging:
              mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));
            }
          }
        }
      } else if (currentLiveDocs != null) {
        assert currentLiveDocs.length() == maxDoc;
        // This segment had no deletes before but now it
        // does:
        for (int j = 0; j < maxDoc; j++) {
          if (currentLiveDocs.get(j) == false) {
            mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));
          }
        }
      }

      // Now carry over all doc values updates that were resolved while we were merging, remapping the docIDs to the newly merged docIDs.
      // We only carry over packets that finished resolving; if any are still running (concurrently) they will detect that our merge completed
      // and re-resolve against the newly merged segment:
      
      Map<String,List<DocValuesFieldUpdates>> mergingDVUpdates = rld.getMergingDVUpdates();
      for (Map.Entry<String,List<DocValuesFieldUpdates>> ent : mergingDVUpdates.entrySet()) {

        String field = ent.getKey();

        Map<Long,DocValuesFieldUpdates> mappedField = mappedDVUpdates.get(field);
        if (mappedField == null) {
          mappedField = new HashMap<>();
          mappedDVUpdates.put(field, mappedField);
        }

        for (DocValuesFieldUpdates updates : ent.getValue()) {

          if (bufferedUpdatesStream.stillRunning(updates.delGen)) {
            continue;
          }
              
          // sanity check:
          assert field.equals(updates.field);

          DocValuesFieldUpdates mappedUpdates = mappedField.get(updates.delGen);
          if (mappedUpdates == null) {
            switch (updates.type) {
              case NUMERIC:
                mappedUpdates = new NumericDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());
                break;
              case BINARY:
                mappedUpdates = new BinaryDocValuesFieldUpdates(updates.delGen, updates.field, merge.info.info.maxDoc());
                break;
              default:
                throw new AssertionError();
            }
            mappedField.put(updates.delGen, mappedUpdates);
          }

          DocValuesFieldUpdates.Iterator it = updates.iterator();
          int doc;
          while ((doc = it.nextDoc()) != NO_MORE_DOCS) {
            int mappedDoc = segDocMap.get(segLeafDocMap.get(doc));
            if (mappedDoc != -1) {
              // not deleted
              mappedUpdates.add(mappedDoc, it);
              anyDVUpdates = true;
            }
          }
        }
      }
    }

    if (anyDVUpdates) {
      // Persist the merged DV updates onto the RAU for the merged segment:
      for(Map<Long,DocValuesFieldUpdates> d : mappedDVUpdates.values()) {
        for (DocValuesFieldUpdates updates : d.values()) {
          updates.finish();
          mergedDeletesAndUpdates.addDVUpdate(updates);
        }
      }
    }

    if (infoStream.isEnabled("IW")) {
      if (mergedDeletesAndUpdates == null) {
        infoStream.message("IW", "no new deletes or field updates since merge started");
      } else {
        String msg = mergedDeletesAndUpdates.getPendingDeleteCount() + " new deletes";
        if (anyDVUpdates) {
          msg += " and " + mergedDeletesAndUpdates.getNumDVUpdates() + " new field updates";
          msg += " (" + mergedDeletesAndUpdates.ramBytesUsed.get() + ") bytes";
        }
        msg += " since merge started";
        infoStream.message("IW", msg);
      }
    }

    merge.info.setBufferedDeletesGen(minGen);

    return mergedDeletesAndUpdates;
  }

