  @Override
  public T create(IndexReader reader) throws IOException
  {
    String field = StringHelper.intern(this.field); // TODO?? necessary?
    Terms terms = MultiFields.getTerms(reader, field);

    final boolean fasterButMoreRAM = hasOption(FASTER_BUT_MORE_RAM);

    final PagedBytes bytes = new PagedBytes(15);

    int startBytesBPV;
    int startTermsBPV;
    int startNumUniqueTerms;

    int maxDoc = reader.maxDoc();
    final int termCountHardLimit;
    if (maxDoc == Integer.MAX_VALUE) {
      termCountHardLimit = Integer.MAX_VALUE;
    } else {
      termCountHardLimit = maxDoc+1;
    }

    if (terms != null) {
      // Try for coarse estimate for number of bits; this
      // should be an underestimate most of the time, which
      // is fine -- GrowableWriter will reallocate as needed
      long numUniqueTerms = 0;
      try {
        numUniqueTerms = terms.getUniqueTermCount();
      } catch (UnsupportedOperationException uoe) {
        numUniqueTerms = -1;
      }
      if (numUniqueTerms != -1) {

        if (numUniqueTerms > termCountHardLimit) {
          // app is misusing the API (there is more than
          // one term per doc); in this case we make best
          // effort to load what we can (see LUCENE-2142)
          numUniqueTerms = termCountHardLimit;
        }

        startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);
        startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);

        startNumUniqueTerms = (int) numUniqueTerms;
      } else {
        startBytesBPV = 1;
        startTermsBPV = 1;
        startNumUniqueTerms = 1;
      }
    } else {
      startBytesBPV = 1;
      startTermsBPV = 1;
      startNumUniqueTerms = 1;
    }

    GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);
    final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);

    // 0 is reserved for "unset"
    bytes.copyUsingLengthPrefix(new BytesRef());
    int termOrd = 1;

    if (terms != null) {
      final TermsEnum termsEnum = terms.iterator();
      final Bits delDocs = MultiFields.getDeletedDocs(reader);
      DocsEnum docs = null;

      while(true) {
        final BytesRef term = termsEnum.next();
        if (term == null) {
          break;
        }
        if (termOrd >= termCountHardLimit) {
          break;
        }

        if (termOrd == termOrdToBytesOffset.size()) {
          // NOTE: this code only runs if the incoming
          // reader impl doesn't implement
          // getUniqueTermCount (which should be uncommon)
          termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));
        }
        termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));
        docs = termsEnum.docs(delDocs, docs);
        while (true) {
          final int docID = docs.nextDoc();
          if (docID == DocIdSetIterator.NO_MORE_DOCS) {
            break;
          }
          docToTermOrd.set(docID, termOrd);
        }
        termOrd++;
      }

      if (termOrdToBytesOffset.size() > termOrd) {
        termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);
      }
    }

    // maybe an int-only impl?
    return (T)new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);
  }

