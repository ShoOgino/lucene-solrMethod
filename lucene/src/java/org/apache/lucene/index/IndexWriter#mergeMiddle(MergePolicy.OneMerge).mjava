  /** Does the actual (time-consuming) work of the merge,
   *  but without holding synchronized lock on IndexWriter
   *  instance */
  private int mergeMiddle(MergePolicy.OneMerge merge)
    throws CorruptIndexException, IOException {
    
    merge.checkAborted(directory);

    final String mergedName = merge.info.name;
    
    SegmentMerger merger = null;

    int mergedDocCount = 0;

    SegmentInfos sourceSegments = merge.segments;
    final int numSegments = sourceSegments.size();

    if (infoStream != null)
      message("merging " + merge.segString(directory));

    merger = new SegmentMerger(directory, termIndexInterval, mergedName, merge, codecs, payloadProcessorProvider);

    merge.readers = new SegmentReader[numSegments];
    merge.readersClone = new SegmentReader[numSegments];

    boolean mergeDocStores = false;

    final String currentDocStoreSegment;
    synchronized(this) {
      currentDocStoreSegment = docWriter.getDocStoreSegment();
    }

    boolean currentDSSMerged = false;
      
    // This is try/finally to make sure merger's readers are
    // closed:
    boolean success = false;
    try {
      int totDocCount = 0;

      for (int i = 0; i < numSegments; i++) {
        final SegmentInfo info = sourceSegments.info(i);

        // Hold onto the "live" reader; we will use this to
        // commit merged deletes
        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,
                                                                 MERGE_READ_BUFFER_SIZE,
                                                                 -config.getReaderTermsIndexDivisor());

        // We clone the segment readers because other
        // deletes may come in while we're merging so we
        // need readers that will not change
        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);
        merger.add(clone);

        if (clone.hasDeletions()) {
          mergeDocStores = true;
        }
        
        if (info.getDocStoreOffset() != -1 && currentDocStoreSegment != null) {
          currentDSSMerged |= currentDocStoreSegment.equals(info.getDocStoreSegment());
        }

        totDocCount += clone.numDocs();
      }

      if (infoStream != null) {
        message("merge: total "+totDocCount+" docs");
      }

      merge.checkAborted(directory);

      // If deletions have arrived and it has now become
      // necessary to merge doc stores, go and open them:
      if (mergeDocStores && !merge.mergeDocStores) {
        merge.mergeDocStores = true;
        synchronized(this) {

          // If 1) we must now merge doc stores, and 2) at
          // least one of the segments we are merging uses
          // the doc store we are now writing to, we must at
          // this point force this doc store closed (by
          // calling flush).  If we didn't do this then the
          // readers will attempt to open an IndexInput
          // on files that have still-open IndexOutputs
          // against them:
          if (currentDSSMerged) {
            if (infoStream != null) {
              message("now flush at mergeMiddle");
            }
            doFlush(true, false);
            updatePendingMerges(1, false);
          }
        }

        for(int i=0;i<numSegments;i++) {
          merge.readersClone[i].openDocStores();
        }

        // Clear DSS
        merge.info.setDocStore(-1, null, false);
      }

      // This is where all the work happens:
      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);

      // Record which codec was used to write the segment
      merge.info.setSegmentCodecs(merger.getSegmentCodecs());

      if (infoStream != null) {
        message("merge segmentCodecs=" + merger.getSegmentCodecs());
      }
      
      assert mergedDocCount == totDocCount;

      // Very important to do this before opening the reader
      // because codec must know if prox was written for
      // this segment:
      //System.out.println("merger set hasProx=" + merger.hasProx() + " seg=" + merge.info.name);
      merge.info.setHasProx(merger.hasProx());

      if (merge.useCompoundFile) {

        success = false;
        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION);

        try {
          if (infoStream != null) {
            message("create compound file " + compoundFileName);
          }
          merger.createCompoundFile(compoundFileName, merge.info);
          success = true;
        } catch (IOException ioe) {
          synchronized(this) {
            if (merge.isAborted()) {
              // This can happen if rollback or close(false)
              // is called -- fall through to logic below to
              // remove the partially created CFS:
            } else {
              handleMergeException(ioe, merge);
            }
          }
        } catch (Throwable t) {
          handleMergeException(t, merge);
        } finally {
          if (!success) {
            if (infoStream != null) {
              message("hit exception creating compound file during merge");
            }

            synchronized(this) {
              deleter.deleteFile(compoundFileName);
              deleter.deleteNewFiles(merger.getMergedFiles(merge.info));
            }
          }
        }

        success = false;

        synchronized(this) {

          // delete new non cfs files directly: they were never
          // registered with IFD
          deleter.deleteNewFiles(merger.getMergedFiles(merge.info));

          if (merge.isAborted()) {
            if (infoStream != null) {
              message("abort merge after building CFS");
            }
            deleter.deleteFile(compoundFileName);
            return 0;
          }
        }

        merge.info.setUseCompoundFile(true);
      }

      final int termsIndexDivisor;
      final boolean loadDocStores;

      // if the merged segment warmer was not installed when
      // this merge was started, causing us to not force
      // the docStores to close, we can't warm it now
      final boolean canWarm = merge.info.getDocStoreSegment() == null || currentDocStoreSegment == null || !merge.info.getDocStoreSegment().equals(currentDocStoreSegment);

      if (poolReaders && mergedSegmentWarmer != null && canWarm) {
        // Load terms index & doc stores so the segment
        // warmer can run searches, load documents/term
        // vectors
        termsIndexDivisor = config.getReaderTermsIndexDivisor();
        loadDocStores = true;
      } else {
        termsIndexDivisor = -1;
        loadDocStores = false;
      }

      // TODO: in the non-realtime case, we may want to only
      // keep deletes (it's costly to open entire reader
      // when we just need deletes)

      final SegmentReader mergedReader = readerPool.get(merge.info, loadDocStores, BufferedIndexInput.BUFFER_SIZE, termsIndexDivisor);
      try {
        if (poolReaders && mergedSegmentWarmer != null) {
          mergedSegmentWarmer.warm(mergedReader);
        }

        if (!commitMerge(merge, merger, mergedDocCount, mergedReader)) {
          // commitMerge will return false if this merge was aborted
          return 0;
        }
      } finally {
        synchronized(this) {
          readerPool.release(mergedReader);
        }
      }
      success = true;

    } finally {
      // Readers are already closed in commitMerge if we didn't hit
      // an exc:
      if (!success) {
        closeMergeReaders(merge, true);
      }
    }

    return mergedDocCount;
  }

