  @Override
  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
    if (tvx != null) {
      // At least one doc in this run had term vectors enabled
      fill(state.numDocs);
      assert state.segmentName != null;
      String idxName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_INDEX_EXTENSION);
      String fldName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_FIELDS_EXTENSION);
      String docName = IndexFileNames.segmentFileName(state.segmentName, "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);

      tvx.close();
      tvf.close();
      tvd.close();
      tvx = null;
      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))
        throw new RuntimeException("after flush: tvx size mismatch: " + state.numDocs + " docs vs " + state.directory.fileLength(idxName) + " length in bytes of " + idxName + " file exists?=" + state.directory.fileExists(idxName));

      state.flushedFiles.add(idxName);
      state.flushedFiles.add(fldName);
      state.flushedFiles.add(docName);

      lastDocID = 0;
      state.hasVectors = hasVectors;
      hasVectors = false;
    }

    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {
      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;
      perField.termsHashPerField.reset();
      perField.shrinkHash();
    }
  }

