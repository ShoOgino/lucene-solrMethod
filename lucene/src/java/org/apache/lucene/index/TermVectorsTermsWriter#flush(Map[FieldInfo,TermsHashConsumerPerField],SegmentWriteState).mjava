  @Override
  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {

    if (tvx != null) {

      if (state.numDocsInStore > 0)
        // In case there are some final documents that we
        // didn't see (because they hit a non-aborting exception):
        fill(state.numDocsInStore - docWriter.getDocStoreOffset());

      tvx.flush();
      tvd.flush();
      tvf.flush();
    }

    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {
      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;
      perField.termsHashPerField.reset();
      perField.shrinkHash();
    }
  }

