  /** Merges the provided indexes into this index.
   * <p>After this completes, the index is optimized. </p>
   * <p>The provided IndexReaders are not closed.</p>
   *
   * <p><b>NOTE:</b> while this is running, any attempts to
   * add or delete documents (with another thread) will be
   * paused until this method completes.
   *
   * <p>See {@link #addIndexes} for details on transactional 
   * semantics, temporary free space required in the Directory, 
   * and non-CFS segments on an Exception.</p>
   *
   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError
   * you should immediately close the writer.  See <a
   * href="#OOME">above</a> for details.</p>
   *
   * <p><b>NOTE</b>: if you call {@link #close(boolean)}
   * with <tt>false</tt>, which aborts all running merges,
   * then any thread still running this method might hit a
   * {@link MergePolicy.MergeAbortedException}.
   *
   * @throws CorruptIndexException if the index is corrupt
   * @throws IOException if there is a low-level IO error
   */
  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {
    ensureOpen();
    int numDocs = 0;

    try {
      if (infoStream != null)
        message("flush at addIndexes(IndexReader...)");
      flush(false, true);

      String mergedName = newSegmentName();
      for (IndexReader indexReader : readers) {
        numDocs += indexReader.numDocs();
       }
       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      

      // TODO: somehow we should fix this merge so it's
      // abortable so that IW.close(false) is able to stop it
      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),
                                               mergedName, null, payloadProcessorProvider,
                                               new FieldInfos(globalFieldNumberMap), codec, context);

      for (IndexReader reader : readers)      // add new indexes
        merger.add(reader);
      int docCount = merger.merge();                // merge 'em

      final FieldInfos fieldInfos = merger.fieldInfos();
      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,
                                         false, merger.getCodec(),
                                         fieldInfos);
      setDiagnostics(info, "addIndexes(IndexReader...)");

      boolean useCompoundFile;
      synchronized(this) { // Guard segmentInfos
        if (stopMerges) {
          deleter.deleteNewFiles(info.files());
          return;
        }
        ensureOpen();
        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);
      }

      // Now create the compound file if needed
      if (useCompoundFile) {
        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);

        // delete new non cfs files directly: they were never
        // registered with IFD
        synchronized(this) {
          deleter.deleteNewFiles(info.files());
        }
        info.setUseCompoundFile(true);
      }

      // Register the new segment
      synchronized(this) {
        if (stopMerges) {
          deleter.deleteNewFiles(info.files());
          return;
        }
        ensureOpen();
        segmentInfos.add(info);
        checkpoint();
      }
    } catch (OutOfMemoryError oom) {
      handleOOM(oom, "addIndexes(IndexReader...)");
    }
  }

