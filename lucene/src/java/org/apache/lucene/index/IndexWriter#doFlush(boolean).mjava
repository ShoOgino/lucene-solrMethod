  // TODO: this method should not have to be entirely
  // synchronized, ie, merges should be allowed to commit
  // even while a flush is happening
  private synchronized boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {

    if (hitOOM) {
      throw new IllegalStateException("this writer hit an OutOfMemoryError; cannot flush");
    }

    doBeforeFlush();

    assert testPoint("startDoFlush");

    // We may be flushing because it was triggered by doc
    // count, del count, ram usage (in which case flush
    // pending is already set), or we may be flushing
    // due to external event eg getReader or commit is
    // called (in which case we now set it, and this will
    // pause all threads):
    flushControl.setFlushPendingNoWait("explicit flush");

    boolean success = false;

    try {

      if (infoStream != null) {
        message("  start flush: applyAllDeletes=" + applyAllDeletes);
        message("  index before flush " + segString());
      }
    
      final SegmentInfo newSegment = docWriter.flush(this, deleter, mergePolicy, segmentInfos);
      if (newSegment != null) {
        setDiagnostics(newSegment, "flush");
        segmentInfos.add(newSegment);
        checkpoint();
      }

      if (!applyAllDeletes) {
        // If deletes alone are consuming > 1/2 our RAM
        // buffer, force them all to apply now. This is to
        // prevent too-frequent flushing of a long tail of
        // tiny segments:
        if (flushControl.getFlushDeletes() ||
            (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&
             bufferedDeletesStream.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {
          applyAllDeletes = true;
          if (infoStream != null) {
            message("force apply deletes bytesUsed=" + bufferedDeletesStream.bytesUsed() + " vs ramBuffer=" + (1024*1024*config.getRAMBufferSizeMB()));
          }
        }
      }

      if (applyAllDeletes) {
        if (infoStream != null) {
          message("apply all deletes during flush");
        }
        flushDeletesCount.incrementAndGet();
        final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, segmentInfos);
        if (result.anyDeletes) {
          checkpoint();
        }
        bufferedDeletesStream.prune(segmentInfos);
        assert !bufferedDeletesStream.any();
        flushControl.clearDeletes();
      } else if (infoStream != null) {
        message("don't apply deletes now delTermCount=" + bufferedDeletesStream.numTerms() + " bytesUsed=" + bufferedDeletesStream.bytesUsed());
      }

      doAfterFlush();
      flushCount.incrementAndGet();

      success = true;

      return newSegment != null;

    } catch (OutOfMemoryError oom) {
      handleOOM(oom, "doFlush");
      // never hit
      return false;
    } finally {
      flushControl.clearFlushPending();
      if (!success && infoStream != null)
        message("hit exception during flush");
    }
  }

