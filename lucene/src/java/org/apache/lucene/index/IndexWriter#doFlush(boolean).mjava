  // TODO: this method should not have to be entirely
  // synchronized, ie, merges should be allowed to commit
  // even while a flush is happening
  private boolean doFlush(boolean applyAllDeletes) throws CorruptIndexException, IOException {
    if (hitOOM) {
      throw new IllegalStateException("this writer hit an OutOfMemoryError; cannot flush");
    }

    doBeforeFlush();

    assert testPoint("startDoFlush");

    // We may be flushing because it was triggered by doc
    // count, del count, ram usage (in which case flush
    // pending is already set), or we may be flushing
    // due to external event eg getReader or commit is
    // called (in which case we now set it, and this will
    // pause all threads):
    flushControl.setFlushPendingNoWait("explicit flush");

    boolean success = false;

    try {

      if (infoStream != null) {
        message("  start flush: applyAllDeletes=" + applyAllDeletes);
        message("  index before flush " + segString());
      }

      boolean maybeMerge = docWriter.flushAllThreads(applyAllDeletes);

      synchronized(this) {
        if (!applyAllDeletes) {
          // If deletes alone are consuming > 1/2 our RAM
          // buffer, force them all to apply now. This is to
          // prevent too-frequent flushing of a long tail of
          // tiny segments:
          if (flushControl.getFlushDeletes() ||
              (config.getRAMBufferSizeMB() != IndexWriterConfig.DISABLE_AUTO_FLUSH &&
               bufferedDeletes.bytesUsed() > (1024*1024*config.getRAMBufferSizeMB()/2))) {
            applyAllDeletes = true;
            if (infoStream != null) {
              message("force apply deletes bytesUsed=" + bufferedDeletes.bytesUsed() + " vs ramBuffer=" + (1024*1024*config.getRAMBufferSizeMB()));
            }
          }
        }

        if (applyAllDeletes) {
          if (infoStream != null) {
            message("apply all deletes during flush");
          }
          flushDeletesCount.incrementAndGet();
          if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, segmentInfos)) {
            checkpoint();
          }
          flushControl.clearDeletes();
        } else if (infoStream != null) {
          message("don't apply deletes now delTermCount=" + bufferedDeletes.numTerms() + " bytesUsed=" + bufferedDeletes.bytesUsed());
        }

        doAfterFlush();
        flushCount.incrementAndGet();

        success = true;

        return maybeMerge;

      }
    } catch (OutOfMemoryError oom) {
      handleOOM(oom, "doFlush");
      // never hit
      return false;
    } finally {
      flushControl.clearFlushPending();
      if (!success && infoStream != null)
        message("hit exception during flush");
    }
  }

