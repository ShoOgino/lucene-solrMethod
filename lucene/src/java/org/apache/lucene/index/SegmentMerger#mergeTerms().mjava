  private final void mergeTerms() throws CorruptIndexException, IOException {

    // Let CodecProvider decide which codec will be used to write
    // the new segment:
    
    int docBase = 0;

    final List<Fields> fields = new ArrayList<Fields>();
    final List<IndexReader> subReaders = new ArrayList<IndexReader>();
    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
    final List<Bits> bits = new ArrayList<Bits>();
    final List<Integer> bitsStarts = new ArrayList<Integer>();

    final int numReaders = readers.size();
    for(int i=0;i<numReaders;i++) {
      docBase = new ReaderUtil.Gather(readers.get(i)) {
          @Override
          protected void add(int base, IndexReader r) throws IOException {
            final Fields f = r.fields();
            if (f != null) {
              subReaders.add(r);
              fields.add(f);
              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
              bits.add(r.getDeletedDocs());
              bitsStarts.add(base);
            }
          }
        }.run(docBase);
    }

    bitsStarts.add(docBase);

    // we may gather more readers than mergeState.readerCount
    mergeState = new MergeState();
    mergeState.readers = subReaders;
    mergeState.readerCount = subReaders.size();
    mergeState.fieldInfos = fieldInfos;
    mergeState.mergedDocCount = mergedDocs;
    
    // Remap docIDs
    mergeState.delCounts = new int[mergeState.readerCount];
    mergeState.docMaps = new int[mergeState.readerCount][];
    mergeState.docBase = new int[mergeState.readerCount];
    mergeState.hasPayloadProcessorProvider = payloadProcessorProvider != null;
    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[mergeState.readerCount];
    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[mergeState.readerCount];

    docBase = 0;
    int inputDocBase = 0;

    final int[] starts = new int[mergeState.readerCount+1];

    for(int i=0;i<mergeState.readerCount;i++) {

      final IndexReader reader = subReaders.get(i);

      starts[i] = inputDocBase;

      mergeState.delCounts[i] = reader.numDeletedDocs();
      mergeState.docBase[i] = docBase;
      docBase += reader.numDocs();
      inputDocBase += reader.maxDoc();
      if (mergeState.delCounts[i] != 0) {
        int delCount = 0;
        final Bits delDocs = MultiFields.getDeletedDocs(reader);
        assert delDocs != null;
        final int maxDoc = reader.maxDoc();
        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];
        int newDocID = 0;
        for(int j=0;j<maxDoc;j++) {
          if (delDocs.get(j)) {
            docMap[j] = -1;
            delCount++;  // only for assert
          } else {
            docMap[j] = newDocID++;
          }
        }
        assert delCount == mergeState.delCounts[i]: "reader delCount=" + mergeState.delCounts[i] + " vs recomputed delCount=" + delCount;
      }
      
      if (payloadProcessorProvider != null) {
        mergeState.dirPayloadProcessor[i] = payloadProcessorProvider.getDirProcessor(reader.directory());
      }
    }
    starts[mergeState.readerCount] = inputDocBase;
    codec = segmentWriteState.segmentCodecs.codec();
    final FieldsConsumer consumer = codec.fieldsConsumer(segmentWriteState);

    // NOTE: this is silly, yet, necessary -- we create a
    // MultiBits as our skip docs only to have it broken
    // apart when we step through the docs enums in
    // MultiDocsEnum.
    mergeState.multiDeletedDocs = new MultiBits(bits, bitsStarts);
    
    try {
      consumer.merge(mergeState,
                     new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));
    } finally {
      consumer.close();
    }
  }

