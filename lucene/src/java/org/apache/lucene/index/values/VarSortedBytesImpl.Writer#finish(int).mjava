    // Important that we get docCount, in case there were
    // some last docs that we didn't see
    @Override
    synchronized public void finish(int docCount) throws IOException {
      final int count = hash.size();
      if (count == 0)
        return;
      initIndexOut();
      initDataOut();
      int[] sortedEntries = hash.sort(comp);

      // first dump bytes data, recording index & offset as
      // we go
      long offset = 0;
      long lastOffset = 0;
      final int[] index = new int[count];
      final long[] offsets = new long[count];
      for (int i = 0; i < count; i++) {
        final int e = sortedEntries[i];
        offsets[i] = offset;
        index[e] = 1 + i;

        final BytesRef bytes = hash.get(e);
        // TODO: we could prefix code...
        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
        lastOffset = offset;
        offset += bytes.length;
      }

      // total bytes of data
      idxOut.writeLong(offset);

      // write index -- first doc -> 1+ord
      // nocommit -- allow not -1:
      final PackedInts.Writer indexWriter = PackedInts.getWriter(idxOut,
          docCount, PackedInts.bitsRequired(count));
      final int limit = docCount > docToEntry.length ? docToEntry.length
          : docCount;
      for (int i = 0; i < limit; i++) {
        final int e = docToEntry[i];
        indexWriter.add(e == -1 ? 0 : index[e]);
      }
      for (int i = limit; i < docCount; i++) {
        indexWriter.add(0);
      }
      indexWriter.finish();

      // next ord (0-based) -> offset
      // nocommit -- allow not -1:
      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count,
          PackedInts.bitsRequired(lastOffset));
      for (int i = 0; i < count; i++) {
        offsetWriter.add(offsets[i]);
      }
      offsetWriter.finish();

      super.finish(docCount);
      bytesUsed.addAndGet((-docToEntry.length)
          * RamUsageEstimator.NUM_BYTES_INT);

    }

