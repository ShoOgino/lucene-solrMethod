  @Override
  public void processFields(final IndexableField[] fields,
                            final int count) throws IOException {

    fieldState.reset();

    final boolean doInvert = consumer.start(fields, count);

    for(int i=0;i<count;i++) {

      final IndexableField field = fields[i];

      // TODO FI: this should be "genericized" to querying
      // consumer if it wants to see this particular field
      // tokenized.
      if (field.fieldType().indexed() && doInvert) {

        if (i > 0)
          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);

        final TokenStream stream = field.tokenStream(docState.analyzer);
        // reset the TokenStream to the first token
        stream.reset();

        try {
          boolean hasMoreTokens = stream.incrementToken();

          fieldState.attributeSource = stream;

          OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);
          PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);

          consumer.start(field);

          for (;;) {

            // If we hit an exception in stream.next below
            // (which is fairly common, eg if analyzer
            // chokes on a given document), then it's
            // non-aborting and (above) this one document
            // will be marked as deleted, but still
            // consume a docID

            if (!hasMoreTokens) break;

            final int posIncr = posIncrAttribute.getPositionIncrement();
            fieldState.position += posIncr;
            if (fieldState.position > 0) {
              fieldState.position--;
            }

            if (posIncr == 0)
              fieldState.numOverlap++;

            boolean success = false;
            try {
              // If we hit an exception in here, we abort
              // all buffered documents since the last
              // flush, on the likelihood that the
              // internal state of the consumer is now
              // corrupt and should not be flushed to a
              // new segment:
              consumer.add();
              success = true;
            } finally {
              if (!success) {
                docState.docWriter.setAborting();
              }
            }
            fieldState.length++;
            fieldState.position++;

            hasMoreTokens = stream.incrementToken();
          }
          // trigger streams to perform end-of-stream operations
          stream.end();

          fieldState.offset += offsetAttribute.endOffset();
        } finally {
          stream.close();
        }

        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);
        fieldState.boost *= field.boost();
      }

      // LUCENE-2387: don't hang onto the field, so GC can
      // reclaim
      fields[i] = null;
    }

    consumer.finish();
    endConsumer.finish();
  }

