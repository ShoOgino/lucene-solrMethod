  // Primary entry point (for first TermsHash)
  @Override
  void add() throws IOException {

    assert !postingsCompacted;

    // We are first in the chain so we must "intern" the
    // term text into textStart address

    // Get the text & hash of this term.
    int code = termAtt.toBytesRef(utf8);

    int hashPos = code & postingsHashMask;

    // Locate RawPostingList in hash
    int termID = postingsHash[hashPos];

    if (termID != -1 && !postingEquals(termID)) {
      // Conflict: keep searching different locations in
      // the hash table.
      final int inc = ((code>>8)+code)|1;
      do {
        code += inc;
        hashPos = code & postingsHashMask;
        termID = postingsHash[hashPos];
      } while (termID != -1 && !postingEquals(termID));
    }

    if (termID == -1) {

      // First time we are seeing this token since we last
      // flushed the hash.
      final int textLen2 = 2+utf8.length;
      if (textLen2 + bytePool.byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE) {
        // Not enough room in current block

        if (utf8.length > DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8) {
          // Just skip this term, to remain as robust as
          // possible during indexing.  A TokenFilter
          // can be inserted into the analyzer chain if
          // other behavior is wanted (pruning the term
          // to a prefix, throwing an exception, etc).
          if (docState.maxTermPrefix == null) {
            final int saved = utf8.length;
            try {
              utf8.length = Math.min(30, DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8);
              docState.maxTermPrefix = utf8.toString();
            } finally {
              utf8.length = saved;
            }
          }

          consumer.skippingLongTerm();
          return;
        }
        bytePool.nextBuffer();
      }

      // New posting
      termID = numPostings++;
      if (termID >= postingsArray.size) {
        growParallelPostingsArray();
      }

      assert termID != -1;
      assert postingsHash[hashPos] == -1;

      postingsHash[hashPos] = termID;

      final byte[] text = bytePool.buffer;
      final int textUpto = bytePool.byteUpto;
      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;

      // We first encode the length, followed by the UTF8
      // bytes.  Length is encoded as vInt, but will consume
      // 1 or 2 bytes at most (we reject too-long terms,
      // above).

      // encode length @ start of bytes
      if (utf8.length < 128) {
        // 1 byte to store length
        text[textUpto] = (byte) utf8.length;
        bytePool.byteUpto += utf8.length + 1;
        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);
      } else {
        // 2 byte to store length
        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));
        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);
        bytePool.byteUpto += utf8.length + 2;
        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);
      }

      if (numPostings == postingsHashHalfSize) {
        rehashPostings(2*postingsHashSize);
        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);
      }

      // Init stream slices
      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE) {
        intPool.nextBuffer();
      }

      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {
        bytePool.nextBuffer();
      }

      intUptos = intPool.buffer;
      intUptoStart = intPool.intUpto;
      intPool.intUpto += streamCount;

      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;

      for(int i=0;i<streamCount;i++) {
        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);
        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;
      }
      postingsArray.byteStarts[termID] = intUptos[intUptoStart];

      consumer.newTerm(termID);

    } else {
      final int intStart = postingsArray.intStarts[termID];
      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];
      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;
      consumer.addTerm(termID);
    }

    if (doNextCall)
      nextPerField.add(postingsArray.textStarts[termID]);
  }

