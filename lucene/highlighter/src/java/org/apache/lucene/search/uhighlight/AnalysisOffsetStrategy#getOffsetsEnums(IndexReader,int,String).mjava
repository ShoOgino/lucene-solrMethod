  @Override
  public List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException {
    // note: don't need LimitTokenOffsetFilter since content is already truncated to maxLength
    TokenStream tokenStream = tokenStream(content);

    if (memoryIndex != null) { // also handles automata.length > 0
      // We use a MemoryIndex and index the tokenStream so that later we have the PostingsEnum with offsets.

      // note: An *alternative* strategy is to get PostingsEnums without offsets from the main index
      //  and then marry this up with a fake PostingsEnum backed by a TokenStream (which has the offsets) and
      //  can use that to filter applicable tokens?  It would have the advantage of being able to exit
      //  early and save some re-analysis.  This would be an additional method/offset-source approach
      //  since it's still useful to highlight without any index (so we build MemoryIndex).

      // note: probably unwise to re-use TermsEnum on reset mem index so we don't. But we do re-use the
      //   leaf reader, which is a bit more top level than in the guts.
      memoryIndex.reset();

      // Filter the tokenStream to applicable terms
      if (preMemIndexFilterAutomaton != null) {
        tokenStream = newKeepWordFilter(tokenStream, preMemIndexFilterAutomaton);
      }
      memoryIndex.addField(field, tokenStream);//note: calls tokenStream.reset() & close()
      tokenStream = null; // it's consumed; done.
      docId = 0;

      if (automata.length > 0) {
        Terms foundTerms = leafReader.terms(field);
        if (foundTerms == null) {
          return Collections.emptyList(); //No offsets for this field.
        }
        // Un-invert for the automata. Much more compact than a CachingTokenStream
        tokenStream = MultiTermHighlighting.uninvertAndFilterTerms(foundTerms, 0, automata, content.length());
      }

    }

    return createOffsetsEnums(leafReader, docId, tokenStream);
  }

