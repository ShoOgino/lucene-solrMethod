  /** Test showing the behavior with whitespace */
  public void testTokenizer() throws Exception {
    Reader reader = new StringReader("我购买了道具和服装。我购买了道具和服装。");
    SmartChineseSentenceTokenizerFactory tokenizerFactory = new SmartChineseSentenceTokenizerFactory(new HashMap<String,String>());
    TokenStream stream = tokenizerFactory.create(reader);
    SmartChineseWordTokenFilterFactory factory = new SmartChineseWordTokenFilterFactory(new HashMap<String,String>());
    stream = factory.create(stream);
    // TODO: fix smart chinese to not emit punctuation tokens
    // at the moment: you have to clean up with WDF, or use the stoplist, etc
    assertTokenStreamContents(stream, 
       new String[] { "我", "购买", "了", "道具", "和", "服装", ",", 
        "我", "购买", "了", "道具", "和", "服装", ","
        });
  }

