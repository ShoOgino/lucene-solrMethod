  /** The incoming PathSlice for the dim we will split is already partitioned/sorted. */
  private void build(int nodeID, int leafNodeOffset,
                     PathSlice source,
                     IndexOutput out,
                     long minValue, long maxValue,
                     long[] blockMinValues,
                     long[] leafBlockFPs) throws IOException {

    long count = source.count;

    if (source.writer instanceof OfflineSliceWriter && count <= maxValuesSortInHeap) {
      // Cutover to heap:
      SliceWriter writer = new HeapSliceWriter((int) count);
      SliceReader reader = source.writer.getReader(source.start);
      for(int i=0;i<count;i++) {
        boolean hasNext = reader.next();
        assert hasNext;
        writer.append(reader.value(), reader.ord(), reader.docID());
      }
      source = new PathSlice(writer, 0, count);
    }

    // We should never hit dead-end nodes on recursion even in the adversarial cases:
    assert count > 0;

    if (nodeID >= leafNodeOffset) {
      // Leaf node: write block
      assert maxValue >= minValue;

      //System.out.println("\nleaf:\n  lat range: " + ((long) maxLatEnc-minLatEnc));
      //System.out.println("  lon range: " + ((long) maxLonEnc-minLonEnc));

      // Sort by docID in the leaf so we can .or(DISI) at search time:
      SliceReader reader = source.writer.getReader(source.start);

      int[] docIDs = new int[(int) count];

      boolean success = false;
      try {
        for (int i=0;i<source.count;i++) {

          // NOTE: we discard ord at this point; we only needed it temporarily
          // during building to uniquely identify each point to properly handle
          // the multi-valued case (one docID having multiple values):

          // We also discard lat/lon, since at search time, we reside on the
          // wrapped doc values for this:

          boolean result = reader.next();
          assert result;
          docIDs[i] = reader.docID();
        }
        success = true;
      } finally {
        if (success) {
          IOUtils.close(reader);
        } else {
          IOUtils.closeWhileHandlingException(reader);
        }
      }

      // TODO: not clear we need to do this anymore (we used to make a DISI over
      // the block at search time), but maybe it buys some memory
      // locality/sequentiality at search time?
      Arrays.sort(docIDs);

      // Dedup docIDs: for the multi-valued case where more than one value for the doc
      // wound up in this leaf cell, we only need to store the docID once:
      int lastDocID = -1;
      int uniqueCount = 0;
      for(int i=0;i<docIDs.length;i++) {
        int docID = docIDs[i];
        if (docID != lastDocID) {
          uniqueCount++;
          lastDocID = docID;
        }
      }
      assert uniqueCount <= count;

      // TODO: in theory we could compute exactly what this fp will be, since we fixed-width (writeInt) encode docID, and up-front we know
      // how many docIDs are in every leaf since we don't do anything special about multiple splitValue boundary case?
      long startFP = out.getFilePointer();
      out.writeVInt(uniqueCount);

      // Save the block file pointer:
      int blockID = nodeID - leafNodeOffset;
      leafBlockFPs[blockID] = startFP;
      //System.out.println("    leafFP=" + startFP);

      blockMinValues[blockID] = minValue;

      lastDocID = -1;
      for (int i=0;i<docIDs.length;i++) {
        // Absolute int encode; with "vInt of deltas" encoding, the .kdd size dropped from
        // 697 MB -> 539 MB, but query time for 225 queries went from 1.65 sec -> 2.64 sec.
        // I think if we also indexed prefix terms here we could do less costly compression
        // on those lists:
        int docID = docIDs[i];
        if (docID != lastDocID) {
          out.writeInt(docID);
          lastDocID = docID;
        }
      }
      //long endFP = out.getFilePointer();
      //System.out.println("  bytes/doc: " + ((endFP - startFP) / count));
    } else {
      // Inner node: sort, partition/recurse

      assert nodeID < blockMinValues.length: "nodeID=" + nodeID + " blockMinValues.length=" + blockMinValues.length;

      assert source.count == count;

      long leftCount = source.count / 2;

      // NOTE: we don't tweak leftCount for the boundary cases, which means at search time if we are looking for exactly splitValue then we
      // must search both left and right trees:
      long splitValue = getSplitValue(source, leftCount, minValue, maxValue);

      build(2*nodeID, leafNodeOffset,
            new PathSlice(source.writer, source.start, leftCount),
            out,
            minValue, splitValue,
            blockMinValues, leafBlockFPs);

      build(2*nodeID+1, leafNodeOffset,
            new PathSlice(source.writer, source.start+leftCount, count-leftCount),
            out,
            splitValue, maxValue,
            blockMinValues, leafBlockFPs);
    }
  }

